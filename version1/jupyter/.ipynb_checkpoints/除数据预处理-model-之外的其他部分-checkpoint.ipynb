{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 待研究问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [tensorflow-hub](https://tfhub.dev/)  \n",
    "- [tensorflow-hub的使用](https://tf.wiki/zh/appendix/tfhub.html)  \n",
    "- [TensorFlow Embeddings ](https://www.tensorflow.org/tutorials/text/word_embeddings)  \n",
    "- [一个使用tf-hub导入的模型，可以实现导入句子，导出Embedding](https://tfhub.dev/google/universal-sentence-encoder-large/5)  \n",
    "- [tf.data](https://tf.wiki/zh/basic/tools.html#tfdata)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# GPU/cpu问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## GPU测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 显存限制-防止溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "        # 通过这里的memory_limit来进行设置\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 设置cpu运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'# gpu报错 使用cpu运行\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 使用jupyter的时候要导入.py文件包的时候的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 正常情况下不用使用\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "# 要导入代码的路径 ,utils无法导入的时候使用,添加上自己code的路径 ,项目代码结构 code/utils ....\n",
    "sys.path.append('/home/roger/kaikeba/03_lecture/code')  # 这里设置自己的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 加载txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 方式一：这里的root在config文件中定义，导入这里即可\n",
    "with open (root,\"r\", encoding='utf-8') as f:\n",
    "    data_in = f.read()\n",
    "with open (root,\"r\", encoding='utf-8') as f:\n",
    "    data_out = f.read()\n",
    "    \n",
    "# 方式二：若不在config中定义时可直接填写路径\n",
    "root='data/couplet'  # 两个路径前边相同，故设置了相同的部分为root\n",
    "with open (root+\"/train/in.txt\",\"r\", encoding='utf-8') as f:\n",
    "    data_in = f.read()\n",
    "with open (root+\"/train/out.txt\",\"r\", encoding='utf-8') as f:\n",
    "    data_out = f.read()\n",
    "    \n",
    "# 方式三：使用numpy的方法  这个方法被封装在了config文件里边的load_dataset()函数里边\n",
    "#使用这样的一行代码可以实现数据集的加载\n",
    "# train_X,train_Y,test_X = load_dataset()\n",
    "train_X = np.loadtxt(train_x_path)\n",
    "train_Y = np.loadtxt(train_y_path)\n",
    "test_X = np.loadtxt(test_x_path)\n",
    "# 设置类型转换\n",
    "train_X.dtype = 'float64'\n",
    "train_Y.dtype = 'float64'\n",
    "test_X.dtype = 'float64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 加载csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 使用的是pandas的方法：\n",
    "data = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 加载预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 这里的字典在预处理文件中都处理好的直接加载进来（该处理的函数在utils文件的data_loader文件中）\n",
    "embedding_matrix=load_embedding_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 加载字典/构造字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 加载字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 这里的字典在预处理文件中都处理好的直接加载进来（该处理的函数在utils文件的data_loader文件中）\n",
    "vocab,reverse_vocab=load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 构造字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 下边是一种方式，提供一种思路\n",
    "import itertools\n",
    "# 获取所有的字\n",
    "words = list(itertools.chain.from_iterable(train_X))+list(itertools.chain.from_iterable(train_Y))\n",
    "# 去重\n",
    "words = set(words)\n",
    "# 构建vocab\n",
    "vocab = {word: index+1 for index ,word in enumerate(words)}\n",
    "# 添加unk标签\n",
    "vocab[\"unk\"] = 0\n",
    "# 也可以使用data_loader文件中封装的方法构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## build_dataset\n",
    "使用utils里边的data_loader封装好的build_dataset方法进行数据的预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tf.keras的方法进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# 这里的预处理是使用的是tensorflow.keras.preprocessing.sequence里边的pad_sequences进行的预处理\n",
    "# 它把<PAD>置为0，填充不足的位数\n",
    "\n",
    "# 转换成索引\n",
    "train_X_ids = [[vocab.get(word,0) for word in sen] for sen in train_X]\n",
    "train_Y_ids = [[vocab.get(word,0) for word in sen] for sen in train_Y]\n",
    "# 填充长度\n",
    "train_X_ids = pad_sequences(train_X_ids,maxlen=100,padding='post')\n",
    "train_Y_ids = pad_sequences(train_Y_ids,maxlen=100,padding='post')\n",
    "# 维度不够就扩展维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理的最终返回值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常返回的都是由索引组成的数据  \n",
    "`array([[32800.,   403.,   986., ..., 32803., 32803., 32803.],\n",
    "       [32800.,   791., 32801., ..., 32803., 32803., 32803.],\n",
    "       [32800.,  1452.,    82., ..., 32803., 32803., 32803.],\n",
    "       ...,\n",
    "       [32800.,  3669.,  4535., ..., 32803., 32803., 32803.],\n",
    "       [32800.,  3669., 32801., ..., 32803., 32803., 32803.],\n",
    "       [32800.,   253.,  1369., ..., 32803., 32803., 32803.]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置模型需要的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里要具体问题具体分析\n",
    "# 训练集的长度\n",
    "BUFFER_SIZE = len(train_X)\n",
    "\n",
    "# 输入的长度\n",
    "max_length_inp=train_X.shape[1]\n",
    "# 输出的长度\n",
    "max_length_targ=train_Y.shape[1]\n",
    "\n",
    "# 这里的Batch_size通常越大越好，但是过大的话可能会超出内存直接崩掉\n",
    "# 具体的大小要结合硬件设备来确定\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 训练一轮需要迭代多少步（即训练多少步才能将整个训练集训练完一遍）  直接用训练集总数除以batch_size\n",
    "steps_per_epoch = len(train_X)//BATCH_SIZE\n",
    "\n",
    "# 词向量维度  这里必须要设置为预训练的词向量的维数\n",
    "embedding_dim = 300\n",
    "\n",
    "# 隐藏层单元数  Encoder Decoder的隐藏层单元数量，可以自己设置\n",
    "# 这里大一点会拟合的更好，但是过大会过拟合（如数据量少，隐藏单元多，这样基本就过拟合）\n",
    "units = 1024\n",
    "\n",
    "# 词表大小\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建tf.data数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练集\n",
    "# 这里直接将我们的训练数据使用TensorFlow里边的tf.data.Dataset这个方法直接把它封起来，进行打乱\n",
    "# 这里和之前拿到训练数据，构建dataset的思路一样\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "# 从这里边将数据导入模型的方式\n",
    "for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        # 这里的inp: train_X, targ：train_Y\n",
    "        # 遍历轮数和训练集\n",
    "        # batch:第几次batch_size,(inp, targ)输入的内容是什么\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X_ids,train_Y_ids, batch_size =64, epochs =1, validation_split = 0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/epochs_10_batch_64_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('data/epochs_10_batch_64_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
