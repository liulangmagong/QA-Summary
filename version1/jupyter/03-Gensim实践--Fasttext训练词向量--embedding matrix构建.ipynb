{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点掌握要求：\n",
    "1. 通过gensim训练词向量 即Gensim工具的使用\n",
    " + 1.1 利用分词后的项目数据生成训练词向量用的训练数据\n",
    " + 1.2 保存词向量训练数据\n",
    " + 1.3 应用gensim中Word2Vec或Fasttext训练词向量\n",
    " + 1.4 保存训练好的词向量\n",
    "\n",
    "2. 构建embedding_matrix\n",
    "\n",
    "> 读取上步计算词向量和构建的`vocab`词表，以`vocab`中的`index`为`key`值构建`embedding_matrix`\n",
    "这一部分主要是为了后边的模型，就是Embedding层的那一部分，即直接在构建Embedding层的时候导入训练好的词向量，这样模型就不用再花费较多的时间来处理\n",
    "\n",
    "`eg: embedding_matrix[i] = [embedding_vector]`\n",
    "3. 获得Embedding_matrix的两种方法\n",
    "    - 定义函数获取\n",
    "    - 直接调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "# 引入 word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Project\\QA\n"
     ]
    }
   ],
   "source": [
    "# 获取根目录\n",
    "root = pathlib.Path(os.path.abspath('__file__')).parent.parent\n",
    "print(root)\n",
    "\n",
    "# 数据路径\n",
    "merger_data_path = os.path.join(root, 'data', 'merged_train_test_seg_data.csv')\n",
    "# 词向量模型保存路径\n",
    "save_model_path = os.path.join(root, 'data/wv', 'word2vec.model')\n",
    "# 词向量矩阵保存路径\n",
    "save_embedding_matrix_path=os.path.join(root, 'data', 'embedding_matrix.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用word2vec训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "help(word2vec.Word2Vec)\n",
    "\n",
    "\n",
    "里边有一个sg参数，通过设置该参数来指定是使用哪一个算法\n",
    "sg : {0, 1}, optional\n",
    "           Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "    \n",
    "上一节讲到的一个softmax的优化的方法，这里使用下边这个参数就可以指定使用哪一个优化方法\n",
    "hs : {0, 1}, optional\n",
    "           If 1, hierarchical softmax will be used for model training. 分层softmax\n",
    "           If 0, and `negative` is non-zero, negative sampling will be used.  负采样\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里直接使用word2vec.Word2Vec这个包来训练word2vec这个模型,训练词向量\n",
    "# 实例化word2vec模型为model_wv\n",
    "model_wv = word2vec.Word2Vec(LineSentence(merger_data_path), sg=1,workers=8,min_count=5,size=200)\n",
    "# LineSentence(merger_data_path): 一行一行的将该路径下的数据读进来\n",
    "# sg=1: 使用Skip-Gram来构建word2vec\n",
    "# workers=8: 使用8个进程来跑\n",
    "# min_count=5：词频小于5的直接滤掉\n",
    "# size=200：训练一个200维的词向量\n",
    "# 这里word2vec如何定义是使用Skip-Gram还是CBOW，这里可以直接通过help(word2vec.Word2Vec)来查看\n",
    "# 这里名字里边的wv就是word2vec的缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 11:31:23,928 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('瑞虎', 0.7469425797462463),\n",
       " ('瑞虎5', 0.7084462642669678),\n",
       " ('瑞虎3', 0.6410641074180603),\n",
       " ('东方之子', 0.6377476453781128),\n",
       " ('风云', 0.6346753239631653),\n",
       " ('江淮', 0.6338895559310913),\n",
       " ('名爵3', 0.632521390914917),\n",
       " ('风云2', 0.6258667707443237),\n",
       " ('瑞麒', 0.6176993250846863),\n",
       " ('4G16', 0.6092939376831055)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用训练好的词向量模型计算相似度\n",
    "model_wv.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用FastText训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 这里直接使用FastText这个包来训练FastText这个模型，训练词向量\n",
    "# 实例化FastText模型为model_ft\n",
    "model_ft = FastText(sentences=LineSentence(merger_data_path), workers=8, min_count=5, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 11:32:34,177 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-05-17 11:32:34,215 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('奇瑞E5', 0.8964439034461975),\n",
       " ('奇瑞A1', 0.8838685750961304),\n",
       " ('奇瑞A5', 0.8812012672424316),\n",
       " ('奇瑞A3', 0.8743686676025391),\n",
       " ('瑞虎5', 0.8706405758857727),\n",
       " ('东南', 0.8699465990066528),\n",
       " ('奇瑞QQ6', 0.8604031801223755),\n",
       " ('瑞虎7', 0.8598191142082214),\n",
       " ('奇瑞QQ', 0.8594138622283936),\n",
       " ('东风皮卡', 0.8575624227523804)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用训练好的词向量模型计算相似度\n",
    "model_ft.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 11:32:36,263 : INFO : saving Word2Vec object under D:\\Learning\\Project\\QA\\data/wv\\word2vec.model, separately None\n",
      "2020-05-17 11:32:36,266 : INFO : not storing attribute vectors_norm\n",
      "2020-05-17 11:32:36,267 : INFO : not storing attribute cum_table\n",
      "2020-05-17 11:32:36,754 : INFO : saved D:\\Learning\\Project\\QA\\data/wv\\word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model_wv.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 11:32:36,759 : INFO : loading Word2Vec object from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model\n",
      "2020-05-17 11:32:37,107 : INFO : loading wv recursively from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model.wv.* with mmap=None\n",
      "2020-05-17 11:32:37,108 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-05-17 11:32:37,108 : INFO : loading vocabulary recursively from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model.vocabulary.* with mmap=None\n",
      "2020-05-17 11:32:37,109 : INFO : loading trainables recursively from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model.trainables.* with mmap=None\n",
      "2020-05-17 11:32:37,110 : INFO : setting ignored attribute cum_table to None\n",
      "2020-05-17 11:32:37,110 : INFO : loaded D:\\Learning\\Project\\QA\\data/wv\\word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 11:32:37,188 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('瑞虎', 0.7469425797462463),\n",
       " ('瑞虎5', 0.7084462642669678),\n",
       " ('瑞虎3', 0.6410641074180603),\n",
       " ('东方之子', 0.6377476453781128),\n",
       " ('风云', 0.6346753239631653),\n",
       " ('江淮', 0.6338895559310913),\n",
       " ('名爵3', 0.632521390914917),\n",
       " ('风云2', 0.6258667707443237),\n",
       " ('瑞麒', 0.6176993250846863),\n",
       " ('4G16', 0.6092939376831055)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用保存好的模型计算相似度\n",
    "model.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建embedding_matrix\n",
    "这里提前构建好词的embedding矩阵，这样的话后边进行模型训练的时候，就不用再进行词的Embedding了，直接将这里的Embedding矩阵导入即可。\n",
    "就是一个根据id获取词向量的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建vocab\n",
    "查看构建出来的vocab效果好不好：\n",
    "在Gensim里边有这么一个方法：score，用来判断这个词向量好还是不好，也就是看一个它的输入和输出，它的输入是词，输出也是词。后边自己试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个vocab是从model里边来，model里边是定义了一个词频参数，低于5的就滤掉，原先的语料就不用再去统计\n",
    "# 这个框架就直接是低于5的就滤掉了，就直接实现了过滤掉了低频词\n",
    "vocab = {word:index for index, word in enumerate(model_wv.wv.index2word)}\n",
    "# 所以这里的表要定义成字典的形式，便于根据key得到value（词向量）\n",
    "reverse_vocab = {index: word for index, word in enumerate(model_wv.wv.index2word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法一\n",
    "这种方法就完整的复现了第二次课里所描述的方法，就是直接拿到第 `i` 个词的词向量赋值给初始化的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(wv_model):\n",
    "    # 获取vocab大小\n",
    "    vocab_size = len(wv_model.wv.vocab)\n",
    "    # 获取embedding维度:词向量的维度\n",
    "    embedding_dim = wv_model.wv.vector_size\n",
    "    print('vocab_size, embedding_dim:', vocab_size, embedding_dim)\n",
    "    # 初始化词向量矩阵  shape和词向量矩阵一样  对应的行数就是 vocab_size(词的个数)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    # 这里的vocab_size就是词的个数\n",
    "    # 按顺序填充\n",
    "    for i in range(vocab_size):\n",
    "        # 遍历整个初始化的词表，将对应 id 的词向量放入词表中\n",
    "        embedding_matrix[i, :] = wv_model.wv[wv_model.wv.index2word[i]]\n",
    "        # wv_model.wv.index2word[i]从第一个词开始依次输出词表里边的词，拿到它对应的向量，然后赋值给这个初始化全为0的numpy矩阵\n",
    "        # 转换一下格式\n",
    "        embedding_matrix = embedding_matrix.astype('float32')\n",
    "    # 断言检查维度是否符合要求，是否是自己想要的大小\n",
    "    assert embedding_matrix.shape == (vocab_size, embedding_dim)\n",
    "    # 保存矩阵\n",
    "    np.savetxt(save_embedding_matrix_path, embedding_matrix, fmt='%0.8f')\n",
    "    print('embedding matrix extracted')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size, embedding_dim: 32801 200\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix=get_embedding_matrix(model_wv)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32905, 200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法二\n",
    "这里直接通过这里的方法，直接拿矩阵也可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_wv=model_wv.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32905, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_wv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比\n",
    "对比两种方法得到的矩阵，所有的参数都是一样的，所以整体来说这个方法要好一些，直接拿取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix==embedding_matrix_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding_matrix==embedding_matrix_wv).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1. 有没有一个标准的处理流程,怕前期数据处理影响后期项目效果? \n",
    "对于数据处理这个部分，一开始的方法可能会是一个比较low的方法，后边会不断的去完善数据处理这个部分，结合任务，不断的优化这个模块，这是一个不断修改，不断矫正的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
