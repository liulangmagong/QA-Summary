{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取中国党员网上一篇文章并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取文本：\n",
      " 标题： 吴邦国重申：中国坚持和平发展道路不会因国力地位变化而改变_党建_共产党员网 \n",
      "正文： ['\\u3000\\u3000亚太地区影响最大的议会间组织——亚太议会论坛28日召在符拉迪沃斯托克召开年会。中国全国人大常委会委员长吴邦国在与会发言中重申，坚持和平发展道路是中国基于时代发展潮流和自身根本利益作出的战略抉择，不会因为综合国力和国际地位的变化而改变。', '\\u3000\\u3000亚太议会论坛成立于1993年，拥有中国、俄罗斯、美国等27个成员国。年会是这个论坛的最高决策机构，每年轮流在太平洋两岸举行。据了解，本届年会为期3天，与会代表将围绕地区安全、经济贸易、区域合作等议题进行坦诚对话。', '\\u3000\\u3000吴邦国在年会主旨发言中阐述了中方在事关亚太地区和平发展重大问题上的原则主张。他指出，促进亚太地区和平合作发展，是亚太各国的共同责任，各方要从战略高度审视地区形势和彼此关系，努力扩大共识、付诸行动。“我们要摒弃冷战思维和零和博弈观念，相互尊重彼此主权和核心利益，推动建立公平有效的地区安全机制；要积极推动高新技术、先进制造、节能环保、能源资源、现代农业等领域务实合作，反对各种形式的保护主义，推动贸易和投资自由化、区域经济一体化；要尊重文明多样性，尊重各国人民自主选择的发展道路，促进不同文明和社会制度相互交流借鉴，推动亚太多元文明共同进步。”', '\\u3000\\u3000吴邦国表示，议会在各自国家政治生活中发挥着重要作用。各成员的议会应敦促和支持本国政府实施有利于和平发展、互利共赢的外交政策，加强议会间各层次、各领域对话交流，努力使议会交往成为推动国家关系发展的建设性力量。他同时强调，中国坚持和平发展，继续推进对外开放。“中国已经取得的发展成就举世瞩目，但面临的矛盾和挑战也世所罕见。发展仍然是解决中国所有问题的关键，我们将一心一意谋发展，始终不渝奉行互利共赢开放战略，努力在更广领域、更高层次扩大对外开放。我们将始终不渝走和平发展道路，坚定奉行独立自主的和平外交政策，这是中国基于时代发展潮流和自身根本利益作出的战略抉择，不会因为中国综合国力和国际地位的变化而改变。”', '\\u3000\\u3000吴邦国表示，中国坚持国家不论大小、强弱、贫富一律平等，不干涉别国内政，永不称霸；坚持通过和平谈判方式解决同周边邻国历史遗留的陆地边界问题，妥善处理同有关国家的岛屿主权和海洋权益争端；推动和平解决国际争端和热点问题，发挥负责任大国作用。', '\\u3000\\u3000在当天的大会发言中，本届论坛的东道主——俄罗斯联邦委员会主席马特维延科女士呼吁有关各方共同努力，推动论坛取得更大成果。“俄罗斯联邦是第二次担任亚太议会论坛主席国，亚太议会论坛是亚太地区最具权威性的议会间的合作组织。过去二十年间，亚太议会论坛走过了一条光辉大道，我们在论坛框架内讨论各种全球性和地区发展的迫切问题。我相信，在各位的共同努力下，本届年会将会是一个成果丰硕的会议，有助于形成我们共同的面向未来的行动纲领。”\\u3000\\u3000（记者 吴倩）']\n",
      "　　亚太地区影响最大的议会间组织——亚太议会论坛28日召在符拉迪沃斯托克召开年会。中国全国人大常委会委员长吴邦国在与会发言中重申，坚持和平发展道路是中国基于时代发展潮流和自身根本利益作出的战略抉择，不会因为综合国力和国际地位的变化而改变。　　亚太议会论坛成立于1993年，拥有中国、俄罗斯、美国等27个成员国。年会是这个论坛的最高决策机构，每年轮流在太平洋两岸举行。据了解，本届年会为期3天，与会代表将围绕地区安全、经济贸易、区域合作等议题进行坦诚对话。　　吴邦国在年会主旨发言中阐述了中方在事关亚太地区和平发展重大问题上的原则主张。他指出，促进亚太地区和平合作发展，是亚太各国的共同责任，各方要从战略高度审视地区形势和彼此关系，努力扩大共识、付诸行动。“我们要摒弃冷战思维和零和博弈观念，相互尊重彼此主权和核心利益，推动建立公平有效的地区安全机制；要积极推动高新技术、先进制造、节能环保、能源资源、现代农业等领域务实合作，反对各种形式的保护主义，推动贸易和投资自由化、区域经济一体化；要尊重文明多样性，尊重各国人民自主选择的发展道路，促进不同文明和社会制度相互交流借鉴，推动亚太多元文明共同进步。”　　吴邦国表示，议会在各自国家政治生活中发挥着重要作用。各成员的议会应敦促和支持本国政府实施有利于和平发展、互利共赢的外交政策，加强议会间各层次、各领域对话交流，努力使议会交往成为推动国家关系发展的建设性力量。他同时强调，中国坚持和平发展，继续推进对外开放。“中国已经取得的发展成就举世瞩目，但面临的矛盾和挑战也世所罕见。发展仍然是解决中国所有问题的关键，我们将一心一意谋发展，始终不渝奉行互利共赢开放战略，努力在更广领域、更高层次扩大对外开放。我们将始终不渝走和平发展道路，坚定奉行独立自主的和平外交政策，这是中国基于时代发展潮流和自身根本利益作出的战略抉择，不会因为中国综合国力和国际地位的变化而改变。”　　吴邦国表示，中国坚持国家不论大小、强弱、贫富一律平等，不干涉别国内政，永不称霸；坚持通过和平谈判方式解决同周边邻国历史遗留的陆地边界问题，妥善处理同有关国家的岛屿主权和海洋权益争端；推动和平解决国际争端和热点问题，发挥负责任大国作用。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from  lxml import html\n",
    "\n",
    "etree=html.etree\n",
    "url='http://news.12371.cn/2013/01/28/ARTI1359357184590944.shtml'\n",
    "data=requests.get(url)\n",
    "data.encoding='utf-8'\n",
    "#print(data)\n",
    "s=etree.HTML(data.text)\n",
    "text1=s.xpath('//*[@id=\"font_area\"]/p/text()')#得到的文本是一个列表，里面有6项，代表6个自然段\n",
    "title=s.xpath('/html/head/title/text()')[0].strip()#[0]是取标题的第一项，trip()去掉首尾空格\n",
    "print(\"爬取文本：\\n\",\"标题：\",title,\"\\n正文：\",text1)\n",
    "text=\"\"\n",
    "\n",
    "# 将得到的文本写入文件\n",
    "for i in range(0,len(text1)-1):\n",
    "   text+=text1[i]\n",
    "sentence_list=[]\n",
    "print(text)\n",
    "title=title+'.txt'\n",
    "with open(title, 'w', encoding='utf-8') as f:\n",
    "   f.writelines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前10个句子为：\n",
      "\n",
      "['亚太地区影响最大的议会间组织——亚太议会论坛28日召在符拉迪沃斯托克召开年会', '中国全国人大常委会委员长吴邦国在与会发言中重申，坚持和平发展道路是中国基于时代发展潮流和自身根本利益作出的战略抉择，不会因为综合国力和国际地位的变化而改变', '亚太议会论坛成立于1993年，拥有中国、俄罗斯、美国等27个成员国', '年会是这个论坛的最高决策机构，每年轮流在太平洋两岸举行', '据了解，本届年会为期3天，与会代表将围绕地区安全、经济贸易、区域合作等议题进行坦诚对话', '吴邦国在年会主旨发言中阐述了中方在事关亚太地区和平发展重大问题上的原则主张', '他指出，促进亚太地区和平合作发展，是亚太各国的共同责任，各方要从战略高度审视地区形势和彼此关系，努力扩大共识、付诸行动', '“我们要摒弃冷战思维和零和博弈观念，相互尊重彼此主权和核心利益，推动建立公平有效的地区安全机制', '要积极推动高新技术、先进制造、节能环保、能源资源、现代农业等领域务实合作，反对各种形式的保护主义，推动贸易和投资自由化、区域经济一体化', '要尊重文明多样性，尊重各国人民自主选择的发展道路，促进不同文明和社会制度相互交流借鉴，推动亚太多元文明共同进步']\n",
      "句子总数： 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re,jieba\n",
    "from itertools import chain\n",
    "\n",
    "#打开文件\n",
    "sentences_list = []\n",
    "file_path='D:/Learning/Project/QA/version1/jupyter/吴邦国重申：中国坚持和平发展道路不会因国力地位变化而改变_党建_共产党员网.txt'\n",
    "fp = open(file_path,'r',encoding=\"utf8\")\n",
    "for line in fp.readlines():\n",
    "        if line.strip():\n",
    "            # 把元素按照[。！；？]进行分隔，得到句子。\n",
    "            line_split = re.split(r'[。！；？]',line.strip())\n",
    "            # [。！；？]这些符号也会划分出来，把它们去掉。\n",
    "            line_split = [line.strip() for line in line_split if line.strip() not in ['。','！','？','；'] and len(line.strip())>1]\n",
    "            sentences_list.append(line_split)\n",
    "sentences_list = list(chain.from_iterable(sentences_list))\n",
    "print(\"前10个句子为：\\n\")\n",
    "print(sentences_list[:10])\n",
    "print(\"句子总数：\", len(sentences_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词，这里的停用词用的网上的，很容易搜到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\17854\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 19 个句子。\n",
      "\n",
      "前10个句子分词后的结果为：\n",
      " [['亚太地区', '影响', '最大', '议会', '间', '组织', '亚太', '议会', '论坛', '日召', '符拉迪沃斯托克', '召开', '年会'], ['中国', '全国人大常委会', '委员长', '吴邦国', '与会', '发言', '中', '重申', '坚持', '和平', '发展', '道路', '中国', '基于', '时代', '发展', '潮流', '根本利益', '作出', '战略', '抉择', '不会', '综合国力', '国际', '地位', '变化', '改变'], ['亚太', '议会', '论坛', '成立', '年', '拥有', '中国', '俄罗斯', '美国', '成员国'], ['年会', '论坛', '最高', '决策机构', '每年', '轮流', '太平洋', '两岸', '举行'], ['了解', '本届', '年会', '为期', '天', '与会代表', '围绕', '地区', '安全', '经济', '贸易', '区域合作', '议题', '进行', '坦诚', '对话'], ['吴邦国', '年', '会', '主旨', '发言', '中', '阐述', '中方', '事关', '亚太地区', '和平', '发展', '重大', '问题', '上', '原则', '主张'], ['指出', '促进', '亚太地区', '和平', '合作', '发展', '亚太', '各国', '共同', '责任', '各方', '战略', '高度', '审视', '地区', '形势', '关系', '努力', '扩大', '共识', '付诸行动'], ['摒弃', '冷战', '思维', '零', '博弈', '观念', '相互尊重', '主权', '核心', '利益', '推动', '建立', '公平', '有效', '地区', '安全', '机制'], ['积极', '推动', '高新技术', '先进', '制造', '节能', '环保', '能源', '资源', '现代农业', '领域', '务实', '合作', '反对', '形式', '保护主义', '推动', '贸易', '投资', '自由化', '区域', '经济', '一体化'], ['尊重', '文明', '多样性', '尊重', '各国', '人民', '自主', '选择', '发展', '道路', '促进', '不同', '文明', '社会制度', '相互', '交流', '借鉴', '推动', '亚太', '多元', '文明', '共同进步']]\n",
      "\n",
      "数据预处理后句子的数量不变！\n"
     ]
    }
   ],
   "source": [
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('D:/Learning/Project/QA/version1/data/stopwords/stopwords.txt',encoding='utf-8').readlines()]\n",
    "# print(stoplist)\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "\n",
    "sentence_word_list = []\n",
    "for sentence in sentences_list:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list.append(line_seg)\n",
    "print(\"一共有\",len(sentences_list),'个句子。\\n')\n",
    "print(\"前10个句子分词后的结果为：\\n\",sentence_word_list[:10])\n",
    "\n",
    "# 保证处理后句子的数量不变，我们后面才好根据textrank值取出未处理之前的句子作为摘要。\n",
    "if len(sentences_list) == len(sentence_word_list):\n",
    "    print(\"\\n数据预处理后句子的数量不变！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用word2vec生成词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec之类的模型，准确来说应该是“自监督”的，它事实上训练了一个语言模型，通过语言模型来获取词向量。\n",
    "所谓语言模型，就是通过前个字预测下一个字的概率，就是一个多分类器而已，我们输入one hot，然后连接一个全连接层，然后再连接若干个层，最后接一个softmax分类器，就可以得到语言模型了，然后将大批量文本输入训练就行了，最后得到第一个全连接层的参数，就是字、词向量表，当然，Word2Vec还做了大量的简化，但是那都是在语言模型本身做的简化，它的第一层还是全连接层，全连接层的参数就是字、词向量表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'发展': 1, '中国': 2, '议会': 3, '和平': 4, '推动': 5, '亚太': 6, '吴邦国': 7, '坚持': 8, '战略': 9, '国家': 10, '亚太地区': 11, '论坛': 12, '年会': 13, '中': 14, '道路': 15, '地区': 16, '努力': 17, '领域': 18, '文明': 19, '间': 20, '发言': 21, '基于': 22, '时代': 23, '潮流': 24, '根本利益': 25, '作出': 26, '抉择': 27, '不会': 28, '综合国力': 29, '国际': 30, '地位': 31, '变化': 32, '改变': 33, '年': 34, '安全': 35, '经济': 36, '贸易': 37, '对话': 38, '问题': 39, '促进': 40, '合作': 41, '各国': 42, '关系': 43, '扩大': 44, '主权': 45, '尊重': 46, '交流': 47, '表示': 48, '发挥': 49, '作用': 50, '互利': 51, '外交政策': 52, '对外开放': 53, '解决': 54, '始终不渝': 55, '奉行': 56, '更': 57, '影响': 58, '最大': 59, '组织': 60, '日召': 61, '符拉迪沃斯托克': 62, '召开': 63, '全国人大常委会': 64, '委员长': 65, '与会': 66, '重申': 67, '成立': 68, '拥有': 69, '俄罗斯': 70, '美国': 71, '成员国': 72, '最高': 73, '决策机构': 74, '每年': 75, '轮流': 76, '太平洋': 77, '两岸': 78, '举行': 79, '了解': 80, '本届': 81, '为期': 82, '天': 83, '与会代表': 84, '围绕': 85, '区域合作': 86, '议题': 87, '进行': 88, '坦诚': 89, '会': 90, '主旨': 91, '阐述': 92, '中方': 93, '事关': 94, '重大': 95, '上': 96, '原则': 97, '主张': 98, '指出': 99, '共同': 100, '责任': 101, '各方': 102, '高度': 103, '审视': 104, '形势': 105, '共识': 106, '付诸行动': 107, '摒弃': 108, '冷战': 109, '思维': 110, '零': 111, '博弈': 112, '观念': 113, '相互尊重': 114, '核心': 115, '利益': 116, '建立': 117, '公平': 118, '有效': 119, '机制': 120, '积极': 121, '高新技术': 122, '先进': 123, '制造': 124, '节能': 125, '环保': 126, '能源': 127, '资源': 128, '现代农业': 129, '务实': 130, '反对': 131, '形式': 132, '保护主义': 133, '投资': 134, '自由化': 135, '区域': 136, '一体化': 137, '多样性': 138, '人民': 139, '自主': 140, '选择': 141, '不同': 142, '社会制度': 143, '相互': 144, '借鉴': 145, '多元': 146, '共同进步': 147, '政治': 148, '生活': 149, '重要': 150, '成员': 151, '应': 152, '敦促': 153, '支持': 154, '本国': 155, '政府': 156, '实施': 157, '有利于': 158, '共': 159, '赢': 160, '加强': 161, '各层次': 162, '使': 163, '交往': 164, '成为': 165, '建设性': 166, '力量': 167, '强调': 168, '继续': 169, '推进': 170, '已经': 171, '取得': 172, '成就': 173, '举世瞩目': 174, '面临': 175, '矛盾': 176, '挑战': 177, '世所': 178, '罕见': 179, '仍然': 180, '所有': 181, '关键': 182, '一心一意': 183, '谋发展': 184, '共赢': 185, '开放': 186, '广': 187, '高层次': 188, '走': 189, '坚定': 190, '独立自主': 191, '这是': 192, '大小': 193, '强弱': 194, '贫富': 195, '一律平等': 196, '不': 197, '干涉': 198, '别国': 199, '内政': 200, '永不': 201, '称霸': 202, '和平谈判': 203, '方式': 204, '周边': 205, '邻国': 206, '历史': 207, '遗留': 208, '陆地': 209, '边界问题': 210, '妥善处理': 211, '岛屿': 212, '海洋权益': 213, '争端': 214, '和平解决': 215, '国际争端': 216, '热点问题': 217, '负责': 218, '大国': 219}\n"
     ]
    }
   ],
   "source": [
    "#求句子最大长度\n",
    "maxLen=0\n",
    "for sentence in sentences_list:\n",
    "    length=0\n",
    "    for wd in sentence:\n",
    "        length=length+1\n",
    "    if (length>maxLen):maxLen=length\n",
    "\n",
    "#fit_on_texts函数可以将输入的文本中的每个词编号，\n",
    "#编号是根据词频的，词频越大，编号越小\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence_word_list)\n",
    "vocab = tokenizer.word_index  # 得到每个词的编号\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec的训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 如果有需要的话，可以输入一个路径，保存训练好的模型\\nmodel.save(\"w2vModel1\")\\nprint(model)\\n#加载模型\\nmodel = word2vec.Word2Vec.load(\"w2vModel1\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "# 设置词语向量维度\n",
    "num_featrues = 300\n",
    "# 保证被考虑词语的最低频度，对于小语料，设置为1才可能能输出所有的词，因为有的词可能在每个句子中只出现一次\n",
    "min_word_count = 1\n",
    "# 设置并行化训练使用CPU计算核心数量\n",
    "num_workers =4\n",
    "# 设置词语上下文窗口大小\n",
    "context = 5\n",
    "#开始训练\n",
    "model = word2vec.Word2Vec(sentence_word_list, workers=num_workers, size=num_featrues, min_count=min_word_count, window=context)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "'''\n",
    "# 如果有需要的话，可以输入一个路径，保存训练好的模型\n",
    "model.save(\"w2vModel1\")\n",
    "print(model)\n",
    "#加载模型\n",
    "model = word2vec.Word2Vec.load(\"w2vModel1\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用训练后的word2vec自定义Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出了： 0 个词\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = {}\n",
    "count=0\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        # model.wv[word]存的就是这个word的词向量\n",
    "        word_embeddings[word] =model.wv[word]\n",
    "    except KeyError:\n",
    "        continue\n",
    "print('输出了：',count,'个词')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到词语的embedding，用WordAVG作为句子的向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for line in sentence_word_list:\n",
    "    if len(line)!=0:\n",
    "        # 如果句子中的词语不在字典中，那就把embedding设为300维元素为0的向量。\n",
    "        # 得到句子中全部词的词向量后，求平均值，得到句子的向量表示\n",
    "        #TypeError: type numpy.ndarray doesn't define __round__ method,将round改为np.round\n",
    "        v = np.round(sum(word_embeddings.get(word, np.zeros((300,))) for word in line)/(len(line)))\n",
    "    else:\n",
    "        # 如果句子为[]，那么就向量表示为300维元素为0个向量。\n",
    "        v = np.zeros((300,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始干正事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子相似度矩阵的形状为： (19, 19)\n",
      "第1条摘要：\n",
      "\n",
      " 要积极推动高新技术、先进制造、节能环保、能源资源、现代农业等领域务实合作，反对各种形式的保护主义，推动贸易和投资自由化、区域经济一体化 \n",
      "\n",
      "第2条摘要：\n",
      "\n",
      " 要尊重文明多样性，尊重各国人民自主选择的发展道路，促进不同文明和社会制度相互交流借鉴，推动亚太多元文明共同进步 \n",
      "\n",
      "第3条摘要：\n",
      "\n",
      " 推动和平解决国际争端和热点问题，发挥负责任大国作用 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#计算句子之间的余弦相似度，构成相似度矩阵\n",
    "sim_mat = np.zeros([len(sentences_list), len(sentences_list)])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for i in range(len(sentences_list)):\n",
    "  for j in range(len(sentences_list)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
    "print(\"句子相似度矩阵的形状为：\",sim_mat.shape)\n",
    "\n",
    "#迭代得到句子的textrank值，排序并取出摘要\"\"\"\n",
    "import networkx as nx\n",
    "\n",
    "# 利用句子相似度矩阵构建图结构，句子为节点，句子相似度为转移概率\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "\n",
    "# 得到所有句子的textrank值\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# 根据textrank值对未处理的句子进行排序\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences_list)), reverse=True)\n",
    "\n",
    "# 取出得分最高的前3个句子作为摘要\n",
    "sn = 3\n",
    "for i in range(sn):\n",
    "    print(\"第\"+str(i+1)+\"条摘要：\\n\\n\",ranked_sentences[i][1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
