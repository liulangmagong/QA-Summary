{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from seq2seq_tf2.batcher import train_batch_generator\n",
    "from utils.data_loader import build_dataset,load_dataset,preprocess_sentence,load_test_dataset\n",
    "from utils.wv_loader import load_embedding_matrix,load_vocab\n",
    "from utils.config import *\n",
    "from utils.params_utils import *\n",
    "from utils.gpu_utils import config_gpu\n",
    "from utils.plot_utils import plot_attention\n",
    "\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# 配置GPU\n",
    "config_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build_dataset(train_data_path,test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数设置  数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_enc_len 200\n",
      "load_train_dataset返回到train_batch_generator里边的训练集训练数据train_X： [[31816   415   903 ... 31818 31818 31818]\n",
      " [31816   813 31819 ... 31818 31818 31818]\n",
      " [31816  1393    88 ...  3321  6567  2232]\n",
      " ...\n",
      " [31816   225   894 ... 31818 31818 31818]\n",
      " [31816 12684  3145 ... 31818 31818 31818]\n",
      " [31816  3275    75 ...   409     1     3]]\n",
      "load_train_dataset返回到train_batch_generator里边的训练集测试数据train_Y： [[31816   326   391 ... 31818 31818 31818]\n",
      " [31816   326   391 ... 31818 31818 31818]\n",
      " [31816    80     8 ... 31818 31818 31818]\n",
      " ...\n",
      " [31816    32    23 ... 31818 31818 31818]\n",
      " [31816    32    23 ... 31818 31818 31818]\n",
      " [31816    32    23 ... 31818 31818 31818]]\n",
      "load_train_dataset返回到train_batch_generator里边的训练集训练数据的形状： (82873, 200)\n",
      "load_train_dataset返回到train_batch_generator里边的训练集测试数据的形状： (82873, 41)\n"
     ]
    }
   ],
   "source": [
    "# 加载vocab\n",
    "vocab,reverse_vocab=load_vocab(vocab_path)\n",
    "\n",
    "# 加载预训练权重\n",
    "embedding_matrix=load_embedding_matrix()\n",
    "\n",
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"embed_size\"] = 500\n",
    "params[\"enc_units\"] = 512\n",
    "params[\"attn_units\"] = 512\n",
    "params[\"dec_units\"] = 512\n",
    "params[\"batch_size\"] = 32\n",
    "params[\"epochs\"] = 5\n",
    "params[\"max_enc_len\"] = 200\n",
    "params[\"max_dec_len\"] = 41 \n",
    "\n",
    "# 加载数据集\n",
    "dataset, steps_per_epoch = train_batch_generator(batch_size=32,\n",
    "                                                 max_enc_len=params[\"max_enc_len\"],\n",
    "                                                 max_dec_len=params[\"max_dec_len\"])\n",
    "test_X = load_test_dataset(params[\"max_dec_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_tf2.seq2seq_model import Seq2Seq\n",
    "\n",
    "# 传入参数，构建初始化模型框架   checkpoint的保存的参数，加载之前必须构建好模型的框架\n",
    "model=Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import checkpoint_dir, checkpoint_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(Seq2Seq=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建优化器，定义损失函数\n",
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# 这里做了修改，即计算loss的时候将这两个部分的loss都进行取出，不计算\n",
    "pad_index=vocab['<PAD>']\n",
    "nuk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)  # 1\n",
    "    nuk_mask = tf.math.equal(real, nuk_index)  # 1\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,nuk_mask))  # 0， 0\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[@tf.function](https://zhuanlan.zhihu.com/p/67192636)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # 逐个预测序列\n",
    "        predictions, _ = model(dec_input, dec_hidden, enc_output, targ)\n",
    "        \n",
    "        batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "\n",
    "        variables = model.encoder.trainable_variables + model.decoder.trainable_variables+ model.attention.trainable_variables\n",
    "    \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.7275\n",
      "Epoch 1 Batch 1 Loss 0.7556\n",
      "Epoch 1 Batch 2 Loss 0.7951\n",
      "Epoch 1 Batch 3 Loss 0.7539\n",
      "Epoch 1 Batch 4 Loss 1.1512\n",
      "Epoch 1 Batch 5 Loss 0.8018\n",
      "Epoch 1 Batch 6 Loss 0.8899\n",
      "Epoch 1 Batch 7 Loss 0.8596\n",
      "Epoch 1 Batch 8 Loss 0.7371\n",
      "Epoch 1 Batch 9 Loss 0.6927\n",
      "Epoch 1 Batch 10 Loss 0.7799\n",
      "Epoch 1 Batch 11 Loss 0.6522\n",
      "Epoch 1 Batch 12 Loss 0.8805\n",
      "Epoch 1 Batch 13 Loss 0.9190\n",
      "Epoch 1 Batch 14 Loss 1.0017\n",
      "Epoch 1 Batch 15 Loss 0.6937\n",
      "Epoch 1 Batch 16 Loss 0.7246\n",
      "Epoch 1 Batch 17 Loss 0.8001\n",
      "Epoch 1 Batch 18 Loss 0.6500\n",
      "Epoch 1 Batch 19 Loss 0.9564\n",
      "Epoch 1 Batch 20 Loss 0.9028\n",
      "Epoch 1 Batch 21 Loss 0.8553\n",
      "Epoch 1 Batch 22 Loss 1.0096\n",
      "Epoch 1 Batch 23 Loss 0.9836\n",
      "Epoch 1 Batch 24 Loss 0.8970\n",
      "Epoch 1 Batch 25 Loss 0.9343\n",
      "Epoch 1 Batch 26 Loss 0.9130\n",
      "Epoch 1 Batch 27 Loss 0.7455\n",
      "Epoch 1 Batch 28 Loss 0.9140\n",
      "Epoch 1 Batch 29 Loss 0.9401\n",
      "Epoch 1 Batch 30 Loss 0.9019\n",
      "Epoch 1 Batch 31 Loss 0.7424\n",
      "Epoch 1 Batch 32 Loss 0.9070\n",
      "Epoch 1 Batch 33 Loss 0.9948\n",
      "Epoch 1 Batch 34 Loss 0.9006\n",
      "Epoch 1 Batch 35 Loss 0.9943\n",
      "Epoch 1 Batch 36 Loss 0.8669\n",
      "Epoch 1 Batch 37 Loss 0.7967\n",
      "Epoch 1 Batch 38 Loss 1.0422\n",
      "Epoch 1 Batch 39 Loss 0.8562\n",
      "Epoch 1 Batch 40 Loss 1.0168\n",
      "Epoch 1 Batch 41 Loss 0.8169\n",
      "Epoch 1 Batch 42 Loss 0.7815\n",
      "Epoch 1 Batch 43 Loss 0.6294\n",
      "Epoch 1 Batch 44 Loss 0.9998\n",
      "Epoch 1 Batch 45 Loss 1.0564\n",
      "Epoch 1 Batch 46 Loss 1.0897\n",
      "Epoch 1 Batch 47 Loss 0.9120\n",
      "Epoch 1 Batch 48 Loss 0.8258\n",
      "Epoch 1 Batch 49 Loss 0.8164\n",
      "Epoch 1 Batch 50 Loss 0.9565\n",
      "Epoch 1 Batch 51 Loss 0.9675\n",
      "Epoch 1 Batch 52 Loss 0.7280\n",
      "Epoch 1 Batch 53 Loss 1.0375\n",
      "Epoch 1 Batch 54 Loss 1.0037\n",
      "Epoch 1 Batch 55 Loss 0.6371\n",
      "Epoch 1 Batch 56 Loss 0.9576\n",
      "Epoch 1 Batch 57 Loss 0.8237\n",
      "Epoch 1 Batch 58 Loss 0.7177\n",
      "Epoch 1 Batch 59 Loss 0.8898\n",
      "Epoch 1 Batch 60 Loss 0.6503\n",
      "Epoch 1 Batch 61 Loss 0.7897\n",
      "Epoch 1 Batch 62 Loss 0.8627\n",
      "Epoch 1 Batch 63 Loss 0.8300\n",
      "Epoch 1 Batch 64 Loss 0.8270\n",
      "Epoch 1 Batch 65 Loss 0.8430\n",
      "Epoch 1 Batch 66 Loss 0.6865\n",
      "Epoch 1 Batch 67 Loss 0.7191\n",
      "Epoch 1 Batch 68 Loss 0.8582\n",
      "Epoch 1 Batch 69 Loss 0.8960\n",
      "Epoch 1 Batch 70 Loss 0.9872\n",
      "Epoch 1 Batch 71 Loss 0.9825\n",
      "Epoch 1 Batch 72 Loss 0.6230\n",
      "Epoch 1 Batch 73 Loss 0.6461\n",
      "Epoch 1 Batch 74 Loss 0.8366\n",
      "Epoch 1 Batch 75 Loss 0.9914\n",
      "Epoch 1 Batch 76 Loss 1.0314\n",
      "Epoch 1 Batch 77 Loss 0.7911\n",
      "Epoch 1 Batch 78 Loss 0.7808\n",
      "Epoch 1 Batch 79 Loss 0.9717\n",
      "Epoch 1 Batch 80 Loss 1.0374\n",
      "Epoch 1 Batch 81 Loss 1.0484\n",
      "Epoch 1 Batch 82 Loss 0.8537\n",
      "Epoch 1 Batch 83 Loss 0.8809\n",
      "Epoch 1 Batch 84 Loss 0.8972\n",
      "Epoch 1 Batch 85 Loss 0.7163\n",
      "Epoch 1 Batch 86 Loss 0.7490\n",
      "Epoch 1 Batch 87 Loss 0.6550\n",
      "Epoch 1 Batch 88 Loss 1.1152\n",
      "Epoch 1 Batch 89 Loss 1.0121\n",
      "Epoch 1 Batch 90 Loss 0.7047\n",
      "Epoch 1 Batch 91 Loss 0.9412\n",
      "Epoch 1 Batch 92 Loss 0.7012\n",
      "Epoch 1 Batch 93 Loss 0.8612\n",
      "Epoch 1 Batch 94 Loss 0.7005\n",
      "Epoch 1 Batch 95 Loss 0.7902\n",
      "Epoch 1 Batch 96 Loss 0.5795\n",
      "Epoch 1 Batch 97 Loss 1.0017\n",
      "Epoch 1 Batch 98 Loss 0.7094\n",
      "Epoch 1 Batch 99 Loss 0.8877\n",
      "Epoch 1 Batch 100 Loss 0.7309\n",
      "Epoch 1 Batch 101 Loss 0.9981\n",
      "Epoch 1 Batch 102 Loss 0.7788\n",
      "Epoch 1 Batch 103 Loss 0.8034\n",
      "Epoch 1 Batch 104 Loss 0.8838\n",
      "Epoch 1 Batch 105 Loss 0.7636\n",
      "Epoch 1 Batch 106 Loss 1.0249\n",
      "Epoch 1 Batch 107 Loss 1.0011\n",
      "Epoch 1 Batch 108 Loss 0.8688\n",
      "Epoch 1 Batch 109 Loss 0.7232\n",
      "Epoch 1 Batch 110 Loss 0.8596\n",
      "Epoch 1 Batch 111 Loss 1.0471\n",
      "Epoch 1 Batch 112 Loss 0.7271\n",
      "Epoch 1 Batch 113 Loss 0.8747\n",
      "Epoch 1 Batch 114 Loss 0.7223\n",
      "Epoch 1 Batch 115 Loss 0.7560\n",
      "Epoch 1 Batch 116 Loss 0.9259\n",
      "Epoch 1 Batch 117 Loss 0.8446\n",
      "Epoch 1 Batch 118 Loss 0.7649\n",
      "Epoch 1 Batch 119 Loss 0.8409\n",
      "Epoch 1 Batch 120 Loss 0.8856\n",
      "Epoch 1 Batch 121 Loss 0.6557\n",
      "Epoch 1 Batch 122 Loss 0.9677\n",
      "Epoch 1 Batch 123 Loss 0.7961\n",
      "Epoch 1 Batch 124 Loss 0.9207\n",
      "Epoch 1 Batch 125 Loss 0.8659\n",
      "Epoch 1 Batch 126 Loss 0.8854\n",
      "Epoch 1 Batch 127 Loss 1.0410\n",
      "Epoch 1 Batch 128 Loss 0.6373\n",
      "Epoch 1 Batch 129 Loss 0.9367\n",
      "Epoch 1 Batch 130 Loss 0.9130\n",
      "Epoch 1 Batch 131 Loss 0.9396\n",
      "Epoch 1 Batch 132 Loss 0.6229\n",
      "Epoch 1 Batch 133 Loss 0.9754\n",
      "Epoch 1 Batch 134 Loss 0.7827\n",
      "Epoch 1 Batch 135 Loss 1.0947\n",
      "Epoch 1 Batch 136 Loss 0.5903\n",
      "Epoch 1 Batch 137 Loss 0.8341\n",
      "Epoch 1 Batch 138 Loss 0.8192\n",
      "Epoch 1 Batch 139 Loss 1.0000\n",
      "Epoch 1 Batch 140 Loss 0.8257\n",
      "Epoch 1 Batch 141 Loss 0.9711\n",
      "Epoch 1 Batch 142 Loss 0.9123\n",
      "Epoch 1 Batch 143 Loss 1.0741\n",
      "Epoch 1 Batch 144 Loss 0.7631\n",
      "Epoch 1 Batch 145 Loss 0.7925\n",
      "Epoch 1 Batch 146 Loss 0.9594\n",
      "Epoch 1 Batch 147 Loss 0.7928\n",
      "Epoch 1 Batch 148 Loss 0.8186\n",
      "Epoch 1 Batch 149 Loss 0.8408\n",
      "Epoch 1 Batch 150 Loss 0.8128\n",
      "Epoch 1 Batch 151 Loss 1.0533\n",
      "Epoch 1 Batch 152 Loss 0.7115\n",
      "Epoch 1 Batch 153 Loss 0.8777\n",
      "Epoch 1 Batch 154 Loss 0.8745\n",
      "Epoch 1 Batch 155 Loss 0.7484\n",
      "Epoch 1 Batch 156 Loss 0.9953\n",
      "Epoch 1 Batch 157 Loss 0.9898\n",
      "Epoch 1 Batch 158 Loss 0.9865\n",
      "Epoch 1 Batch 159 Loss 0.8051\n",
      "Epoch 1 Batch 160 Loss 0.5534\n",
      "Epoch 1 Batch 161 Loss 0.7841\n",
      "Epoch 1 Batch 162 Loss 0.7056\n",
      "Epoch 1 Batch 163 Loss 0.8974\n",
      "Epoch 1 Batch 164 Loss 0.9781\n",
      "Epoch 1 Batch 165 Loss 0.8467\n",
      "Epoch 1 Batch 166 Loss 0.8544\n",
      "Epoch 1 Batch 167 Loss 1.1502\n",
      "Epoch 1 Batch 168 Loss 0.8770\n",
      "Epoch 1 Batch 169 Loss 0.7511\n",
      "Epoch 1 Batch 170 Loss 0.9460\n",
      "Epoch 1 Batch 171 Loss 0.8712\n",
      "Epoch 1 Batch 172 Loss 0.9440\n",
      "Epoch 1 Batch 173 Loss 0.8057\n",
      "Epoch 1 Batch 174 Loss 0.9749\n",
      "Epoch 1 Batch 175 Loss 0.7032\n",
      "Epoch 1 Batch 176 Loss 0.8502\n",
      "Epoch 1 Batch 177 Loss 0.8696\n",
      "Epoch 1 Batch 178 Loss 0.6629\n",
      "Epoch 1 Batch 179 Loss 0.7442\n",
      "Epoch 1 Batch 180 Loss 0.8707\n",
      "Epoch 1 Batch 181 Loss 0.8653\n",
      "Epoch 1 Batch 182 Loss 0.7321\n",
      "Epoch 1 Batch 183 Loss 0.8845\n",
      "Epoch 1 Batch 184 Loss 0.9666\n",
      "Epoch 1 Batch 185 Loss 0.7964\n",
      "Epoch 1 Batch 186 Loss 1.1268\n",
      "Epoch 1 Batch 187 Loss 0.9992\n",
      "Epoch 1 Batch 188 Loss 0.8670\n",
      "Epoch 1 Batch 189 Loss 0.7684\n",
      "Epoch 1 Batch 190 Loss 0.6592\n",
      "Epoch 1 Batch 191 Loss 0.8792\n",
      "Epoch 1 Batch 192 Loss 1.0079\n",
      "Epoch 1 Batch 193 Loss 1.0120\n",
      "Epoch 1 Batch 194 Loss 0.8628\n",
      "Epoch 1 Batch 195 Loss 0.8752\n",
      "Epoch 1 Batch 196 Loss 0.7703\n",
      "Epoch 1 Batch 197 Loss 0.9067\n",
      "Epoch 1 Batch 198 Loss 0.9187\n",
      "Epoch 1 Batch 199 Loss 1.1880\n",
      "Epoch 1 Batch 200 Loss 1.2633\n",
      "Epoch 1 Batch 201 Loss 0.8927\n",
      "Epoch 1 Batch 202 Loss 0.7638\n",
      "Epoch 1 Batch 203 Loss 0.8838\n",
      "Epoch 1 Batch 204 Loss 0.8757\n",
      "Epoch 1 Batch 205 Loss 0.7179\n",
      "Epoch 1 Batch 206 Loss 0.6886\n",
      "Epoch 1 Batch 207 Loss 0.7553\n",
      "Epoch 1 Batch 208 Loss 0.6984\n",
      "Epoch 1 Batch 209 Loss 0.6617\n",
      "Epoch 1 Batch 210 Loss 0.7949\n",
      "Epoch 1 Batch 211 Loss 0.8301\n",
      "Epoch 1 Batch 212 Loss 1.1440\n",
      "Epoch 1 Batch 213 Loss 0.8130\n",
      "Epoch 1 Batch 214 Loss 0.8083\n",
      "Epoch 1 Batch 215 Loss 0.8642\n",
      "Epoch 1 Batch 216 Loss 0.8368\n",
      "Epoch 1 Batch 217 Loss 1.0362\n",
      "Epoch 1 Batch 218 Loss 0.8102\n",
      "Epoch 1 Batch 219 Loss 0.7489\n",
      "Epoch 1 Batch 220 Loss 0.9600\n",
      "Epoch 1 Batch 221 Loss 0.8343\n",
      "Epoch 1 Batch 222 Loss 0.7323\n",
      "Epoch 1 Batch 223 Loss 0.8669\n",
      "Epoch 1 Batch 224 Loss 0.6968\n",
      "Epoch 1 Batch 225 Loss 0.6927\n",
      "Epoch 1 Batch 226 Loss 0.8991\n",
      "Epoch 1 Batch 227 Loss 0.8231\n",
      "Epoch 1 Batch 228 Loss 0.8691\n",
      "Epoch 1 Batch 229 Loss 1.0585\n",
      "Epoch 1 Batch 230 Loss 0.9178\n",
      "Epoch 1 Batch 231 Loss 0.9943\n",
      "Epoch 1 Batch 232 Loss 0.7461\n",
      "Epoch 1 Batch 233 Loss 0.9462\n",
      "Epoch 1 Batch 234 Loss 0.8203\n",
      "Epoch 1 Batch 235 Loss 0.5777\n",
      "Epoch 1 Batch 236 Loss 0.7448\n",
      "Epoch 1 Batch 237 Loss 0.7531\n",
      "Epoch 1 Batch 238 Loss 0.6663\n",
      "Epoch 1 Batch 239 Loss 0.7885\n",
      "Epoch 1 Batch 240 Loss 1.1623\n",
      "Epoch 1 Batch 241 Loss 1.0536\n",
      "Epoch 1 Batch 242 Loss 0.9102\n",
      "Epoch 1 Batch 243 Loss 0.8694\n",
      "Epoch 1 Batch 244 Loss 0.8620\n",
      "Epoch 1 Batch 245 Loss 0.7747\n",
      "Epoch 1 Batch 246 Loss 0.9815\n",
      "Epoch 1 Batch 247 Loss 0.9228\n",
      "Epoch 1 Batch 248 Loss 0.7319\n",
      "Epoch 1 Batch 249 Loss 0.8666\n",
      "Epoch 1 Batch 250 Loss 0.7354\n",
      "Epoch 1 Batch 251 Loss 0.8507\n",
      "Epoch 1 Batch 252 Loss 1.1031\n",
      "Epoch 1 Batch 253 Loss 1.0111\n",
      "Epoch 1 Batch 254 Loss 0.8009\n",
      "Epoch 1 Batch 255 Loss 0.9399\n",
      "Epoch 1 Batch 256 Loss 0.8799\n",
      "Epoch 1 Batch 257 Loss 1.2049\n",
      "Epoch 1 Batch 258 Loss 0.7668\n",
      "Epoch 1 Batch 259 Loss 0.8762\n",
      "Epoch 1 Batch 260 Loss 1.0389\n",
      "Epoch 1 Batch 261 Loss 0.9819\n",
      "Epoch 1 Batch 262 Loss 1.1001\n",
      "Epoch 1 Batch 263 Loss 0.9283\n",
      "Epoch 1 Batch 264 Loss 0.8723\n",
      "Epoch 1 Batch 265 Loss 0.8929\n",
      "Epoch 1 Batch 266 Loss 1.0544\n",
      "Epoch 1 Batch 267 Loss 0.9320\n",
      "Epoch 1 Batch 268 Loss 0.8609\n",
      "Epoch 1 Batch 269 Loss 0.7015\n",
      "Epoch 1 Batch 270 Loss 0.8220\n",
      "Epoch 1 Batch 271 Loss 1.0681\n",
      "Epoch 1 Batch 272 Loss 0.8024\n",
      "Epoch 1 Batch 273 Loss 0.8174\n",
      "Epoch 1 Batch 274 Loss 0.8896\n",
      "Epoch 1 Batch 275 Loss 0.8378\n",
      "Epoch 1 Batch 276 Loss 0.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 277 Loss 0.9510\n",
      "Epoch 1 Batch 278 Loss 1.0159\n",
      "Epoch 1 Batch 279 Loss 0.7384\n",
      "Epoch 1 Batch 280 Loss 0.9928\n",
      "Epoch 1 Batch 281 Loss 0.7907\n",
      "Epoch 1 Batch 282 Loss 1.0060\n",
      "Epoch 1 Batch 283 Loss 1.0430\n",
      "Epoch 1 Batch 284 Loss 0.8651\n",
      "Epoch 1 Batch 285 Loss 0.8046\n",
      "Epoch 1 Batch 286 Loss 1.0006\n",
      "Epoch 1 Batch 287 Loss 0.6978\n",
      "Epoch 1 Batch 288 Loss 1.0299\n",
      "Epoch 1 Batch 289 Loss 0.8078\n",
      "Epoch 1 Batch 290 Loss 0.7376\n",
      "Epoch 1 Batch 291 Loss 0.9218\n",
      "Epoch 1 Batch 292 Loss 0.8474\n",
      "Epoch 1 Batch 293 Loss 0.7635\n",
      "Epoch 1 Batch 294 Loss 0.9600\n",
      "Epoch 1 Batch 295 Loss 0.8271\n",
      "Epoch 1 Batch 296 Loss 0.9536\n",
      "Epoch 1 Batch 297 Loss 0.9197\n",
      "Epoch 1 Batch 298 Loss 0.9641\n",
      "Epoch 1 Batch 299 Loss 0.7884\n",
      "Epoch 1 Batch 300 Loss 0.6760\n",
      "Epoch 1 Batch 301 Loss 0.6597\n",
      "Epoch 1 Batch 302 Loss 0.7773\n",
      "Epoch 1 Batch 303 Loss 0.8068\n",
      "Epoch 1 Batch 304 Loss 0.8664\n",
      "Epoch 1 Batch 305 Loss 0.9372\n",
      "Epoch 1 Batch 306 Loss 0.8518\n",
      "Epoch 1 Batch 307 Loss 0.7012\n",
      "Epoch 1 Batch 308 Loss 0.9305\n",
      "Epoch 1 Batch 309 Loss 1.0776\n",
      "Epoch 1 Batch 310 Loss 1.0056\n",
      "Epoch 1 Batch 311 Loss 0.9702\n",
      "Epoch 1 Batch 312 Loss 1.0346\n",
      "Epoch 1 Batch 313 Loss 1.0224\n",
      "Epoch 1 Batch 314 Loss 0.8587\n",
      "Epoch 1 Batch 315 Loss 0.5474\n",
      "Epoch 1 Batch 316 Loss 0.7992\n",
      "Epoch 1 Batch 317 Loss 1.0246\n",
      "Epoch 1 Batch 318 Loss 0.7908\n",
      "Epoch 1 Batch 319 Loss 0.9221\n",
      "Epoch 1 Batch 320 Loss 0.7541\n",
      "Epoch 1 Batch 321 Loss 1.0983\n",
      "Epoch 1 Batch 322 Loss 0.7820\n",
      "Epoch 1 Batch 323 Loss 0.6935\n",
      "Epoch 1 Batch 324 Loss 0.8910\n",
      "Epoch 1 Batch 325 Loss 0.6434\n",
      "Epoch 1 Batch 326 Loss 0.8285\n",
      "Epoch 1 Batch 327 Loss 0.9888\n",
      "Epoch 1 Batch 328 Loss 0.8027\n",
      "Epoch 1 Batch 329 Loss 1.1479\n",
      "Epoch 1 Batch 330 Loss 0.9226\n",
      "Epoch 1 Batch 331 Loss 0.9551\n",
      "Epoch 1 Batch 332 Loss 0.8835\n",
      "Epoch 1 Batch 333 Loss 0.8585\n",
      "Epoch 1 Batch 334 Loss 0.8505\n",
      "Epoch 1 Batch 335 Loss 0.8858\n",
      "Epoch 1 Batch 336 Loss 1.1189\n",
      "Epoch 1 Batch 337 Loss 0.8690\n",
      "Epoch 1 Batch 338 Loss 0.9479\n",
      "Epoch 1 Batch 339 Loss 0.9045\n",
      "Epoch 1 Batch 340 Loss 0.9635\n",
      "Epoch 1 Batch 341 Loss 0.8444\n",
      "Epoch 1 Batch 342 Loss 0.7149\n",
      "Epoch 1 Batch 343 Loss 1.0269\n",
      "Epoch 1 Batch 344 Loss 1.0334\n",
      "Epoch 1 Batch 345 Loss 0.8335\n",
      "Epoch 1 Batch 346 Loss 0.9261\n",
      "Epoch 1 Batch 347 Loss 0.8434\n",
      "Epoch 1 Batch 348 Loss 0.9977\n",
      "Epoch 1 Batch 349 Loss 0.9193\n",
      "Epoch 1 Batch 350 Loss 0.8745\n",
      "Epoch 1 Batch 351 Loss 1.1261\n",
      "Epoch 1 Batch 352 Loss 0.8155\n",
      "Epoch 1 Batch 353 Loss 0.8336\n",
      "Epoch 1 Batch 354 Loss 0.7682\n",
      "Epoch 1 Batch 355 Loss 0.7470\n",
      "Epoch 1 Batch 356 Loss 0.8873\n",
      "Epoch 1 Batch 357 Loss 0.9424\n",
      "Epoch 1 Batch 358 Loss 0.8862\n",
      "Epoch 1 Batch 359 Loss 0.6680\n",
      "Epoch 1 Batch 360 Loss 0.6552\n",
      "Epoch 1 Batch 361 Loss 0.7806\n",
      "Epoch 1 Batch 362 Loss 0.9170\n",
      "Epoch 1 Batch 363 Loss 0.9550\n",
      "Epoch 1 Batch 364 Loss 0.7932\n",
      "Epoch 1 Batch 365 Loss 0.8875\n",
      "Epoch 1 Batch 366 Loss 0.8457\n",
      "Epoch 1 Batch 367 Loss 0.7931\n",
      "Epoch 1 Batch 368 Loss 0.7184\n",
      "Epoch 1 Batch 369 Loss 1.0478\n",
      "Epoch 1 Batch 370 Loss 0.8167\n",
      "Epoch 1 Batch 371 Loss 0.6749\n",
      "Epoch 1 Batch 372 Loss 0.8811\n",
      "Epoch 1 Batch 373 Loss 1.0249\n",
      "Epoch 1 Batch 374 Loss 1.1113\n",
      "Epoch 1 Batch 375 Loss 0.9002\n",
      "Epoch 1 Batch 376 Loss 0.7078\n",
      "Epoch 1 Batch 377 Loss 1.1229\n",
      "Epoch 1 Batch 378 Loss 0.7365\n",
      "Epoch 1 Batch 379 Loss 0.9967\n",
      "Epoch 1 Batch 380 Loss 0.8470\n",
      "Epoch 1 Batch 381 Loss 0.6988\n",
      "Epoch 1 Batch 382 Loss 0.9303\n",
      "Epoch 1 Batch 383 Loss 0.8516\n",
      "Epoch 1 Batch 384 Loss 0.7998\n",
      "Epoch 1 Batch 385 Loss 0.8136\n",
      "Epoch 1 Batch 386 Loss 0.7738\n",
      "Epoch 1 Batch 387 Loss 0.9168\n",
      "Epoch 1 Batch 388 Loss 0.9681\n",
      "Epoch 1 Batch 389 Loss 0.9632\n",
      "Epoch 1 Batch 390 Loss 0.8798\n",
      "Epoch 1 Batch 391 Loss 0.8260\n",
      "Epoch 1 Batch 392 Loss 1.1215\n",
      "Epoch 1 Batch 393 Loss 1.0518\n",
      "Epoch 1 Batch 394 Loss 0.7365\n",
      "Epoch 1 Batch 395 Loss 1.1917\n",
      "Epoch 1 Batch 396 Loss 0.8198\n",
      "Epoch 1 Batch 397 Loss 0.8995\n",
      "Epoch 1 Batch 398 Loss 0.8751\n",
      "Epoch 1 Batch 399 Loss 0.9028\n",
      "Epoch 1 Batch 400 Loss 0.8159\n",
      "Epoch 1 Batch 401 Loss 0.9804\n",
      "Epoch 1 Batch 402 Loss 0.7947\n",
      "Epoch 1 Batch 403 Loss 0.8497\n",
      "Epoch 1 Batch 404 Loss 0.8392\n",
      "Epoch 1 Batch 405 Loss 0.9710\n",
      "Epoch 1 Batch 406 Loss 0.7419\n",
      "Epoch 1 Batch 407 Loss 0.9683\n",
      "Epoch 1 Batch 408 Loss 0.8012\n",
      "Epoch 1 Batch 409 Loss 0.8309\n",
      "Epoch 1 Batch 410 Loss 0.5833\n",
      "Epoch 1 Batch 411 Loss 1.0090\n",
      "Epoch 1 Batch 412 Loss 0.8694\n",
      "Epoch 1 Batch 413 Loss 0.9596\n",
      "Epoch 1 Batch 414 Loss 0.6594\n",
      "Epoch 1 Batch 415 Loss 0.7868\n",
      "Epoch 1 Batch 416 Loss 1.0496\n",
      "Epoch 1 Batch 417 Loss 0.7302\n",
      "Epoch 1 Batch 418 Loss 1.0027\n",
      "Epoch 1 Batch 419 Loss 1.0464\n",
      "Epoch 1 Batch 420 Loss 0.7826\n",
      "Epoch 1 Batch 421 Loss 0.9759\n",
      "Epoch 1 Batch 422 Loss 0.7520\n",
      "Epoch 1 Batch 423 Loss 0.8660\n",
      "Epoch 1 Batch 424 Loss 0.7893\n",
      "Epoch 1 Batch 425 Loss 0.8187\n",
      "Epoch 1 Batch 426 Loss 0.7318\n",
      "Epoch 1 Batch 427 Loss 0.7718\n",
      "Epoch 1 Batch 428 Loss 0.8322\n",
      "Epoch 1 Batch 429 Loss 0.7400\n",
      "Epoch 1 Batch 430 Loss 0.7800\n",
      "Epoch 1 Batch 431 Loss 0.8122\n",
      "Epoch 1 Batch 432 Loss 0.8606\n",
      "Epoch 1 Batch 433 Loss 0.5695\n",
      "Epoch 1 Batch 434 Loss 0.8469\n",
      "Epoch 1 Batch 435 Loss 0.8392\n",
      "Epoch 1 Batch 436 Loss 0.8880\n",
      "Epoch 1 Batch 437 Loss 1.0256\n",
      "Epoch 1 Batch 438 Loss 0.8359\n",
      "Epoch 1 Batch 439 Loss 0.7088\n",
      "Epoch 1 Batch 440 Loss 0.9418\n",
      "Epoch 1 Batch 441 Loss 1.1709\n",
      "Epoch 1 Batch 442 Loss 0.9481\n",
      "Epoch 1 Batch 443 Loss 0.9073\n",
      "Epoch 1 Batch 444 Loss 0.9370\n",
      "Epoch 1 Batch 445 Loss 0.7615\n",
      "Epoch 1 Batch 446 Loss 0.8270\n",
      "Epoch 1 Batch 447 Loss 0.9193\n",
      "Epoch 1 Batch 448 Loss 0.8912\n",
      "Epoch 1 Batch 449 Loss 0.8886\n",
      "Epoch 1 Batch 450 Loss 0.8846\n",
      "Epoch 1 Batch 451 Loss 0.9082\n",
      "Epoch 1 Batch 452 Loss 1.1536\n",
      "Epoch 1 Batch 453 Loss 0.9844\n",
      "Epoch 1 Batch 454 Loss 0.9359\n",
      "Epoch 1 Batch 455 Loss 0.7857\n",
      "Epoch 1 Batch 456 Loss 1.0749\n",
      "Epoch 1 Batch 457 Loss 0.8256\n",
      "Epoch 1 Batch 458 Loss 0.9192\n",
      "Epoch 1 Batch 459 Loss 0.7330\n",
      "Epoch 1 Batch 460 Loss 0.8407\n",
      "Epoch 1 Batch 461 Loss 0.8142\n",
      "Epoch 1 Batch 462 Loss 1.1327\n",
      "Epoch 1 Batch 463 Loss 0.8737\n",
      "Epoch 1 Batch 464 Loss 0.8811\n",
      "Epoch 1 Batch 465 Loss 0.8497\n",
      "Epoch 1 Batch 466 Loss 1.0836\n",
      "Epoch 1 Batch 467 Loss 0.9734\n",
      "Epoch 1 Batch 468 Loss 0.8946\n",
      "Epoch 1 Batch 469 Loss 0.9408\n",
      "Epoch 1 Batch 470 Loss 1.1585\n",
      "Epoch 1 Batch 471 Loss 0.6931\n",
      "Epoch 1 Batch 472 Loss 0.9971\n",
      "Epoch 1 Batch 473 Loss 0.6610\n",
      "Epoch 1 Batch 474 Loss 0.9615\n",
      "Epoch 1 Batch 475 Loss 0.7749\n",
      "Epoch 1 Batch 476 Loss 1.0176\n",
      "Epoch 1 Batch 477 Loss 0.7782\n",
      "Epoch 1 Batch 478 Loss 1.0474\n",
      "Epoch 1 Batch 479 Loss 0.8721\n",
      "Epoch 1 Batch 480 Loss 0.8901\n",
      "Epoch 1 Batch 481 Loss 1.0484\n",
      "Epoch 1 Batch 482 Loss 1.2581\n",
      "Epoch 1 Batch 483 Loss 1.1477\n",
      "Epoch 1 Batch 484 Loss 1.0045\n",
      "Epoch 1 Batch 485 Loss 0.9558\n",
      "Epoch 1 Batch 486 Loss 0.8665\n",
      "Epoch 1 Batch 487 Loss 0.8074\n",
      "Epoch 1 Batch 488 Loss 1.1101\n",
      "Epoch 1 Batch 489 Loss 0.9488\n",
      "Epoch 1 Batch 490 Loss 0.9919\n",
      "Epoch 1 Batch 491 Loss 0.8467\n",
      "Epoch 1 Batch 492 Loss 0.7651\n",
      "Epoch 1 Batch 493 Loss 0.9353\n",
      "Epoch 1 Batch 494 Loss 1.0747\n",
      "Epoch 1 Batch 495 Loss 1.0282\n",
      "Epoch 1 Batch 496 Loss 0.7809\n",
      "Epoch 1 Batch 497 Loss 0.8915\n",
      "Epoch 1 Batch 498 Loss 0.8834\n",
      "Epoch 1 Batch 499 Loss 0.8759\n",
      "Epoch 1 Batch 500 Loss 1.0646\n",
      "Epoch 1 Batch 501 Loss 0.8866\n",
      "Epoch 1 Batch 502 Loss 0.8980\n",
      "Epoch 1 Batch 503 Loss 0.7604\n",
      "Epoch 1 Batch 504 Loss 0.8755\n",
      "Epoch 1 Batch 505 Loss 1.1279\n",
      "Epoch 1 Batch 506 Loss 0.7754\n",
      "Epoch 1 Batch 507 Loss 0.9667\n",
      "Epoch 1 Batch 508 Loss 0.8916\n",
      "Epoch 1 Batch 509 Loss 1.0859\n",
      "Epoch 1 Batch 510 Loss 0.9018\n",
      "Epoch 1 Batch 511 Loss 1.0819\n",
      "Epoch 1 Batch 512 Loss 0.6663\n",
      "Epoch 1 Batch 513 Loss 0.8131\n",
      "Epoch 1 Batch 514 Loss 0.9493\n",
      "Epoch 1 Batch 515 Loss 0.7433\n",
      "Epoch 1 Batch 516 Loss 0.9208\n",
      "Epoch 1 Batch 517 Loss 0.9882\n",
      "Epoch 1 Batch 518 Loss 0.8226\n",
      "Epoch 1 Batch 519 Loss 0.9028\n",
      "Epoch 1 Batch 520 Loss 0.6881\n",
      "Epoch 1 Batch 521 Loss 0.9510\n",
      "Epoch 1 Batch 522 Loss 0.9964\n",
      "Epoch 1 Batch 523 Loss 0.8488\n",
      "Epoch 1 Batch 524 Loss 1.0950\n",
      "Epoch 1 Batch 525 Loss 0.8879\n",
      "Epoch 1 Batch 526 Loss 0.9089\n",
      "Epoch 1 Batch 527 Loss 0.8268\n",
      "Epoch 1 Batch 528 Loss 0.7887\n",
      "Epoch 1 Batch 529 Loss 0.7988\n",
      "Epoch 1 Batch 530 Loss 1.1233\n",
      "Epoch 1 Batch 531 Loss 0.6274\n",
      "Epoch 1 Batch 532 Loss 0.7228\n",
      "Epoch 1 Batch 533 Loss 0.8231\n",
      "Epoch 1 Batch 534 Loss 0.8565\n",
      "Epoch 1 Batch 535 Loss 1.0578\n",
      "Epoch 1 Batch 536 Loss 1.0171\n",
      "Epoch 1 Batch 537 Loss 0.7402\n",
      "Epoch 1 Batch 538 Loss 0.9949\n",
      "Epoch 1 Batch 539 Loss 0.7476\n",
      "Epoch 1 Batch 540 Loss 0.9652\n",
      "Epoch 1 Batch 541 Loss 0.9711\n",
      "Epoch 1 Batch 542 Loss 0.5726\n",
      "Epoch 1 Batch 543 Loss 1.1286\n",
      "Epoch 1 Batch 544 Loss 0.6999\n",
      "Epoch 1 Batch 545 Loss 0.8417\n",
      "Epoch 1 Batch 546 Loss 0.8593\n",
      "Epoch 1 Batch 547 Loss 0.7503\n",
      "Epoch 1 Batch 548 Loss 0.9032\n",
      "Epoch 1 Batch 549 Loss 1.0080\n",
      "Epoch 1 Batch 550 Loss 0.7586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 551 Loss 0.9388\n",
      "Epoch 1 Batch 552 Loss 0.6740\n",
      "Epoch 1 Batch 553 Loss 0.8205\n",
      "Epoch 1 Batch 554 Loss 0.8150\n",
      "Epoch 1 Batch 555 Loss 0.7889\n",
      "Epoch 1 Batch 556 Loss 0.9143\n",
      "Epoch 1 Batch 557 Loss 0.8073\n",
      "Epoch 1 Batch 558 Loss 0.8273\n",
      "Epoch 1 Batch 559 Loss 0.7772\n",
      "Epoch 1 Batch 560 Loss 0.8171\n",
      "Epoch 1 Batch 561 Loss 0.8687\n",
      "Epoch 1 Batch 562 Loss 0.9253\n",
      "Epoch 1 Batch 563 Loss 0.9018\n",
      "Epoch 1 Batch 564 Loss 0.8203\n",
      "Epoch 1 Batch 565 Loss 0.9274\n",
      "Epoch 1 Batch 566 Loss 1.0673\n",
      "Epoch 1 Batch 567 Loss 0.9125\n",
      "Epoch 1 Batch 568 Loss 1.1722\n",
      "Epoch 1 Batch 569 Loss 1.0352\n",
      "Epoch 1 Batch 570 Loss 0.8585\n",
      "Epoch 1 Batch 571 Loss 0.8840\n",
      "Epoch 1 Batch 572 Loss 0.8432\n",
      "Epoch 1 Batch 573 Loss 0.9771\n",
      "Epoch 1 Batch 574 Loss 0.9469\n",
      "Epoch 1 Batch 575 Loss 0.8176\n",
      "Epoch 1 Batch 576 Loss 1.1008\n",
      "Epoch 1 Batch 577 Loss 0.8849\n",
      "Epoch 1 Batch 578 Loss 0.8796\n",
      "Epoch 1 Batch 579 Loss 0.9519\n",
      "Epoch 1 Batch 580 Loss 0.9949\n",
      "Epoch 1 Batch 581 Loss 1.0187\n",
      "Epoch 1 Batch 582 Loss 0.9292\n",
      "Epoch 1 Batch 583 Loss 0.8192\n",
      "Epoch 1 Batch 584 Loss 0.7170\n",
      "Epoch 1 Batch 585 Loss 0.7010\n",
      "Epoch 1 Batch 586 Loss 0.6418\n",
      "Epoch 1 Batch 587 Loss 1.1674\n",
      "Epoch 1 Batch 588 Loss 0.8181\n",
      "Epoch 1 Batch 589 Loss 0.8295\n",
      "Epoch 1 Batch 590 Loss 0.9437\n",
      "Epoch 1 Batch 591 Loss 0.7749\n",
      "Epoch 1 Batch 592 Loss 0.8224\n",
      "Epoch 1 Batch 593 Loss 0.8037\n",
      "Epoch 1 Batch 594 Loss 0.9189\n",
      "Epoch 1 Batch 595 Loss 0.9580\n",
      "Epoch 1 Batch 596 Loss 0.8724\n",
      "Epoch 1 Batch 597 Loss 0.6941\n",
      "Epoch 1 Batch 598 Loss 0.7877\n",
      "Epoch 1 Batch 599 Loss 0.8719\n",
      "Epoch 1 Batch 600 Loss 0.8245\n",
      "Epoch 1 Batch 601 Loss 0.8462\n",
      "Epoch 1 Batch 602 Loss 1.0213\n",
      "Epoch 1 Batch 603 Loss 1.0735\n",
      "Epoch 1 Batch 604 Loss 0.8142\n",
      "Epoch 1 Batch 605 Loss 0.9883\n",
      "Epoch 1 Batch 606 Loss 1.0223\n",
      "Epoch 1 Batch 607 Loss 0.8193\n",
      "Epoch 1 Batch 608 Loss 0.8067\n",
      "Epoch 1 Batch 609 Loss 0.8187\n",
      "Epoch 1 Batch 610 Loss 0.8647\n",
      "Epoch 1 Batch 611 Loss 0.9431\n",
      "Epoch 1 Batch 612 Loss 1.2521\n",
      "Epoch 1 Batch 613 Loss 0.8355\n",
      "Epoch 1 Batch 614 Loss 0.7436\n",
      "Epoch 1 Batch 615 Loss 0.8198\n",
      "Epoch 1 Batch 616 Loss 0.8665\n",
      "Epoch 1 Batch 617 Loss 1.0363\n",
      "Epoch 1 Batch 618 Loss 1.0369\n",
      "Epoch 1 Batch 619 Loss 1.0022\n",
      "Epoch 1 Batch 620 Loss 0.8984\n",
      "Epoch 1 Batch 621 Loss 1.0294\n",
      "Epoch 1 Batch 622 Loss 0.9691\n",
      "Epoch 1 Batch 623 Loss 0.7193\n",
      "Epoch 1 Batch 624 Loss 1.0728\n",
      "Epoch 1 Batch 625 Loss 0.7714\n",
      "Epoch 1 Batch 626 Loss 0.8274\n",
      "Epoch 1 Batch 627 Loss 0.9446\n",
      "Epoch 1 Batch 628 Loss 1.0103\n",
      "Epoch 1 Batch 629 Loss 0.8441\n",
      "Epoch 1 Batch 630 Loss 0.9797\n",
      "Epoch 1 Batch 631 Loss 0.8242\n",
      "Epoch 1 Batch 632 Loss 0.8753\n",
      "Epoch 1 Batch 633 Loss 0.9745\n",
      "Epoch 1 Batch 634 Loss 0.9247\n",
      "Epoch 1 Batch 635 Loss 0.7448\n",
      "Epoch 1 Batch 636 Loss 0.8186\n",
      "Epoch 1 Batch 637 Loss 0.8071\n",
      "Epoch 1 Batch 638 Loss 0.8334\n",
      "Epoch 1 Batch 639 Loss 0.8045\n",
      "Epoch 1 Batch 640 Loss 0.7866\n",
      "Epoch 1 Batch 641 Loss 0.7973\n",
      "Epoch 1 Batch 642 Loss 0.8507\n",
      "Epoch 1 Batch 643 Loss 0.6601\n",
      "Epoch 1 Batch 644 Loss 0.9993\n",
      "Epoch 1 Batch 645 Loss 0.8408\n",
      "Epoch 1 Batch 646 Loss 1.1088\n",
      "Epoch 1 Batch 647 Loss 0.7781\n",
      "Epoch 1 Batch 648 Loss 0.9438\n",
      "Epoch 1 Batch 649 Loss 1.1691\n",
      "Epoch 1 Batch 650 Loss 0.7688\n",
      "Epoch 1 Batch 651 Loss 0.9644\n",
      "Epoch 1 Batch 652 Loss 0.9851\n",
      "Epoch 1 Batch 653 Loss 0.9644\n",
      "Epoch 1 Batch 654 Loss 0.8529\n",
      "Epoch 1 Batch 655 Loss 0.8467\n",
      "Epoch 1 Batch 656 Loss 0.9474\n",
      "Epoch 1 Batch 657 Loss 1.0075\n",
      "Epoch 1 Batch 658 Loss 1.0018\n",
      "Epoch 1 Batch 659 Loss 0.7631\n",
      "Epoch 1 Batch 660 Loss 1.0415\n",
      "Epoch 1 Batch 661 Loss 0.8016\n",
      "Epoch 1 Batch 662 Loss 0.8702\n",
      "Epoch 1 Batch 663 Loss 0.8466\n",
      "Epoch 1 Batch 664 Loss 1.0646\n",
      "Epoch 1 Batch 665 Loss 0.7902\n",
      "Epoch 1 Batch 666 Loss 0.7740\n",
      "Epoch 1 Batch 667 Loss 1.0070\n",
      "Epoch 1 Batch 668 Loss 0.8203\n",
      "Epoch 1 Batch 669 Loss 0.9888\n",
      "Epoch 1 Batch 670 Loss 0.9667\n",
      "Epoch 1 Batch 671 Loss 0.8000\n",
      "Epoch 1 Batch 672 Loss 0.7291\n",
      "Epoch 1 Batch 673 Loss 0.9485\n",
      "Epoch 1 Batch 674 Loss 0.9409\n",
      "Epoch 1 Batch 675 Loss 0.8063\n",
      "Epoch 1 Batch 676 Loss 1.1069\n",
      "Epoch 1 Batch 677 Loss 1.1357\n",
      "Epoch 1 Batch 678 Loss 0.9268\n",
      "Epoch 1 Batch 679 Loss 0.7167\n",
      "Epoch 1 Batch 680 Loss 0.9684\n",
      "Epoch 1 Batch 681 Loss 0.9521\n",
      "Epoch 1 Batch 682 Loss 0.8766\n",
      "Epoch 1 Batch 683 Loss 0.7253\n",
      "Epoch 1 Batch 684 Loss 0.8827\n",
      "Epoch 1 Batch 685 Loss 0.9962\n",
      "Epoch 1 Batch 686 Loss 1.0050\n",
      "Epoch 1 Batch 687 Loss 1.0000\n",
      "Epoch 1 Batch 688 Loss 0.6585\n",
      "Epoch 1 Batch 689 Loss 0.8930\n",
      "Epoch 1 Batch 690 Loss 1.0338\n",
      "Epoch 1 Batch 691 Loss 0.9956\n",
      "Epoch 1 Batch 692 Loss 1.0670\n",
      "Epoch 1 Batch 693 Loss 0.9114\n",
      "Epoch 1 Batch 694 Loss 0.7225\n",
      "Epoch 1 Batch 695 Loss 0.9260\n",
      "Epoch 1 Batch 696 Loss 0.8240\n",
      "Epoch 1 Batch 697 Loss 0.9794\n",
      "Epoch 1 Batch 698 Loss 1.0725\n",
      "Epoch 1 Batch 699 Loss 1.0937\n",
      "Epoch 1 Batch 700 Loss 0.9483\n",
      "Epoch 1 Batch 701 Loss 0.8538\n",
      "Epoch 1 Batch 702 Loss 0.9772\n",
      "Epoch 1 Batch 703 Loss 0.9187\n",
      "Epoch 1 Batch 704 Loss 1.2138\n",
      "Epoch 1 Batch 705 Loss 0.8475\n",
      "Epoch 1 Batch 706 Loss 0.7476\n",
      "Epoch 1 Batch 707 Loss 0.7424\n",
      "Epoch 1 Batch 708 Loss 0.7527\n",
      "Epoch 1 Batch 709 Loss 0.8526\n",
      "Epoch 1 Batch 710 Loss 0.7805\n",
      "Epoch 1 Batch 711 Loss 0.7784\n",
      "Epoch 1 Batch 712 Loss 0.8217\n",
      "Epoch 1 Batch 713 Loss 1.1718\n",
      "Epoch 1 Batch 714 Loss 0.8269\n",
      "Epoch 1 Batch 715 Loss 0.9319\n",
      "Epoch 1 Batch 716 Loss 0.9262\n",
      "Epoch 1 Batch 717 Loss 0.7311\n",
      "Epoch 1 Batch 718 Loss 1.0443\n",
      "Epoch 1 Batch 719 Loss 0.8779\n",
      "Epoch 1 Batch 720 Loss 0.8666\n",
      "Epoch 1 Batch 721 Loss 1.0576\n",
      "Epoch 1 Batch 722 Loss 0.9538\n",
      "Epoch 1 Batch 723 Loss 0.6907\n",
      "Epoch 1 Batch 724 Loss 0.8090\n",
      "Epoch 1 Batch 725 Loss 0.9563\n",
      "Epoch 1 Batch 726 Loss 0.9567\n",
      "Epoch 1 Batch 727 Loss 0.9243\n",
      "Epoch 1 Batch 728 Loss 0.8051\n",
      "Epoch 1 Batch 729 Loss 0.9839\n",
      "Epoch 1 Batch 730 Loss 0.9971\n",
      "Epoch 1 Batch 731 Loss 0.9501\n",
      "Epoch 1 Batch 732 Loss 0.7854\n",
      "Epoch 1 Batch 733 Loss 0.7565\n",
      "Epoch 1 Batch 734 Loss 1.0994\n",
      "Epoch 1 Batch 735 Loss 0.8034\n",
      "Epoch 1 Batch 736 Loss 0.8175\n",
      "Epoch 1 Batch 737 Loss 0.9737\n",
      "Epoch 1 Batch 738 Loss 0.8147\n",
      "Epoch 1 Batch 739 Loss 0.7533\n",
      "Epoch 1 Batch 740 Loss 1.0171\n",
      "Epoch 1 Batch 741 Loss 0.7170\n",
      "Epoch 1 Batch 742 Loss 0.8282\n",
      "Epoch 1 Batch 743 Loss 0.8016\n",
      "Epoch 1 Batch 744 Loss 1.0351\n",
      "Epoch 1 Batch 745 Loss 0.9614\n",
      "Epoch 1 Batch 746 Loss 0.8701\n",
      "Epoch 1 Batch 747 Loss 1.1057\n",
      "Epoch 1 Batch 748 Loss 0.9332\n",
      "Epoch 1 Batch 749 Loss 0.5635\n",
      "Epoch 1 Batch 750 Loss 0.9023\n",
      "Epoch 1 Batch 751 Loss 0.7351\n",
      "Epoch 1 Batch 752 Loss 0.8539\n",
      "Epoch 1 Batch 753 Loss 0.6862\n",
      "Epoch 1 Batch 754 Loss 0.9074\n",
      "Epoch 1 Batch 755 Loss 0.9360\n",
      "Epoch 1 Batch 756 Loss 0.9791\n",
      "Epoch 1 Batch 757 Loss 1.0514\n",
      "Epoch 1 Batch 758 Loss 0.9124\n",
      "Epoch 1 Batch 759 Loss 0.9078\n",
      "Epoch 1 Batch 760 Loss 0.9093\n",
      "Epoch 1 Batch 761 Loss 1.1545\n",
      "Epoch 1 Batch 762 Loss 1.0325\n",
      "Epoch 1 Batch 763 Loss 0.7376\n",
      "Epoch 1 Batch 764 Loss 1.1588\n",
      "Epoch 1 Batch 765 Loss 0.9594\n",
      "Epoch 1 Batch 766 Loss 0.8261\n",
      "Epoch 1 Batch 767 Loss 1.2049\n",
      "Epoch 1 Batch 768 Loss 0.7180\n",
      "Epoch 1 Batch 769 Loss 0.8540\n",
      "Epoch 1 Batch 770 Loss 0.8733\n",
      "Epoch 1 Batch 771 Loss 0.8152\n",
      "Epoch 1 Batch 772 Loss 1.0088\n",
      "Epoch 1 Batch 773 Loss 1.1577\n",
      "Epoch 1 Batch 774 Loss 0.8025\n",
      "Epoch 1 Batch 775 Loss 0.8755\n",
      "Epoch 1 Batch 776 Loss 0.9191\n",
      "Epoch 1 Batch 777 Loss 0.9605\n",
      "Epoch 1 Batch 778 Loss 0.9125\n",
      "Epoch 1 Batch 779 Loss 0.8588\n",
      "Epoch 1 Batch 780 Loss 0.7031\n",
      "Epoch 1 Batch 781 Loss 0.9459\n",
      "Epoch 1 Batch 782 Loss 0.8556\n",
      "Epoch 1 Batch 783 Loss 0.9056\n",
      "Epoch 1 Batch 784 Loss 1.0379\n",
      "Epoch 1 Batch 785 Loss 0.8399\n",
      "Epoch 1 Batch 786 Loss 0.9522\n",
      "Epoch 1 Batch 787 Loss 0.9563\n",
      "Epoch 1 Batch 788 Loss 0.8572\n",
      "Epoch 1 Batch 789 Loss 1.0098\n",
      "Epoch 1 Batch 790 Loss 0.8827\n",
      "Epoch 1 Batch 791 Loss 1.0832\n",
      "Epoch 1 Batch 792 Loss 1.0649\n",
      "Epoch 1 Batch 793 Loss 0.8223\n",
      "Epoch 1 Batch 794 Loss 0.8704\n",
      "Epoch 1 Batch 795 Loss 1.1361\n",
      "Epoch 1 Batch 796 Loss 0.8690\n",
      "Epoch 1 Batch 797 Loss 0.7382\n",
      "Epoch 1 Batch 798 Loss 0.8734\n",
      "Epoch 1 Batch 799 Loss 0.8474\n",
      "Epoch 1 Batch 800 Loss 1.1189\n",
      "Epoch 1 Batch 801 Loss 0.7653\n",
      "Epoch 1 Batch 802 Loss 0.7527\n",
      "Epoch 1 Batch 803 Loss 0.8245\n",
      "Epoch 1 Batch 804 Loss 1.0166\n",
      "Epoch 1 Batch 805 Loss 0.9819\n",
      "Epoch 1 Batch 806 Loss 0.9060\n",
      "Epoch 1 Batch 807 Loss 0.7095\n",
      "Epoch 1 Batch 808 Loss 0.8779\n",
      "Epoch 1 Batch 809 Loss 0.7058\n",
      "Epoch 1 Batch 810 Loss 0.8398\n",
      "Epoch 1 Batch 811 Loss 0.8960\n",
      "Epoch 1 Batch 812 Loss 0.9100\n",
      "Epoch 1 Batch 813 Loss 0.8587\n",
      "Epoch 1 Batch 814 Loss 0.8337\n",
      "Epoch 1 Batch 815 Loss 0.9249\n",
      "Epoch 1 Batch 816 Loss 0.9400\n",
      "Epoch 1 Batch 817 Loss 0.8439\n",
      "Epoch 1 Batch 818 Loss 0.7899\n",
      "Epoch 1 Batch 819 Loss 0.9936\n",
      "Epoch 1 Batch 820 Loss 0.8109\n",
      "Epoch 1 Batch 821 Loss 0.8497\n",
      "Epoch 1 Batch 822 Loss 0.9656\n",
      "Epoch 1 Batch 823 Loss 1.0962\n",
      "Epoch 1 Batch 824 Loss 0.8770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 825 Loss 1.1490\n",
      "Epoch 1 Batch 826 Loss 0.7939\n",
      "Epoch 1 Batch 827 Loss 0.9111\n",
      "Epoch 1 Batch 828 Loss 0.9998\n",
      "Epoch 1 Batch 829 Loss 1.0661\n",
      "Epoch 1 Batch 830 Loss 1.0174\n",
      "Epoch 1 Batch 831 Loss 1.1186\n",
      "Epoch 1 Batch 832 Loss 0.9854\n",
      "Epoch 1 Batch 833 Loss 0.9676\n",
      "Epoch 1 Batch 834 Loss 1.0047\n",
      "Epoch 1 Batch 835 Loss 0.7514\n",
      "Epoch 1 Batch 836 Loss 1.0771\n",
      "Epoch 1 Batch 837 Loss 0.8091\n",
      "Epoch 1 Batch 838 Loss 1.0204\n",
      "Epoch 1 Batch 839 Loss 0.8639\n",
      "Epoch 1 Batch 840 Loss 0.9451\n",
      "Epoch 1 Batch 841 Loss 0.8551\n",
      "Epoch 1 Batch 842 Loss 0.9294\n",
      "Epoch 1 Batch 843 Loss 0.7970\n",
      "Epoch 1 Batch 844 Loss 0.8807\n",
      "Epoch 1 Batch 845 Loss 0.8660\n",
      "Epoch 1 Batch 846 Loss 0.9267\n",
      "Epoch 1 Batch 847 Loss 1.0449\n",
      "Epoch 1 Batch 848 Loss 0.7321\n",
      "Epoch 1 Batch 849 Loss 0.7741\n",
      "Epoch 1 Batch 850 Loss 0.9878\n",
      "Epoch 1 Batch 851 Loss 0.9402\n",
      "Epoch 1 Batch 852 Loss 0.8678\n",
      "Epoch 1 Batch 853 Loss 1.0991\n",
      "Epoch 1 Batch 854 Loss 0.9206\n",
      "Epoch 1 Batch 855 Loss 0.9950\n",
      "Epoch 1 Batch 856 Loss 0.9520\n",
      "Epoch 1 Batch 857 Loss 0.7868\n",
      "Epoch 1 Batch 858 Loss 0.8957\n",
      "Epoch 1 Batch 859 Loss 0.9302\n",
      "Epoch 1 Batch 860 Loss 0.8738\n",
      "Epoch 1 Batch 861 Loss 1.1022\n",
      "Epoch 1 Batch 862 Loss 0.8772\n",
      "Epoch 1 Batch 863 Loss 0.9767\n",
      "Epoch 1 Batch 864 Loss 0.7749\n",
      "Epoch 1 Batch 865 Loss 0.9067\n",
      "Epoch 1 Batch 866 Loss 0.8972\n",
      "Epoch 1 Batch 867 Loss 0.9990\n",
      "Epoch 1 Batch 868 Loss 0.6907\n",
      "Epoch 1 Batch 869 Loss 0.9992\n",
      "Epoch 1 Batch 870 Loss 0.8302\n",
      "Epoch 1 Batch 871 Loss 0.7598\n",
      "Epoch 1 Batch 872 Loss 0.9052\n",
      "Epoch 1 Batch 873 Loss 0.8057\n",
      "Epoch 1 Batch 874 Loss 0.8843\n",
      "Epoch 1 Batch 875 Loss 0.8650\n",
      "Epoch 1 Batch 876 Loss 1.1371\n",
      "Epoch 1 Batch 877 Loss 0.9632\n",
      "Epoch 1 Batch 878 Loss 0.9114\n",
      "Epoch 1 Batch 879 Loss 0.6986\n",
      "Epoch 1 Batch 880 Loss 0.8023\n",
      "Epoch 1 Batch 881 Loss 1.2885\n",
      "Epoch 1 Batch 882 Loss 1.1115\n",
      "Epoch 1 Batch 883 Loss 0.8456\n",
      "Epoch 1 Batch 884 Loss 0.9114\n",
      "Epoch 1 Batch 885 Loss 0.9276\n",
      "Epoch 1 Batch 886 Loss 0.9417\n",
      "Epoch 1 Batch 887 Loss 1.0495\n",
      "Epoch 1 Batch 888 Loss 1.1436\n",
      "Epoch 1 Batch 889 Loss 0.8039\n",
      "Epoch 1 Batch 890 Loss 1.0112\n",
      "Epoch 1 Batch 891 Loss 1.0438\n",
      "Epoch 1 Batch 892 Loss 0.8992\n",
      "Epoch 1 Batch 893 Loss 1.1204\n",
      "Epoch 1 Batch 894 Loss 0.9234\n",
      "Epoch 1 Batch 895 Loss 1.0345\n",
      "Epoch 1 Batch 896 Loss 0.9960\n",
      "Epoch 1 Batch 897 Loss 1.0476\n",
      "Epoch 1 Batch 898 Loss 1.0220\n",
      "Epoch 1 Batch 899 Loss 0.8974\n",
      "Epoch 1 Batch 900 Loss 0.9346\n",
      "Epoch 1 Batch 901 Loss 0.7602\n",
      "Epoch 1 Batch 902 Loss 0.9654\n",
      "Epoch 1 Batch 903 Loss 0.9610\n",
      "Epoch 1 Batch 904 Loss 0.8875\n",
      "Epoch 1 Batch 905 Loss 0.8513\n",
      "Epoch 1 Batch 906 Loss 1.0393\n",
      "Epoch 1 Batch 907 Loss 0.8680\n",
      "Epoch 1 Batch 908 Loss 1.0149\n",
      "Epoch 1 Batch 909 Loss 0.8463\n",
      "Epoch 1 Batch 910 Loss 0.8683\n",
      "Epoch 1 Batch 911 Loss 1.0855\n",
      "Epoch 1 Batch 912 Loss 0.9123\n",
      "Epoch 1 Batch 913 Loss 0.8136\n",
      "Epoch 1 Batch 914 Loss 0.9562\n",
      "Epoch 1 Batch 915 Loss 0.9488\n",
      "Epoch 1 Batch 916 Loss 1.1196\n",
      "Epoch 1 Batch 917 Loss 0.9550\n",
      "Epoch 1 Batch 918 Loss 1.0508\n",
      "Epoch 1 Batch 919 Loss 0.7943\n",
      "Epoch 1 Batch 920 Loss 0.9595\n",
      "Epoch 1 Batch 921 Loss 0.7197\n",
      "Epoch 1 Batch 922 Loss 1.0690\n",
      "Epoch 1 Batch 923 Loss 1.0029\n",
      "Epoch 1 Batch 924 Loss 0.8816\n",
      "Epoch 1 Batch 925 Loss 0.7775\n",
      "Epoch 1 Batch 926 Loss 0.9260\n",
      "Epoch 1 Batch 927 Loss 0.9540\n",
      "Epoch 1 Batch 928 Loss 1.0755\n",
      "Epoch 1 Batch 929 Loss 0.8180\n",
      "Epoch 1 Batch 930 Loss 1.0737\n",
      "Epoch 1 Batch 931 Loss 0.9698\n",
      "Epoch 1 Batch 932 Loss 0.9140\n",
      "Epoch 1 Batch 933 Loss 0.8687\n",
      "Epoch 1 Batch 934 Loss 0.8989\n",
      "Epoch 1 Batch 935 Loss 0.8586\n",
      "Epoch 1 Batch 936 Loss 0.9814\n",
      "Epoch 1 Batch 937 Loss 0.9056\n",
      "Epoch 1 Batch 938 Loss 0.9082\n",
      "Epoch 1 Batch 939 Loss 0.7158\n",
      "Epoch 1 Batch 940 Loss 0.8208\n",
      "Epoch 1 Batch 941 Loss 0.6939\n",
      "Epoch 1 Batch 942 Loss 0.8628\n",
      "Epoch 1 Batch 943 Loss 0.9661\n",
      "Epoch 1 Batch 944 Loss 0.8785\n",
      "Epoch 1 Batch 945 Loss 0.7854\n",
      "Epoch 1 Batch 946 Loss 0.7459\n",
      "Epoch 1 Batch 947 Loss 1.0059\n",
      "Epoch 1 Batch 948 Loss 0.8792\n",
      "Epoch 1 Batch 949 Loss 0.9142\n",
      "Epoch 1 Batch 950 Loss 0.9989\n",
      "Epoch 1 Batch 951 Loss 1.0505\n",
      "Epoch 1 Batch 952 Loss 0.9020\n",
      "Epoch 1 Batch 953 Loss 1.2231\n",
      "Epoch 1 Batch 954 Loss 1.1708\n",
      "Epoch 1 Batch 955 Loss 0.7004\n",
      "Epoch 1 Batch 956 Loss 0.8855\n",
      "Epoch 1 Batch 957 Loss 0.7307\n",
      "Epoch 1 Batch 958 Loss 1.0308\n",
      "Epoch 1 Batch 959 Loss 1.0708\n",
      "Epoch 1 Batch 960 Loss 0.8548\n",
      "Epoch 1 Batch 961 Loss 0.7865\n",
      "Epoch 1 Batch 962 Loss 0.9920\n",
      "Epoch 1 Batch 963 Loss 0.8477\n",
      "Epoch 1 Batch 964 Loss 0.7414\n",
      "Epoch 1 Batch 965 Loss 0.9441\n",
      "Epoch 1 Batch 966 Loss 0.8344\n",
      "Epoch 1 Batch 967 Loss 1.1131\n",
      "Epoch 1 Batch 968 Loss 0.7627\n",
      "Epoch 1 Batch 969 Loss 0.8399\n",
      "Epoch 1 Batch 970 Loss 0.7738\n",
      "Epoch 1 Batch 971 Loss 1.0131\n",
      "Epoch 1 Batch 972 Loss 0.8126\n",
      "Epoch 1 Batch 973 Loss 1.0778\n",
      "Epoch 1 Batch 974 Loss 0.6931\n",
      "Epoch 1 Batch 975 Loss 0.9966\n",
      "Epoch 1 Batch 976 Loss 0.7572\n",
      "Epoch 1 Batch 977 Loss 1.1728\n",
      "Epoch 1 Batch 978 Loss 0.9484\n",
      "Epoch 1 Batch 979 Loss 1.0584\n",
      "Epoch 1 Batch 980 Loss 0.8919\n",
      "Epoch 1 Batch 981 Loss 0.7799\n",
      "Epoch 1 Batch 982 Loss 0.9761\n",
      "Epoch 1 Batch 983 Loss 0.7667\n",
      "Epoch 1 Batch 984 Loss 0.8652\n",
      "Epoch 1 Batch 985 Loss 0.8556\n",
      "Epoch 1 Batch 986 Loss 0.8662\n",
      "Epoch 1 Batch 987 Loss 0.8526\n",
      "Epoch 1 Batch 988 Loss 1.0122\n",
      "Epoch 1 Batch 989 Loss 1.0014\n",
      "Epoch 1 Batch 990 Loss 0.9211\n",
      "Epoch 1 Batch 991 Loss 0.8043\n",
      "Epoch 1 Batch 992 Loss 0.7899\n",
      "Epoch 1 Batch 993 Loss 0.8900\n",
      "Epoch 1 Batch 994 Loss 0.7328\n",
      "Epoch 1 Batch 995 Loss 0.9518\n",
      "Epoch 1 Batch 996 Loss 1.0050\n",
      "Epoch 1 Batch 997 Loss 1.0387\n",
      "Epoch 1 Batch 998 Loss 0.8898\n",
      "Epoch 1 Batch 999 Loss 0.8288\n",
      "Epoch 1 Batch 1000 Loss 0.7784\n",
      "Epoch 1 Batch 1001 Loss 0.9857\n",
      "Epoch 1 Batch 1002 Loss 1.1280\n",
      "Epoch 1 Batch 1003 Loss 0.8773\n",
      "Epoch 1 Batch 1004 Loss 0.6566\n",
      "Epoch 1 Batch 1005 Loss 0.9378\n",
      "Epoch 1 Batch 1006 Loss 0.9356\n",
      "Epoch 1 Batch 1007 Loss 0.9029\n",
      "Epoch 1 Batch 1008 Loss 0.9138\n",
      "Epoch 1 Batch 1009 Loss 0.6929\n",
      "Epoch 1 Batch 1010 Loss 0.9787\n",
      "Epoch 1 Batch 1011 Loss 0.7598\n",
      "Epoch 1 Batch 1012 Loss 0.6821\n",
      "Epoch 1 Batch 1013 Loss 0.6886\n",
      "Epoch 1 Batch 1014 Loss 0.7872\n",
      "Epoch 1 Batch 1015 Loss 1.1345\n",
      "Epoch 1 Batch 1016 Loss 0.8692\n",
      "Epoch 1 Batch 1017 Loss 0.7996\n",
      "Epoch 1 Batch 1018 Loss 0.8714\n",
      "Epoch 1 Batch 1019 Loss 0.9313\n",
      "Epoch 1 Batch 1020 Loss 0.8797\n",
      "Epoch 1 Batch 1021 Loss 0.9031\n",
      "Epoch 1 Batch 1022 Loss 0.8219\n",
      "Epoch 1 Batch 1023 Loss 0.7771\n",
      "Epoch 1 Batch 1024 Loss 1.2006\n",
      "Epoch 1 Batch 1025 Loss 1.1009\n",
      "Epoch 1 Batch 1026 Loss 0.8948\n",
      "Epoch 1 Batch 1027 Loss 0.9793\n",
      "Epoch 1 Batch 1028 Loss 0.8796\n",
      "Epoch 1 Batch 1029 Loss 0.8370\n",
      "Epoch 1 Batch 1030 Loss 0.8316\n",
      "Epoch 1 Batch 1031 Loss 0.8872\n",
      "Epoch 1 Batch 1032 Loss 0.9402\n",
      "Epoch 1 Batch 1033 Loss 0.8897\n",
      "Epoch 1 Batch 1034 Loss 0.9092\n",
      "Epoch 1 Batch 1035 Loss 1.0370\n",
      "Epoch 1 Batch 1036 Loss 0.6907\n",
      "Epoch 1 Batch 1037 Loss 0.9462\n",
      "Epoch 1 Batch 1038 Loss 1.0958\n",
      "Epoch 1 Batch 1039 Loss 0.7021\n",
      "Epoch 1 Batch 1040 Loss 1.0425\n",
      "Epoch 1 Batch 1041 Loss 1.0173\n",
      "Epoch 1 Batch 1042 Loss 0.7248\n",
      "Epoch 1 Batch 1043 Loss 0.9955\n",
      "Epoch 1 Batch 1044 Loss 0.8224\n",
      "Epoch 1 Batch 1045 Loss 0.9588\n",
      "Epoch 1 Batch 1046 Loss 0.7313\n",
      "Epoch 1 Batch 1047 Loss 1.2681\n",
      "Epoch 1 Batch 1048 Loss 1.0485\n",
      "Epoch 1 Batch 1049 Loss 0.9416\n",
      "Epoch 1 Batch 1050 Loss 0.9019\n",
      "Epoch 1 Batch 1051 Loss 0.8509\n",
      "Epoch 1 Batch 1052 Loss 0.9632\n",
      "Epoch 1 Batch 1053 Loss 0.9875\n",
      "Epoch 1 Batch 1054 Loss 1.0183\n",
      "Epoch 1 Batch 1055 Loss 1.0849\n",
      "Epoch 1 Batch 1056 Loss 0.7391\n",
      "Epoch 1 Batch 1057 Loss 0.9049\n",
      "Epoch 1 Batch 1058 Loss 0.8233\n",
      "Epoch 1 Batch 1059 Loss 0.9085\n",
      "Epoch 1 Batch 1060 Loss 1.0079\n",
      "Epoch 1 Batch 1061 Loss 0.7378\n",
      "Epoch 1 Batch 1062 Loss 1.2191\n",
      "Epoch 1 Batch 1063 Loss 0.8980\n",
      "Epoch 1 Batch 1064 Loss 0.9222\n",
      "Epoch 1 Batch 1065 Loss 1.0332\n",
      "Epoch 1 Batch 1066 Loss 1.2292\n",
      "Epoch 1 Batch 1067 Loss 0.9294\n",
      "Epoch 1 Batch 1068 Loss 0.8047\n",
      "Epoch 1 Batch 1069 Loss 0.8661\n",
      "Epoch 1 Batch 1070 Loss 0.8349\n",
      "Epoch 1 Batch 1071 Loss 0.8666\n",
      "Epoch 1 Batch 1072 Loss 0.9591\n",
      "Epoch 1 Batch 1073 Loss 0.9319\n",
      "Epoch 1 Batch 1074 Loss 1.2812\n",
      "Epoch 1 Batch 1075 Loss 1.3139\n",
      "Epoch 1 Batch 1076 Loss 0.8787\n",
      "Epoch 1 Batch 1077 Loss 0.7397\n",
      "Epoch 1 Batch 1078 Loss 0.7611\n",
      "Epoch 1 Batch 1079 Loss 0.9483\n",
      "Epoch 1 Batch 1080 Loss 0.9798\n",
      "Epoch 1 Batch 1081 Loss 0.8511\n",
      "Epoch 1 Batch 1082 Loss 1.1578\n",
      "Epoch 1 Batch 1083 Loss 0.7781\n",
      "Epoch 1 Batch 1084 Loss 0.7991\n",
      "Epoch 1 Batch 1085 Loss 0.9047\n",
      "Epoch 1 Batch 1086 Loss 1.0568\n",
      "Epoch 1 Batch 1087 Loss 0.8323\n",
      "Epoch 1 Batch 1088 Loss 0.9350\n",
      "Epoch 1 Batch 1089 Loss 1.0363\n",
      "Epoch 1 Batch 1090 Loss 0.9976\n",
      "Epoch 1 Batch 1091 Loss 0.9899\n",
      "Epoch 1 Batch 1092 Loss 0.7759\n",
      "Epoch 1 Batch 1093 Loss 0.7385\n",
      "Epoch 1 Batch 1094 Loss 0.9680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1095 Loss 0.9831\n",
      "Epoch 1 Batch 1096 Loss 0.8544\n",
      "Epoch 1 Batch 1097 Loss 1.1044\n",
      "Epoch 1 Batch 1098 Loss 1.1040\n",
      "Epoch 1 Batch 1099 Loss 0.9273\n",
      "Epoch 1 Batch 1100 Loss 1.0553\n",
      "Epoch 1 Batch 1101 Loss 1.0157\n",
      "Epoch 1 Batch 1102 Loss 0.9646\n",
      "Epoch 1 Batch 1103 Loss 0.8534\n",
      "Epoch 1 Batch 1104 Loss 1.2524\n",
      "Epoch 1 Batch 1105 Loss 0.8636\n",
      "Epoch 1 Batch 1106 Loss 1.0627\n",
      "Epoch 1 Batch 1107 Loss 0.6658\n",
      "Epoch 1 Batch 1108 Loss 0.6959\n",
      "Epoch 1 Batch 1109 Loss 1.0721\n",
      "Epoch 1 Batch 1110 Loss 0.7938\n",
      "Epoch 1 Batch 1111 Loss 1.1080\n",
      "Epoch 1 Batch 1112 Loss 0.9887\n",
      "Epoch 1 Batch 1113 Loss 1.0119\n",
      "Epoch 1 Batch 1114 Loss 1.0932\n",
      "Epoch 1 Batch 1115 Loss 0.7979\n",
      "Epoch 1 Batch 1116 Loss 1.0075\n",
      "Epoch 1 Batch 1117 Loss 0.8766\n",
      "Epoch 1 Batch 1118 Loss 1.1590\n",
      "Epoch 1 Batch 1119 Loss 0.9234\n",
      "Epoch 1 Batch 1120 Loss 0.9386\n",
      "Epoch 1 Batch 1121 Loss 0.9387\n",
      "Epoch 1 Batch 1122 Loss 0.9656\n",
      "Epoch 1 Batch 1123 Loss 0.9544\n",
      "Epoch 1 Batch 1124 Loss 0.8702\n",
      "Epoch 1 Batch 1125 Loss 0.7672\n",
      "Epoch 1 Batch 1126 Loss 1.0336\n",
      "Epoch 1 Batch 1127 Loss 1.1171\n",
      "Epoch 1 Batch 1128 Loss 0.8662\n",
      "Epoch 1 Batch 1129 Loss 1.0035\n",
      "Epoch 1 Batch 1130 Loss 1.0242\n",
      "Epoch 1 Batch 1131 Loss 0.8590\n",
      "Epoch 1 Batch 1132 Loss 0.7275\n",
      "Epoch 1 Batch 1133 Loss 1.2997\n",
      "Epoch 1 Batch 1134 Loss 0.8462\n",
      "Epoch 1 Batch 1135 Loss 0.7569\n",
      "Epoch 1 Batch 1136 Loss 0.9242\n",
      "Epoch 1 Batch 1137 Loss 0.8938\n",
      "Epoch 1 Batch 1138 Loss 0.7911\n",
      "Epoch 1 Batch 1139 Loss 0.8314\n",
      "Epoch 1 Batch 1140 Loss 1.2214\n",
      "Epoch 1 Batch 1141 Loss 0.8554\n",
      "Epoch 1 Batch 1142 Loss 1.1555\n",
      "Epoch 1 Batch 1143 Loss 0.7527\n",
      "Epoch 1 Batch 1144 Loss 0.8259\n",
      "Epoch 1 Batch 1145 Loss 0.9457\n",
      "Epoch 1 Batch 1146 Loss 0.7940\n",
      "Epoch 1 Batch 1147 Loss 0.7884\n",
      "Epoch 1 Batch 1148 Loss 0.8131\n",
      "Epoch 1 Batch 1149 Loss 0.8034\n",
      "Epoch 1 Batch 1150 Loss 0.8724\n",
      "Epoch 1 Batch 1151 Loss 0.9191\n",
      "Epoch 1 Batch 1152 Loss 1.1529\n",
      "Epoch 1 Batch 1153 Loss 0.7938\n",
      "Epoch 1 Batch 1154 Loss 0.8610\n",
      "Epoch 1 Batch 1155 Loss 1.1195\n",
      "Epoch 1 Batch 1156 Loss 1.0364\n",
      "Epoch 1 Batch 1157 Loss 1.3598\n",
      "Epoch 1 Batch 1158 Loss 1.0343\n",
      "Epoch 1 Batch 1159 Loss 0.8437\n",
      "Epoch 1 Batch 1160 Loss 0.8647\n",
      "Epoch 1 Batch 1161 Loss 1.2974\n",
      "Epoch 1 Batch 1162 Loss 0.7990\n",
      "Epoch 1 Batch 1163 Loss 1.0749\n",
      "Epoch 1 Batch 1164 Loss 0.8971\n",
      "Epoch 1 Batch 1165 Loss 1.0422\n",
      "Epoch 1 Batch 1166 Loss 0.9639\n",
      "Epoch 1 Batch 1167 Loss 0.9203\n",
      "Epoch 1 Batch 1168 Loss 0.8688\n",
      "Epoch 1 Batch 1169 Loss 0.9546\n",
      "Epoch 1 Batch 1170 Loss 1.0182\n",
      "Epoch 1 Batch 1171 Loss 0.8990\n",
      "Epoch 1 Batch 1172 Loss 1.1026\n",
      "Epoch 1 Batch 1173 Loss 1.0055\n",
      "Epoch 1 Batch 1174 Loss 0.9106\n",
      "Epoch 1 Batch 1175 Loss 0.8872\n",
      "Epoch 1 Batch 1176 Loss 1.0538\n",
      "Epoch 1 Batch 1177 Loss 0.9478\n",
      "Epoch 1 Batch 1178 Loss 1.0019\n",
      "Epoch 1 Batch 1179 Loss 0.8816\n",
      "Epoch 1 Batch 1180 Loss 1.2174\n",
      "Epoch 1 Batch 1181 Loss 0.9591\n",
      "Epoch 1 Batch 1182 Loss 0.8939\n",
      "Epoch 1 Batch 1183 Loss 1.0034\n",
      "Epoch 1 Batch 1184 Loss 0.9948\n",
      "Epoch 1 Batch 1185 Loss 0.8384\n",
      "Epoch 1 Batch 1186 Loss 1.0046\n",
      "Epoch 1 Batch 1187 Loss 1.0699\n",
      "Epoch 1 Batch 1188 Loss 0.8115\n",
      "Epoch 1 Batch 1189 Loss 0.9439\n",
      "Epoch 1 Batch 1190 Loss 0.8699\n",
      "Epoch 1 Batch 1191 Loss 1.0408\n",
      "Epoch 1 Batch 1192 Loss 0.7393\n",
      "Epoch 1 Batch 1193 Loss 0.7382\n",
      "Epoch 1 Batch 1194 Loss 0.9314\n",
      "Epoch 1 Batch 1195 Loss 1.1106\n",
      "Epoch 1 Batch 1196 Loss 1.0056\n",
      "Epoch 1 Batch 1197 Loss 1.0267\n",
      "Epoch 1 Batch 1198 Loss 0.7658\n",
      "Epoch 1 Batch 1199 Loss 0.8079\n",
      "Epoch 1 Batch 1200 Loss 0.7801\n",
      "Epoch 1 Batch 1201 Loss 1.0533\n",
      "Epoch 1 Batch 1202 Loss 0.8227\n",
      "Epoch 1 Batch 1203 Loss 0.9880\n",
      "Epoch 1 Batch 1204 Loss 1.0986\n",
      "Epoch 1 Batch 1205 Loss 0.9134\n",
      "Epoch 1 Batch 1206 Loss 1.0036\n",
      "Epoch 1 Batch 1207 Loss 1.1920\n",
      "Epoch 1 Batch 1208 Loss 0.8864\n",
      "Epoch 1 Batch 1209 Loss 1.0259\n",
      "Epoch 1 Batch 1210 Loss 0.8907\n",
      "Epoch 1 Batch 1211 Loss 0.9749\n",
      "Epoch 1 Batch 1212 Loss 0.9791\n",
      "Epoch 1 Batch 1213 Loss 1.1661\n",
      "Epoch 1 Batch 1214 Loss 0.8008\n",
      "Epoch 1 Batch 1215 Loss 1.2373\n",
      "Epoch 1 Batch 1216 Loss 0.7989\n",
      "Epoch 1 Batch 1217 Loss 1.1317\n",
      "Epoch 1 Batch 1218 Loss 1.0646\n",
      "Epoch 1 Batch 1219 Loss 0.9231\n",
      "Epoch 1 Batch 1220 Loss 1.0069\n",
      "Epoch 1 Batch 1221 Loss 1.1838\n",
      "Epoch 1 Batch 1222 Loss 0.9792\n",
      "Epoch 1 Batch 1223 Loss 1.0046\n",
      "Epoch 1 Batch 1224 Loss 0.9807\n",
      "Epoch 1 Batch 1225 Loss 1.0630\n",
      "Epoch 1 Batch 1226 Loss 1.1178\n",
      "Epoch 1 Batch 1227 Loss 0.8306\n",
      "Epoch 1 Batch 1228 Loss 0.8900\n",
      "Epoch 1 Batch 1229 Loss 0.9235\n",
      "Epoch 1 Batch 1230 Loss 0.7096\n",
      "Epoch 1 Batch 1231 Loss 1.0370\n",
      "Epoch 1 Batch 1232 Loss 0.8045\n",
      "Epoch 1 Batch 1233 Loss 0.9862\n",
      "Epoch 1 Batch 1234 Loss 0.8488\n",
      "Epoch 1 Batch 1235 Loss 0.9990\n",
      "Epoch 1 Batch 1236 Loss 0.9922\n",
      "Epoch 1 Batch 1237 Loss 1.0117\n",
      "Epoch 1 Batch 1238 Loss 0.8709\n",
      "Epoch 1 Batch 1239 Loss 0.8565\n",
      "Epoch 1 Batch 1240 Loss 0.9888\n",
      "Epoch 1 Batch 1241 Loss 0.8759\n",
      "Epoch 1 Batch 1242 Loss 1.0249\n",
      "Epoch 1 Batch 1243 Loss 0.8123\n",
      "Epoch 1 Batch 1244 Loss 0.9191\n",
      "Epoch 1 Batch 1245 Loss 0.9277\n",
      "Epoch 1 Batch 1246 Loss 0.9926\n",
      "Epoch 1 Batch 1247 Loss 0.8143\n",
      "Epoch 1 Batch 1248 Loss 1.0950\n",
      "Epoch 1 Batch 1249 Loss 1.0578\n",
      "Epoch 1 Batch 1250 Loss 1.0794\n",
      "Epoch 1 Batch 1251 Loss 0.9746\n",
      "Epoch 1 Batch 1252 Loss 0.8605\n",
      "Epoch 1 Batch 1253 Loss 0.9976\n",
      "Epoch 1 Batch 1254 Loss 1.0317\n",
      "Epoch 1 Batch 1255 Loss 1.0610\n",
      "Epoch 1 Batch 1256 Loss 1.0558\n",
      "Epoch 1 Batch 1257 Loss 0.9289\n",
      "Epoch 1 Batch 1258 Loss 0.9343\n",
      "Epoch 1 Batch 1259 Loss 1.1558\n",
      "Epoch 1 Batch 1260 Loss 0.9816\n",
      "Epoch 1 Batch 1261 Loss 0.9423\n",
      "Epoch 1 Batch 1262 Loss 0.8603\n",
      "Epoch 1 Batch 1263 Loss 1.0620\n",
      "Epoch 1 Batch 1264 Loss 0.8831\n",
      "Epoch 1 Batch 1265 Loss 1.0870\n",
      "Epoch 1 Batch 1266 Loss 1.0105\n",
      "Epoch 1 Batch 1267 Loss 0.9578\n",
      "Epoch 1 Batch 1268 Loss 1.0524\n",
      "Epoch 1 Batch 1269 Loss 1.0031\n",
      "Epoch 1 Batch 1270 Loss 0.8505\n",
      "Epoch 1 Batch 1271 Loss 0.8875\n",
      "Epoch 1 Batch 1272 Loss 1.1024\n",
      "Epoch 1 Batch 1273 Loss 1.0054\n",
      "Epoch 1 Batch 1274 Loss 0.9653\n",
      "Epoch 1 Batch 1275 Loss 0.8205\n",
      "Epoch 1 Batch 1276 Loss 1.0020\n",
      "Epoch 1 Batch 1277 Loss 0.8657\n",
      "Epoch 1 Batch 1278 Loss 1.0593\n",
      "Epoch 1 Batch 1279 Loss 0.9314\n",
      "Epoch 1 Batch 1280 Loss 1.0045\n",
      "Epoch 1 Batch 1281 Loss 1.0740\n",
      "Epoch 1 Batch 1282 Loss 0.8348\n",
      "Epoch 1 Batch 1283 Loss 0.8893\n",
      "Epoch 1 Batch 1284 Loss 0.8501\n",
      "Epoch 1 Batch 1285 Loss 0.9902\n",
      "Epoch 1 Batch 1286 Loss 0.9301\n",
      "Epoch 1 Batch 1287 Loss 0.7645\n",
      "Epoch 1 Batch 1288 Loss 1.2959\n",
      "Epoch 1 Batch 1289 Loss 1.0349\n",
      "Epoch 1 Batch 1290 Loss 0.8740\n",
      "Epoch 1 Batch 1291 Loss 1.1330\n",
      "Epoch 1 Batch 1292 Loss 0.9429\n",
      "Epoch 1 Batch 1293 Loss 0.9689\n",
      "Epoch 1 Batch 1294 Loss 1.1185\n",
      "Epoch 1 Batch 1295 Loss 1.0188\n",
      "Epoch 1 Batch 1296 Loss 0.8776\n",
      "Epoch 1 Batch 1297 Loss 0.7649\n",
      "Epoch 1 Batch 1298 Loss 1.0561\n",
      "Epoch 1 Batch 1299 Loss 0.9773\n",
      "Epoch 1 Batch 1300 Loss 0.8497\n",
      "Epoch 1 Batch 1301 Loss 1.1537\n",
      "Epoch 1 Batch 1302 Loss 0.7548\n",
      "Epoch 1 Batch 1303 Loss 0.8916\n",
      "Epoch 1 Batch 1304 Loss 0.9033\n",
      "Epoch 1 Batch 1305 Loss 0.8983\n",
      "Epoch 1 Batch 1306 Loss 0.8385\n",
      "Epoch 1 Batch 1307 Loss 1.0834\n",
      "Epoch 1 Batch 1308 Loss 0.8194\n",
      "Epoch 1 Batch 1309 Loss 0.8657\n",
      "Epoch 1 Batch 1310 Loss 0.8960\n",
      "Epoch 1 Batch 1311 Loss 0.9063\n",
      "Epoch 1 Batch 1312 Loss 1.0765\n",
      "Epoch 1 Batch 1313 Loss 0.9276\n",
      "Epoch 1 Batch 1314 Loss 1.1106\n",
      "Epoch 1 Batch 1315 Loss 0.6735\n",
      "Epoch 1 Batch 1316 Loss 0.9341\n",
      "Epoch 1 Batch 1317 Loss 0.9690\n",
      "Epoch 1 Batch 1318 Loss 0.9771\n",
      "Epoch 1 Batch 1319 Loss 1.1419\n",
      "Epoch 1 Batch 1320 Loss 0.7178\n",
      "Epoch 1 Batch 1321 Loss 0.9557\n",
      "Epoch 1 Batch 1322 Loss 0.8690\n",
      "Epoch 1 Batch 1323 Loss 1.0205\n",
      "Epoch 1 Batch 1324 Loss 0.9653\n",
      "Epoch 1 Batch 1325 Loss 1.2275\n",
      "Epoch 1 Batch 1326 Loss 0.7096\n",
      "Epoch 1 Batch 1327 Loss 0.6936\n",
      "Epoch 1 Batch 1328 Loss 0.9856\n",
      "Epoch 1 Batch 1329 Loss 0.9236\n",
      "Epoch 1 Batch 1330 Loss 0.9331\n",
      "Epoch 1 Batch 1331 Loss 0.7522\n",
      "Epoch 1 Batch 1332 Loss 1.0581\n",
      "Epoch 1 Batch 1333 Loss 1.0593\n",
      "Epoch 1 Batch 1334 Loss 1.0165\n",
      "Epoch 1 Batch 1335 Loss 0.9254\n",
      "Epoch 1 Batch 1336 Loss 0.9480\n",
      "Epoch 1 Batch 1337 Loss 0.8383\n",
      "Epoch 1 Batch 1338 Loss 1.0782\n",
      "Epoch 1 Batch 1339 Loss 0.7144\n",
      "Epoch 1 Batch 1340 Loss 0.9643\n",
      "Epoch 1 Batch 1341 Loss 0.8333\n",
      "Epoch 1 Batch 1342 Loss 1.0584\n",
      "Epoch 1 Batch 1343 Loss 0.8881\n",
      "Epoch 1 Batch 1344 Loss 0.8026\n",
      "Epoch 1 Batch 1345 Loss 1.0247\n",
      "Epoch 1 Batch 1346 Loss 1.1751\n",
      "Epoch 1 Batch 1347 Loss 1.0524\n",
      "Epoch 1 Batch 1348 Loss 0.9552\n",
      "Epoch 1 Batch 1349 Loss 0.8218\n",
      "Epoch 1 Batch 1350 Loss 0.9496\n",
      "Epoch 1 Batch 1351 Loss 0.7588\n",
      "Epoch 1 Batch 1352 Loss 0.9544\n",
      "Epoch 1 Batch 1353 Loss 0.8737\n",
      "Epoch 1 Batch 1354 Loss 0.9156\n",
      "Epoch 1 Batch 1355 Loss 1.0407\n",
      "Epoch 1 Batch 1356 Loss 0.8485\n",
      "Epoch 1 Batch 1357 Loss 1.2320\n",
      "Epoch 1 Batch 1358 Loss 0.9497\n",
      "Epoch 1 Batch 1359 Loss 1.1758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1360 Loss 0.8536\n",
      "Epoch 1 Batch 1361 Loss 0.9692\n",
      "Epoch 1 Batch 1362 Loss 1.1149\n",
      "Epoch 1 Batch 1363 Loss 1.0488\n",
      "Epoch 1 Batch 1364 Loss 0.9073\n",
      "Epoch 1 Batch 1365 Loss 1.0296\n",
      "Epoch 1 Batch 1366 Loss 0.8180\n",
      "Epoch 1 Batch 1367 Loss 0.9119\n",
      "Epoch 1 Batch 1368 Loss 1.0547\n",
      "Epoch 1 Batch 1369 Loss 0.9708\n",
      "Epoch 1 Batch 1370 Loss 0.7114\n",
      "Epoch 1 Batch 1371 Loss 0.7076\n",
      "Epoch 1 Batch 1372 Loss 0.9346\n",
      "Epoch 1 Batch 1373 Loss 0.8180\n",
      "Epoch 1 Batch 1374 Loss 0.8897\n",
      "Epoch 1 Batch 1375 Loss 0.7849\n",
      "Epoch 1 Batch 1376 Loss 0.7917\n",
      "Epoch 1 Batch 1377 Loss 0.8902\n",
      "Epoch 1 Batch 1378 Loss 1.0540\n",
      "Epoch 1 Batch 1379 Loss 1.1474\n",
      "Epoch 1 Batch 1380 Loss 0.9359\n",
      "Epoch 1 Batch 1381 Loss 0.9616\n",
      "Epoch 1 Batch 1382 Loss 0.9207\n",
      "Epoch 1 Batch 1383 Loss 0.8000\n",
      "Epoch 1 Batch 1384 Loss 0.8976\n",
      "Epoch 1 Batch 1385 Loss 0.9403\n",
      "Epoch 1 Batch 1386 Loss 0.9644\n",
      "Epoch 1 Batch 1387 Loss 0.8345\n",
      "Epoch 1 Batch 1388 Loss 0.7283\n",
      "Epoch 1 Batch 1389 Loss 0.8566\n",
      "Epoch 1 Batch 1390 Loss 0.8594\n",
      "Epoch 1 Batch 1391 Loss 0.8447\n",
      "Epoch 1 Batch 1392 Loss 0.8998\n",
      "Epoch 1 Batch 1393 Loss 1.1236\n",
      "Epoch 1 Batch 1394 Loss 1.1566\n",
      "Epoch 1 Batch 1395 Loss 1.0244\n",
      "Epoch 1 Batch 1396 Loss 0.9419\n",
      "Epoch 1 Batch 1397 Loss 0.8712\n",
      "Epoch 1 Batch 1398 Loss 0.6340\n",
      "Epoch 1 Batch 1399 Loss 1.0122\n",
      "Epoch 1 Batch 1400 Loss 1.2513\n",
      "Epoch 1 Batch 1401 Loss 0.8119\n",
      "Epoch 1 Batch 1402 Loss 0.7256\n",
      "Epoch 1 Batch 1403 Loss 0.8957\n",
      "Epoch 1 Batch 1404 Loss 1.0338\n",
      "Epoch 1 Batch 1405 Loss 0.7148\n",
      "Epoch 1 Batch 1406 Loss 0.7018\n",
      "Epoch 1 Batch 1407 Loss 0.8139\n",
      "Epoch 1 Batch 1408 Loss 0.7511\n",
      "Epoch 1 Batch 1409 Loss 1.0399\n",
      "Epoch 1 Batch 1410 Loss 0.9714\n",
      "Epoch 1 Batch 1411 Loss 0.7158\n",
      "Epoch 1 Batch 1412 Loss 1.0099\n",
      "Epoch 1 Batch 1413 Loss 1.0486\n",
      "Epoch 1 Batch 1414 Loss 0.8775\n",
      "Epoch 1 Batch 1415 Loss 0.9687\n",
      "Epoch 1 Batch 1416 Loss 0.8297\n",
      "Epoch 1 Batch 1417 Loss 0.8188\n",
      "Epoch 1 Batch 1418 Loss 0.8794\n",
      "Epoch 1 Batch 1419 Loss 0.7836\n",
      "Epoch 1 Batch 1420 Loss 0.8393\n",
      "Epoch 1 Batch 1421 Loss 0.9276\n",
      "Epoch 1 Batch 1422 Loss 0.6746\n",
      "Epoch 1 Batch 1423 Loss 0.8570\n",
      "Epoch 1 Batch 1424 Loss 0.9327\n",
      "Epoch 1 Batch 1425 Loss 0.8134\n",
      "Epoch 1 Batch 1426 Loss 0.8481\n",
      "Epoch 1 Batch 1427 Loss 0.7058\n",
      "Epoch 1 Batch 1428 Loss 1.0098\n",
      "Epoch 1 Batch 1429 Loss 0.8673\n",
      "Epoch 1 Batch 1430 Loss 1.0138\n",
      "Epoch 1 Batch 1431 Loss 1.0462\n",
      "Epoch 1 Batch 1432 Loss 0.6336\n",
      "Epoch 1 Batch 1433 Loss 0.9785\n",
      "Epoch 1 Batch 1434 Loss 0.7510\n",
      "Epoch 1 Batch 1435 Loss 0.9573\n",
      "Epoch 1 Batch 1436 Loss 0.7364\n",
      "Epoch 1 Batch 1437 Loss 0.8649\n",
      "Epoch 1 Batch 1438 Loss 1.0712\n",
      "Epoch 1 Batch 1439 Loss 1.0251\n",
      "Epoch 1 Batch 1440 Loss 0.8474\n",
      "Epoch 1 Batch 1441 Loss 0.9428\n",
      "Epoch 1 Batch 1442 Loss 0.9639\n",
      "Epoch 1 Batch 1443 Loss 0.7517\n",
      "Epoch 1 Batch 1444 Loss 0.9779\n",
      "Epoch 1 Batch 1445 Loss 0.8533\n",
      "Epoch 1 Batch 1446 Loss 0.7420\n",
      "Epoch 1 Batch 1447 Loss 0.9951\n",
      "Epoch 1 Batch 1448 Loss 0.7889\n",
      "Epoch 1 Batch 1449 Loss 1.0595\n",
      "Epoch 1 Batch 1450 Loss 0.8811\n",
      "Epoch 1 Batch 1451 Loss 0.9344\n",
      "Epoch 1 Batch 1452 Loss 0.9850\n",
      "Epoch 1 Batch 1453 Loss 0.9011\n",
      "Epoch 1 Batch 1454 Loss 1.0086\n",
      "Epoch 1 Batch 1455 Loss 1.0251\n",
      "Epoch 1 Batch 1456 Loss 0.9542\n",
      "Epoch 1 Batch 1457 Loss 0.9340\n",
      "Epoch 1 Batch 1458 Loss 0.9051\n",
      "Epoch 1 Batch 1459 Loss 1.0067\n",
      "Epoch 1 Batch 1460 Loss 0.9832\n",
      "Epoch 1 Batch 1461 Loss 0.8190\n",
      "Epoch 1 Batch 1462 Loss 0.7184\n",
      "Epoch 1 Batch 1463 Loss 0.9274\n",
      "Epoch 1 Batch 1464 Loss 0.9631\n",
      "Epoch 1 Batch 1465 Loss 1.0311\n",
      "Epoch 1 Batch 1466 Loss 0.7636\n",
      "Epoch 1 Batch 1467 Loss 0.6665\n",
      "Epoch 1 Batch 1468 Loss 0.8599\n",
      "Epoch 1 Batch 1469 Loss 0.7834\n",
      "Epoch 1 Batch 1470 Loss 0.7855\n",
      "Epoch 1 Batch 1471 Loss 1.0993\n",
      "Epoch 1 Batch 1472 Loss 1.0075\n",
      "Epoch 1 Batch 1473 Loss 1.0417\n",
      "Epoch 1 Batch 1474 Loss 1.3038\n",
      "Epoch 1 Batch 1475 Loss 1.2087\n",
      "Epoch 1 Batch 1476 Loss 0.9179\n",
      "Epoch 1 Batch 1477 Loss 1.0449\n",
      "Epoch 1 Batch 1478 Loss 0.8712\n",
      "Epoch 1 Batch 1479 Loss 1.1562\n",
      "Epoch 1 Batch 1480 Loss 0.9974\n",
      "Epoch 1 Batch 1481 Loss 0.9014\n",
      "Epoch 1 Batch 1482 Loss 1.0384\n",
      "Epoch 1 Batch 1483 Loss 1.0653\n",
      "Epoch 1 Batch 1484 Loss 1.2099\n",
      "Epoch 1 Batch 1485 Loss 0.9127\n",
      "Epoch 1 Batch 1486 Loss 0.9194\n",
      "Epoch 1 Batch 1487 Loss 1.0501\n",
      "Epoch 1 Batch 1488 Loss 0.8736\n",
      "Epoch 1 Batch 1489 Loss 1.0401\n",
      "Epoch 1 Batch 1490 Loss 0.7451\n",
      "Epoch 1 Batch 1491 Loss 0.9656\n",
      "Epoch 1 Batch 1492 Loss 1.0870\n",
      "Epoch 1 Batch 1493 Loss 1.0703\n",
      "Epoch 1 Batch 1494 Loss 0.9688\n",
      "Epoch 1 Batch 1495 Loss 1.1356\n",
      "Epoch 1 Batch 1496 Loss 1.0196\n",
      "Epoch 1 Batch 1497 Loss 0.7852\n",
      "Epoch 1 Batch 1498 Loss 0.7892\n",
      "Epoch 1 Batch 1499 Loss 1.0588\n",
      "Epoch 1 Batch 1500 Loss 0.8942\n",
      "Epoch 1 Batch 1501 Loss 1.1304\n",
      "Epoch 1 Batch 1502 Loss 0.9710\n",
      "Epoch 1 Batch 1503 Loss 1.0357\n",
      "Epoch 1 Batch 1504 Loss 0.9412\n",
      "Epoch 1 Batch 1505 Loss 0.8030\n",
      "Epoch 1 Batch 1506 Loss 0.8471\n",
      "Epoch 1 Batch 1507 Loss 0.8525\n",
      "Epoch 1 Batch 1508 Loss 0.8577\n",
      "Epoch 1 Batch 1509 Loss 0.7411\n",
      "Epoch 1 Batch 1510 Loss 0.8242\n",
      "Epoch 1 Batch 1511 Loss 0.7135\n",
      "Epoch 1 Batch 1512 Loss 0.9630\n",
      "Epoch 1 Batch 1513 Loss 0.9037\n",
      "Epoch 1 Batch 1514 Loss 0.8455\n",
      "Epoch 1 Batch 1515 Loss 1.0030\n",
      "Epoch 1 Batch 1516 Loss 0.8052\n",
      "Epoch 1 Batch 1517 Loss 0.7327\n",
      "Epoch 1 Batch 1518 Loss 0.9131\n",
      "Epoch 1 Batch 1519 Loss 1.0289\n",
      "Epoch 1 Batch 1520 Loss 0.7904\n",
      "Epoch 1 Batch 1521 Loss 0.9393\n",
      "Epoch 1 Batch 1522 Loss 0.9469\n",
      "Epoch 1 Batch 1523 Loss 0.8674\n",
      "Epoch 1 Batch 1524 Loss 1.1830\n",
      "Epoch 1 Batch 1525 Loss 0.7540\n",
      "Epoch 1 Batch 1526 Loss 0.9684\n",
      "Epoch 1 Batch 1527 Loss 0.7431\n",
      "Epoch 1 Batch 1528 Loss 0.8003\n",
      "Epoch 1 Batch 1529 Loss 1.0149\n",
      "Epoch 1 Batch 1530 Loss 1.1833\n",
      "Epoch 1 Batch 1531 Loss 0.9312\n",
      "Epoch 1 Batch 1532 Loss 1.1743\n",
      "Epoch 1 Batch 1533 Loss 0.9789\n",
      "Epoch 1 Batch 1534 Loss 0.8899\n",
      "Epoch 1 Batch 1535 Loss 0.7625\n",
      "Epoch 1 Batch 1536 Loss 0.8248\n",
      "Epoch 1 Batch 1537 Loss 1.0374\n",
      "Epoch 1 Batch 1538 Loss 0.9195\n",
      "Epoch 1 Batch 1539 Loss 0.9428\n",
      "Epoch 1 Batch 1540 Loss 0.9646\n",
      "Epoch 1 Batch 1541 Loss 0.8184\n",
      "Epoch 1 Batch 1542 Loss 1.0204\n",
      "Epoch 1 Batch 1543 Loss 1.0398\n",
      "Epoch 1 Batch 1544 Loss 1.2264\n",
      "Epoch 1 Batch 1545 Loss 1.0099\n",
      "Epoch 1 Batch 1546 Loss 0.9934\n",
      "Epoch 1 Batch 1547 Loss 0.8409\n",
      "Epoch 1 Batch 1548 Loss 0.9393\n",
      "Epoch 1 Batch 1549 Loss 0.9342\n",
      "Epoch 1 Batch 1550 Loss 0.8999\n",
      "Epoch 1 Batch 1551 Loss 0.9472\n",
      "Epoch 1 Batch 1552 Loss 0.9358\n",
      "Epoch 1 Batch 1553 Loss 1.1474\n",
      "Epoch 1 Batch 1554 Loss 0.9630\n",
      "Epoch 1 Batch 1555 Loss 0.8200\n",
      "Epoch 1 Batch 1556 Loss 0.9044\n",
      "Epoch 1 Batch 1557 Loss 0.9074\n",
      "Epoch 1 Batch 1558 Loss 0.9445\n",
      "Epoch 1 Batch 1559 Loss 0.8158\n",
      "Epoch 1 Batch 1560 Loss 0.7705\n",
      "Epoch 1 Batch 1561 Loss 0.9423\n",
      "Epoch 1 Batch 1562 Loss 0.8214\n",
      "Epoch 1 Batch 1563 Loss 0.8658\n",
      "Epoch 1 Batch 1564 Loss 0.8229\n",
      "Epoch 1 Batch 1565 Loss 0.6747\n",
      "Epoch 1 Batch 1566 Loss 0.9530\n",
      "Epoch 1 Batch 1567 Loss 0.7550\n",
      "Epoch 1 Batch 1568 Loss 0.9319\n",
      "Epoch 1 Batch 1569 Loss 1.0261\n",
      "Epoch 1 Batch 1570 Loss 0.8731\n",
      "Epoch 1 Batch 1571 Loss 0.8629\n",
      "Epoch 1 Batch 1572 Loss 0.9448\n",
      "Epoch 1 Batch 1573 Loss 1.3862\n",
      "Epoch 1 Batch 1574 Loss 0.8137\n",
      "Epoch 1 Batch 1575 Loss 0.9518\n",
      "Epoch 1 Batch 1576 Loss 0.9360\n",
      "Epoch 1 Batch 1577 Loss 0.9677\n",
      "Epoch 1 Batch 1578 Loss 0.9312\n",
      "Epoch 1 Batch 1579 Loss 0.7875\n",
      "Epoch 1 Batch 1580 Loss 0.9419\n",
      "Epoch 1 Batch 1581 Loss 0.8597\n",
      "Epoch 1 Batch 1582 Loss 0.8558\n",
      "Epoch 1 Batch 1583 Loss 1.0118\n",
      "Epoch 1 Batch 1584 Loss 0.9350\n",
      "Epoch 1 Batch 1585 Loss 0.8182\n",
      "Epoch 1 Batch 1586 Loss 0.9939\n",
      "Epoch 1 Batch 1587 Loss 1.0550\n",
      "Epoch 1 Batch 1588 Loss 1.1157\n",
      "Epoch 1 Batch 1589 Loss 0.6680\n",
      "Epoch 1 Batch 1590 Loss 0.8523\n",
      "Epoch 1 Batch 1591 Loss 1.0971\n",
      "Epoch 1 Batch 1592 Loss 0.9888\n",
      "Epoch 1 Batch 1593 Loss 1.1000\n",
      "Epoch 1 Batch 1594 Loss 0.8123\n",
      "Epoch 1 Batch 1595 Loss 0.8325\n",
      "Epoch 1 Batch 1596 Loss 1.0722\n",
      "Epoch 1 Batch 1597 Loss 0.8733\n",
      "Epoch 1 Batch 1598 Loss 0.9686\n",
      "Epoch 1 Batch 1599 Loss 0.7452\n",
      "Epoch 1 Batch 1600 Loss 1.0106\n",
      "Epoch 1 Batch 1601 Loss 1.1107\n",
      "Epoch 1 Batch 1602 Loss 0.8709\n",
      "Epoch 1 Batch 1603 Loss 0.8731\n",
      "Epoch 1 Batch 1604 Loss 1.1098\n",
      "Epoch 1 Batch 1605 Loss 1.1567\n",
      "Epoch 1 Batch 1606 Loss 0.7454\n",
      "Epoch 1 Batch 1607 Loss 0.7038\n",
      "Epoch 1 Batch 1608 Loss 0.8955\n",
      "Epoch 1 Batch 1609 Loss 1.1086\n",
      "Epoch 1 Batch 1610 Loss 0.9656\n",
      "Epoch 1 Batch 1611 Loss 0.9506\n",
      "Epoch 1 Batch 1612 Loss 1.1993\n",
      "Epoch 1 Batch 1613 Loss 0.9877\n",
      "Epoch 1 Batch 1614 Loss 1.1033\n",
      "Epoch 1 Batch 1615 Loss 0.8562\n",
      "Epoch 1 Batch 1616 Loss 0.9371\n",
      "Epoch 1 Batch 1617 Loss 0.9912\n",
      "Epoch 1 Batch 1618 Loss 1.0640\n",
      "Epoch 1 Batch 1619 Loss 0.8613\n",
      "Epoch 1 Batch 1620 Loss 0.9097\n",
      "Epoch 1 Batch 1621 Loss 0.9941\n",
      "Epoch 1 Batch 1622 Loss 1.2501\n",
      "Epoch 1 Batch 1623 Loss 1.2937\n",
      "Epoch 1 Batch 1624 Loss 0.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1625 Loss 0.9708\n",
      "Epoch 1 Batch 1626 Loss 0.8552\n",
      "Epoch 1 Batch 1627 Loss 1.0164\n",
      "Epoch 1 Batch 1628 Loss 0.9041\n",
      "Epoch 1 Batch 1629 Loss 0.9254\n",
      "Epoch 1 Batch 1630 Loss 0.9340\n",
      "Epoch 1 Batch 1631 Loss 0.9488\n",
      "Epoch 1 Batch 1632 Loss 0.9559\n",
      "Epoch 1 Batch 1633 Loss 0.8573\n",
      "Epoch 1 Batch 1634 Loss 0.9528\n",
      "Epoch 1 Batch 1635 Loss 1.0162\n",
      "Epoch 1 Batch 1636 Loss 0.9615\n",
      "Epoch 1 Batch 1637 Loss 1.0081\n",
      "Epoch 1 Batch 1638 Loss 1.2133\n",
      "Epoch 1 Batch 1639 Loss 0.9501\n",
      "Epoch 1 Batch 1640 Loss 0.8603\n",
      "Epoch 1 Batch 1641 Loss 0.9037\n",
      "Epoch 1 Batch 1642 Loss 1.1091\n",
      "Epoch 1 Batch 1643 Loss 0.9231\n",
      "Epoch 1 Batch 1644 Loss 1.0154\n",
      "Epoch 1 Batch 1645 Loss 0.8274\n",
      "Epoch 1 Batch 1646 Loss 0.9644\n",
      "Epoch 1 Batch 1647 Loss 1.0111\n",
      "Epoch 1 Batch 1648 Loss 0.9704\n",
      "Epoch 1 Batch 1649 Loss 0.9386\n",
      "Epoch 1 Batch 1650 Loss 1.0428\n",
      "Epoch 1 Batch 1651 Loss 1.1065\n",
      "Epoch 1 Batch 1652 Loss 0.9685\n",
      "Epoch 1 Batch 1653 Loss 0.8484\n",
      "Epoch 1 Batch 1654 Loss 0.9541\n",
      "Epoch 1 Batch 1655 Loss 1.1601\n",
      "Epoch 1 Batch 1656 Loss 0.9992\n",
      "Epoch 1 Batch 1657 Loss 0.8928\n",
      "Epoch 1 Batch 1658 Loss 0.8518\n",
      "Epoch 1 Batch 1659 Loss 1.1869\n",
      "Epoch 1 Batch 1660 Loss 1.0234\n",
      "Epoch 1 Batch 1661 Loss 1.1391\n",
      "Epoch 1 Batch 1662 Loss 0.8326\n",
      "Epoch 1 Batch 1663 Loss 1.0968\n",
      "Epoch 1 Batch 1664 Loss 0.9360\n",
      "Epoch 1 Batch 1665 Loss 1.0443\n",
      "Epoch 1 Batch 1666 Loss 0.9722\n",
      "Epoch 1 Batch 1667 Loss 0.9040\n",
      "Epoch 1 Batch 1668 Loss 1.0846\n",
      "Epoch 1 Batch 1669 Loss 0.8427\n",
      "Epoch 1 Batch 1670 Loss 0.8252\n",
      "Epoch 1 Batch 1671 Loss 0.9063\n",
      "Epoch 1 Batch 1672 Loss 0.8401\n",
      "Epoch 1 Batch 1673 Loss 1.2251\n",
      "Epoch 1 Batch 1674 Loss 1.1377\n",
      "Epoch 1 Batch 1675 Loss 1.0020\n",
      "Epoch 1 Batch 1676 Loss 1.1476\n",
      "Epoch 1 Batch 1677 Loss 1.1825\n",
      "Epoch 1 Batch 1678 Loss 0.7472\n",
      "Epoch 1 Batch 1679 Loss 0.8732\n",
      "Epoch 1 Batch 1680 Loss 0.8718\n",
      "Epoch 1 Batch 1681 Loss 0.9730\n",
      "Epoch 1 Batch 1682 Loss 0.8695\n",
      "Epoch 1 Batch 1683 Loss 0.6848\n",
      "Epoch 1 Batch 1684 Loss 0.8578\n",
      "Epoch 1 Batch 1685 Loss 0.9421\n",
      "Epoch 1 Batch 1686 Loss 0.8839\n",
      "Epoch 1 Batch 1687 Loss 0.8394\n",
      "Epoch 1 Batch 1688 Loss 0.9433\n",
      "Epoch 1 Batch 1689 Loss 0.6290\n",
      "Epoch 1 Batch 1690 Loss 0.9274\n",
      "Epoch 1 Batch 1691 Loss 0.9289\n",
      "Epoch 1 Batch 1692 Loss 0.8586\n",
      "Epoch 1 Batch 1693 Loss 1.1322\n",
      "Epoch 1 Batch 1694 Loss 0.7935\n",
      "Epoch 1 Batch 1695 Loss 0.9319\n",
      "Epoch 1 Batch 1696 Loss 0.8753\n",
      "Epoch 1 Batch 1697 Loss 0.9263\n",
      "Epoch 1 Batch 1698 Loss 0.8580\n",
      "Epoch 1 Batch 1699 Loss 0.7513\n",
      "Epoch 1 Batch 1700 Loss 1.1781\n",
      "Epoch 1 Batch 1701 Loss 0.9252\n",
      "Epoch 1 Batch 1702 Loss 1.0287\n",
      "Epoch 1 Batch 1703 Loss 0.7267\n",
      "Epoch 1 Batch 1704 Loss 0.8166\n",
      "Epoch 1 Batch 1705 Loss 0.8919\n",
      "Epoch 1 Batch 1706 Loss 0.8146\n",
      "Epoch 1 Batch 1707 Loss 1.3470\n",
      "Epoch 1 Batch 1708 Loss 1.0500\n",
      "Epoch 1 Batch 1709 Loss 0.9287\n",
      "Epoch 1 Batch 1710 Loss 0.9145\n",
      "Epoch 1 Batch 1711 Loss 0.7652\n",
      "Epoch 1 Batch 1712 Loss 1.0304\n",
      "Epoch 1 Batch 1713 Loss 0.9664\n",
      "Epoch 1 Batch 1714 Loss 1.0215\n",
      "Epoch 1 Batch 1715 Loss 0.8215\n",
      "Epoch 1 Batch 1716 Loss 1.0499\n",
      "Epoch 1 Batch 1717 Loss 0.8913\n",
      "Epoch 1 Batch 1718 Loss 0.7489\n",
      "Epoch 1 Batch 1719 Loss 1.0633\n",
      "Epoch 1 Batch 1720 Loss 0.9680\n",
      "Epoch 1 Batch 1721 Loss 1.0575\n",
      "Epoch 1 Batch 1722 Loss 0.9674\n",
      "Epoch 1 Batch 1723 Loss 0.9917\n",
      "Epoch 1 Batch 1724 Loss 0.7976\n",
      "Epoch 1 Batch 1725 Loss 0.9865\n",
      "Epoch 1 Batch 1726 Loss 0.9470\n",
      "Epoch 1 Batch 1727 Loss 0.8505\n",
      "Epoch 1 Batch 1728 Loss 0.9530\n",
      "Epoch 1 Batch 1729 Loss 1.0074\n",
      "Epoch 1 Batch 1730 Loss 0.9752\n",
      "Epoch 1 Batch 1731 Loss 0.8235\n",
      "Epoch 1 Batch 1732 Loss 0.7384\n",
      "Epoch 1 Batch 1733 Loss 1.0686\n",
      "Epoch 1 Batch 1734 Loss 1.0711\n",
      "Epoch 1 Batch 1735 Loss 0.9176\n",
      "Epoch 1 Batch 1736 Loss 1.1430\n",
      "Epoch 1 Batch 1737 Loss 0.9646\n",
      "Epoch 1 Batch 1738 Loss 1.0163\n",
      "Epoch 1 Batch 1739 Loss 1.0263\n",
      "Epoch 1 Batch 1740 Loss 0.9452\n",
      "Epoch 1 Batch 1741 Loss 0.9970\n",
      "Epoch 1 Batch 1742 Loss 0.9794\n",
      "Epoch 1 Batch 1743 Loss 0.9757\n",
      "Epoch 1 Batch 1744 Loss 0.8312\n",
      "Epoch 1 Batch 1745 Loss 0.9412\n",
      "Epoch 1 Batch 1746 Loss 1.1186\n",
      "Epoch 1 Batch 1747 Loss 0.9267\n",
      "Epoch 1 Batch 1748 Loss 1.0132\n",
      "Epoch 1 Batch 1749 Loss 1.1555\n",
      "Epoch 1 Batch 1750 Loss 0.8854\n",
      "Epoch 1 Batch 1751 Loss 0.8445\n",
      "Epoch 1 Batch 1752 Loss 1.0685\n",
      "Epoch 1 Batch 1753 Loss 0.8088\n",
      "Epoch 1 Batch 1754 Loss 1.0756\n",
      "Epoch 1 Batch 1755 Loss 0.7655\n",
      "Epoch 1 Batch 1756 Loss 1.1214\n",
      "Epoch 1 Batch 1757 Loss 0.8854\n",
      "Epoch 1 Batch 1758 Loss 0.9746\n",
      "Epoch 1 Batch 1759 Loss 0.8284\n",
      "Epoch 1 Batch 1760 Loss 0.7371\n",
      "Epoch 1 Batch 1761 Loss 0.7703\n",
      "Epoch 1 Batch 1762 Loss 0.8377\n",
      "Epoch 1 Batch 1763 Loss 0.9052\n",
      "Epoch 1 Batch 1764 Loss 0.9721\n",
      "Epoch 1 Batch 1765 Loss 0.7739\n",
      "Epoch 1 Batch 1766 Loss 0.9912\n",
      "Epoch 1 Batch 1767 Loss 0.9742\n",
      "Epoch 1 Batch 1768 Loss 0.9642\n",
      "Epoch 1 Batch 1769 Loss 0.8083\n",
      "Epoch 1 Batch 1770 Loss 0.7392\n",
      "Epoch 1 Batch 1771 Loss 0.9558\n",
      "Epoch 1 Batch 1772 Loss 1.1950\n",
      "Epoch 1 Batch 1773 Loss 1.0876\n",
      "Epoch 1 Batch 1774 Loss 0.6894\n",
      "Epoch 1 Batch 1775 Loss 0.9636\n",
      "Epoch 1 Batch 1776 Loss 1.0053\n",
      "Epoch 1 Batch 1777 Loss 1.1595\n",
      "Epoch 1 Batch 1778 Loss 0.9006\n",
      "Epoch 1 Batch 1779 Loss 0.8806\n",
      "Epoch 1 Batch 1780 Loss 1.0696\n",
      "Epoch 1 Batch 1781 Loss 0.9047\n",
      "Epoch 1 Batch 1782 Loss 1.0836\n",
      "Epoch 1 Batch 1783 Loss 0.9954\n",
      "Epoch 1 Batch 1784 Loss 0.9247\n",
      "Epoch 1 Batch 1785 Loss 0.9316\n",
      "Epoch 1 Batch 1786 Loss 1.0367\n",
      "Epoch 1 Batch 1787 Loss 1.0109\n",
      "Epoch 1 Batch 1788 Loss 1.1564\n",
      "Epoch 1 Batch 1789 Loss 0.8975\n",
      "Epoch 1 Batch 1790 Loss 1.0351\n",
      "Epoch 1 Batch 1791 Loss 0.9766\n",
      "Epoch 1 Batch 1792 Loss 1.0840\n",
      "Epoch 1 Batch 1793 Loss 0.9946\n",
      "Epoch 1 Batch 1794 Loss 0.7781\n",
      "Epoch 1 Batch 1795 Loss 0.9785\n",
      "Epoch 1 Batch 1796 Loss 0.8682\n",
      "Epoch 1 Batch 1797 Loss 0.8371\n",
      "Epoch 1 Batch 1798 Loss 0.9412\n",
      "Epoch 1 Batch 1799 Loss 0.9523\n",
      "Epoch 1 Batch 1800 Loss 1.0772\n",
      "Epoch 1 Batch 1801 Loss 1.0998\n",
      "Epoch 1 Batch 1802 Loss 0.9802\n",
      "Epoch 1 Batch 1803 Loss 0.9578\n",
      "Epoch 1 Batch 1804 Loss 1.0275\n",
      "Epoch 1 Batch 1805 Loss 0.8139\n",
      "Epoch 1 Batch 1806 Loss 0.8967\n",
      "Epoch 1 Batch 1807 Loss 1.0168\n",
      "Epoch 1 Batch 1808 Loss 0.9512\n",
      "Epoch 1 Batch 1809 Loss 0.7589\n",
      "Epoch 1 Batch 1810 Loss 1.0247\n",
      "Epoch 1 Batch 1811 Loss 0.8627\n",
      "Epoch 1 Batch 1812 Loss 0.9762\n",
      "Epoch 1 Batch 1813 Loss 0.7987\n",
      "Epoch 1 Batch 1814 Loss 0.7837\n",
      "Epoch 1 Batch 1815 Loss 1.0601\n",
      "Epoch 1 Batch 1816 Loss 0.8800\n",
      "Epoch 1 Batch 1817 Loss 0.9558\n",
      "Epoch 1 Batch 1818 Loss 0.9482\n",
      "Epoch 1 Batch 1819 Loss 0.8234\n",
      "Epoch 1 Batch 1820 Loss 0.8813\n",
      "Epoch 1 Batch 1821 Loss 1.1125\n",
      "Epoch 1 Batch 1822 Loss 0.8839\n",
      "Epoch 1 Batch 1823 Loss 0.7659\n",
      "Epoch 1 Batch 1824 Loss 0.8686\n",
      "Epoch 1 Batch 1825 Loss 0.9632\n",
      "Epoch 1 Batch 1826 Loss 0.9634\n",
      "Epoch 1 Batch 1827 Loss 0.8920\n",
      "Epoch 1 Batch 1828 Loss 1.0624\n",
      "Epoch 1 Batch 1829 Loss 0.8325\n",
      "Epoch 1 Batch 1830 Loss 0.8617\n",
      "Epoch 1 Batch 1831 Loss 0.9917\n",
      "Epoch 1 Batch 1832 Loss 1.1085\n",
      "Epoch 1 Batch 1833 Loss 0.8754\n",
      "Epoch 1 Batch 1834 Loss 1.1781\n",
      "Epoch 1 Batch 1835 Loss 0.9731\n",
      "Epoch 1 Batch 1836 Loss 0.9462\n",
      "Epoch 1 Batch 1837 Loss 1.0085\n",
      "Epoch 1 Batch 1838 Loss 1.0842\n",
      "Epoch 1 Batch 1839 Loss 0.8407\n",
      "Epoch 1 Batch 1840 Loss 1.0021\n",
      "Epoch 1 Batch 1841 Loss 1.0090\n",
      "Epoch 1 Batch 1842 Loss 0.9353\n",
      "Epoch 1 Batch 1843 Loss 0.9283\n",
      "Epoch 1 Batch 1844 Loss 1.0554\n",
      "Epoch 1 Batch 1845 Loss 0.9392\n",
      "Epoch 1 Batch 1846 Loss 1.0633\n",
      "Epoch 1 Batch 1847 Loss 1.1730\n",
      "Epoch 1 Batch 1848 Loss 1.1286\n",
      "Epoch 1 Batch 1849 Loss 0.9725\n",
      "Epoch 1 Batch 1850 Loss 0.9938\n",
      "Epoch 1 Batch 1851 Loss 1.1071\n",
      "Epoch 1 Batch 1852 Loss 0.9560\n",
      "Epoch 1 Batch 1853 Loss 0.8437\n",
      "Epoch 1 Batch 1854 Loss 1.0233\n",
      "Epoch 1 Batch 1855 Loss 0.8770\n",
      "Epoch 1 Batch 1856 Loss 0.9936\n",
      "Epoch 1 Batch 1857 Loss 0.9678\n",
      "Epoch 1 Batch 1858 Loss 0.9104\n",
      "Epoch 1 Batch 1859 Loss 1.0667\n",
      "Epoch 1 Batch 1860 Loss 0.6330\n",
      "Epoch 1 Batch 1861 Loss 1.0370\n",
      "Epoch 1 Batch 1862 Loss 1.0089\n",
      "Epoch 1 Batch 1863 Loss 0.9787\n",
      "Epoch 1 Batch 1864 Loss 0.8032\n",
      "Epoch 1 Batch 1865 Loss 0.9022\n",
      "Epoch 1 Batch 1866 Loss 0.8935\n",
      "Epoch 1 Batch 1867 Loss 0.9868\n",
      "Epoch 1 Batch 1868 Loss 1.1600\n",
      "Epoch 1 Batch 1869 Loss 1.0340\n",
      "Epoch 1 Batch 1870 Loss 1.1731\n",
      "Epoch 1 Batch 1871 Loss 1.0538\n",
      "Epoch 1 Batch 1872 Loss 1.0599\n",
      "Epoch 1 Batch 1873 Loss 0.9527\n",
      "Epoch 1 Batch 1874 Loss 0.9586\n",
      "Epoch 1 Batch 1875 Loss 0.7114\n",
      "Epoch 1 Batch 1876 Loss 0.9354\n",
      "Epoch 1 Batch 1877 Loss 1.1311\n",
      "Epoch 1 Batch 1878 Loss 1.1990\n",
      "Epoch 1 Batch 1879 Loss 0.8423\n",
      "Epoch 1 Batch 1880 Loss 0.5850\n",
      "Epoch 1 Batch 1881 Loss 0.9272\n",
      "Epoch 1 Batch 1882 Loss 0.9824\n",
      "Epoch 1 Batch 1883 Loss 0.7697\n",
      "Epoch 1 Batch 1884 Loss 1.1944\n",
      "Epoch 1 Batch 1885 Loss 0.9141\n",
      "Epoch 1 Batch 1886 Loss 0.9374\n",
      "Epoch 1 Batch 1887 Loss 0.9906\n",
      "Epoch 1 Batch 1888 Loss 1.0044\n",
      "Epoch 1 Batch 1889 Loss 1.1741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1890 Loss 0.7992\n",
      "Epoch 1 Batch 1891 Loss 0.9578\n",
      "Epoch 1 Batch 1892 Loss 1.0512\n",
      "Epoch 1 Batch 1893 Loss 1.0352\n",
      "Epoch 1 Batch 1894 Loss 1.1564\n",
      "Epoch 1 Batch 1895 Loss 0.7373\n",
      "Epoch 1 Batch 1896 Loss 1.0410\n",
      "Epoch 1 Batch 1897 Loss 0.9133\n",
      "Epoch 1 Batch 1898 Loss 0.9281\n",
      "Epoch 1 Batch 1899 Loss 1.0627\n",
      "Epoch 1 Batch 1900 Loss 0.8343\n",
      "Epoch 1 Batch 1901 Loss 1.0404\n",
      "Epoch 1 Batch 1902 Loss 0.7364\n",
      "Epoch 1 Batch 1903 Loss 0.9794\n",
      "Epoch 1 Batch 1904 Loss 0.8312\n",
      "Epoch 1 Batch 1905 Loss 1.2267\n",
      "Epoch 1 Batch 1906 Loss 0.9683\n",
      "Epoch 1 Batch 1907 Loss 0.8255\n",
      "Epoch 1 Batch 1908 Loss 0.9284\n",
      "Epoch 1 Batch 1909 Loss 1.1617\n",
      "Epoch 1 Batch 1910 Loss 1.0739\n",
      "Epoch 1 Batch 1911 Loss 0.9392\n",
      "Epoch 1 Batch 1912 Loss 1.0022\n",
      "Epoch 1 Batch 1913 Loss 0.9622\n",
      "Epoch 1 Batch 1914 Loss 1.0287\n",
      "Epoch 1 Batch 1915 Loss 0.8078\n",
      "Epoch 1 Batch 1916 Loss 0.9795\n",
      "Epoch 1 Batch 1917 Loss 0.8821\n",
      "Epoch 1 Batch 1918 Loss 0.9543\n",
      "Epoch 1 Batch 1919 Loss 0.9935\n",
      "Epoch 1 Batch 1920 Loss 1.3013\n",
      "Epoch 1 Batch 1921 Loss 0.9995\n",
      "Epoch 1 Batch 1922 Loss 1.2794\n",
      "Epoch 1 Batch 1923 Loss 0.9062\n",
      "Epoch 1 Batch 1924 Loss 0.6063\n",
      "Epoch 1 Batch 1925 Loss 1.0713\n",
      "Epoch 1 Batch 1926 Loss 0.6606\n",
      "Epoch 1 Batch 1927 Loss 1.0296\n",
      "Epoch 1 Batch 1928 Loss 0.8563\n",
      "Epoch 1 Batch 1929 Loss 0.9923\n",
      "Epoch 1 Batch 1930 Loss 0.8356\n",
      "Epoch 1 Batch 1931 Loss 0.8415\n",
      "Epoch 1 Batch 1932 Loss 0.9828\n",
      "Epoch 1 Batch 1933 Loss 0.8819\n",
      "Epoch 1 Batch 1934 Loss 1.0749\n",
      "Epoch 1 Batch 1935 Loss 0.9588\n",
      "Epoch 1 Batch 1936 Loss 1.0206\n",
      "Epoch 1 Batch 1937 Loss 0.9177\n",
      "Epoch 1 Batch 1938 Loss 1.0277\n",
      "Epoch 1 Batch 1939 Loss 0.7752\n",
      "Epoch 1 Batch 1940 Loss 1.0053\n",
      "Epoch 1 Batch 1941 Loss 1.0888\n",
      "Epoch 1 Batch 1942 Loss 0.9817\n",
      "Epoch 1 Batch 1943 Loss 1.1128\n",
      "Epoch 1 Batch 1944 Loss 0.8685\n",
      "Epoch 1 Batch 1945 Loss 0.7300\n",
      "Epoch 1 Batch 1946 Loss 0.9123\n",
      "Epoch 1 Batch 1947 Loss 0.9930\n",
      "Epoch 1 Batch 1948 Loss 0.8833\n",
      "Epoch 1 Batch 1949 Loss 1.0960\n",
      "Epoch 1 Batch 1950 Loss 0.9504\n",
      "Epoch 1 Batch 1951 Loss 1.0610\n",
      "Epoch 1 Batch 1952 Loss 1.0727\n",
      "Epoch 1 Batch 1953 Loss 0.8421\n",
      "Epoch 1 Batch 1954 Loss 1.1769\n",
      "Epoch 1 Batch 1955 Loss 1.2618\n",
      "Epoch 1 Batch 1956 Loss 0.8616\n",
      "Epoch 1 Batch 1957 Loss 0.9381\n",
      "Epoch 1 Batch 1958 Loss 0.9970\n",
      "Epoch 1 Batch 1959 Loss 0.8943\n",
      "Epoch 1 Batch 1960 Loss 0.8892\n",
      "Epoch 1 Batch 1961 Loss 0.9724\n",
      "Epoch 1 Batch 1962 Loss 0.8170\n",
      "Epoch 1 Batch 1963 Loss 0.9341\n",
      "Epoch 1 Batch 1964 Loss 0.8761\n",
      "Epoch 1 Batch 1965 Loss 0.8504\n",
      "Epoch 1 Batch 1966 Loss 1.1935\n",
      "Epoch 1 Batch 1967 Loss 0.9437\n",
      "Epoch 1 Batch 1968 Loss 0.9686\n",
      "Epoch 1 Batch 1969 Loss 0.7335\n",
      "Epoch 1 Batch 1970 Loss 1.0109\n",
      "Epoch 1 Batch 1971 Loss 0.8587\n",
      "Epoch 1 Batch 1972 Loss 0.9641\n",
      "Epoch 1 Batch 1973 Loss 1.0528\n",
      "Epoch 1 Batch 1974 Loss 0.8710\n",
      "Epoch 1 Batch 1975 Loss 0.9927\n",
      "Epoch 1 Batch 1976 Loss 1.1038\n",
      "Epoch 1 Batch 1977 Loss 0.8388\n",
      "Epoch 1 Batch 1978 Loss 0.9733\n",
      "Epoch 1 Batch 1979 Loss 0.9331\n",
      "Epoch 1 Batch 1980 Loss 0.9039\n",
      "Epoch 1 Batch 1981 Loss 0.7328\n",
      "Epoch 1 Batch 1982 Loss 1.0103\n",
      "Epoch 1 Batch 1983 Loss 0.8302\n",
      "Epoch 1 Batch 1984 Loss 0.8909\n",
      "Epoch 1 Batch 1985 Loss 1.2334\n",
      "Epoch 1 Batch 1986 Loss 0.9598\n",
      "Epoch 1 Batch 1987 Loss 1.0363\n",
      "Epoch 1 Batch 1988 Loss 0.9895\n",
      "Epoch 1 Batch 1989 Loss 0.8374\n",
      "Epoch 1 Batch 1990 Loss 0.9497\n",
      "Epoch 1 Batch 1991 Loss 1.0042\n",
      "Epoch 1 Batch 1992 Loss 0.8988\n",
      "Epoch 1 Batch 1993 Loss 0.8678\n",
      "Epoch 1 Batch 1994 Loss 0.8698\n",
      "Epoch 1 Batch 1995 Loss 0.9687\n",
      "Epoch 1 Batch 1996 Loss 1.1280\n",
      "Epoch 1 Batch 1997 Loss 1.1834\n",
      "Epoch 1 Batch 1998 Loss 0.8261\n",
      "Epoch 1 Batch 1999 Loss 1.0962\n",
      "Epoch 1 Batch 2000 Loss 0.9206\n",
      "Epoch 1 Batch 2001 Loss 0.9568\n",
      "Epoch 1 Batch 2002 Loss 0.7048\n",
      "Epoch 1 Batch 2003 Loss 0.8981\n",
      "Epoch 1 Batch 2004 Loss 0.9621\n",
      "Epoch 1 Batch 2005 Loss 0.7456\n",
      "Epoch 1 Batch 2006 Loss 1.0458\n",
      "Epoch 1 Batch 2007 Loss 1.0028\n",
      "Epoch 1 Batch 2008 Loss 0.8909\n",
      "Epoch 1 Batch 2009 Loss 0.8346\n",
      "Epoch 1 Batch 2010 Loss 0.6631\n",
      "Epoch 1 Batch 2011 Loss 1.0592\n",
      "Epoch 1 Batch 2012 Loss 0.7743\n",
      "Epoch 1 Batch 2013 Loss 0.9324\n",
      "Epoch 1 Batch 2014 Loss 0.8789\n",
      "Epoch 1 Batch 2015 Loss 1.0135\n",
      "Epoch 1 Batch 2016 Loss 0.8518\n",
      "Epoch 1 Batch 2017 Loss 0.9111\n",
      "Epoch 1 Batch 2018 Loss 0.8573\n",
      "Epoch 1 Batch 2019 Loss 0.9963\n",
      "Epoch 1 Batch 2020 Loss 0.7992\n",
      "Epoch 1 Batch 2021 Loss 0.7474\n",
      "Epoch 1 Batch 2022 Loss 0.9013\n",
      "Epoch 1 Batch 2023 Loss 0.9328\n",
      "Epoch 1 Batch 2024 Loss 0.9097\n",
      "Epoch 1 Batch 2025 Loss 0.9468\n",
      "Epoch 1 Batch 2026 Loss 0.9236\n",
      "Epoch 1 Batch 2027 Loss 1.0564\n",
      "Epoch 1 Batch 2028 Loss 1.0188\n",
      "Epoch 1 Batch 2029 Loss 0.9395\n",
      "Epoch 1 Batch 2030 Loss 1.0838\n",
      "Epoch 1 Batch 2031 Loss 0.7632\n",
      "Epoch 1 Batch 2032 Loss 0.8856\n",
      "Epoch 1 Batch 2033 Loss 1.1808\n",
      "Epoch 1 Batch 2034 Loss 1.0368\n",
      "Epoch 1 Batch 2035 Loss 0.8427\n",
      "Epoch 1 Batch 2036 Loss 0.9163\n",
      "Epoch 1 Batch 2037 Loss 0.9710\n",
      "Epoch 1 Batch 2038 Loss 1.0033\n",
      "Epoch 1 Batch 2039 Loss 0.9436\n",
      "Epoch 1 Batch 2040 Loss 0.8950\n",
      "Epoch 1 Batch 2041 Loss 1.0153\n",
      "Epoch 1 Batch 2042 Loss 1.1778\n",
      "Epoch 1 Batch 2043 Loss 1.0044\n",
      "Epoch 1 Batch 2044 Loss 1.0388\n",
      "Epoch 1 Batch 2045 Loss 0.7618\n",
      "Epoch 1 Batch 2046 Loss 0.9770\n",
      "Epoch 1 Batch 2047 Loss 0.8977\n",
      "Epoch 1 Batch 2048 Loss 0.8000\n",
      "Epoch 1 Batch 2049 Loss 0.9600\n",
      "Epoch 1 Batch 2050 Loss 0.8528\n",
      "Epoch 1 Batch 2051 Loss 0.8735\n",
      "Epoch 1 Batch 2052 Loss 0.8732\n",
      "Epoch 1 Batch 2053 Loss 1.0273\n",
      "Epoch 1 Batch 2054 Loss 1.0175\n",
      "Epoch 1 Batch 2055 Loss 0.9673\n",
      "Epoch 1 Batch 2056 Loss 0.9484\n",
      "Epoch 1 Batch 2057 Loss 0.9912\n",
      "Epoch 1 Batch 2058 Loss 0.8483\n",
      "Epoch 1 Batch 2059 Loss 0.9953\n",
      "Epoch 1 Batch 2060 Loss 0.9872\n",
      "Epoch 1 Batch 2061 Loss 0.8129\n",
      "Epoch 1 Batch 2062 Loss 0.9240\n",
      "Epoch 1 Batch 2063 Loss 0.9803\n",
      "Epoch 1 Batch 2064 Loss 0.8870\n",
      "Epoch 1 Batch 2065 Loss 1.0787\n",
      "Epoch 1 Batch 2066 Loss 1.0881\n",
      "Epoch 1 Batch 2067 Loss 1.1476\n",
      "Epoch 1 Batch 2068 Loss 1.2614\n",
      "Epoch 1 Batch 2069 Loss 1.0944\n",
      "Epoch 1 Batch 2070 Loss 0.8331\n",
      "Epoch 1 Batch 2071 Loss 1.0380\n",
      "Epoch 1 Batch 2072 Loss 1.2499\n",
      "Epoch 1 Batch 2073 Loss 1.0911\n",
      "Epoch 1 Batch 2074 Loss 0.8870\n",
      "Epoch 1 Batch 2075 Loss 0.8862\n",
      "Epoch 1 Batch 2076 Loss 0.7117\n",
      "Epoch 1 Batch 2077 Loss 0.8849\n",
      "Epoch 1 Batch 2078 Loss 0.9825\n",
      "Epoch 1 Batch 2079 Loss 0.9189\n",
      "Epoch 1 Batch 2080 Loss 0.8505\n",
      "Epoch 1 Batch 2081 Loss 0.9485\n",
      "Epoch 1 Batch 2082 Loss 0.8266\n",
      "Epoch 1 Batch 2083 Loss 1.0270\n",
      "Epoch 1 Batch 2084 Loss 0.9697\n",
      "Epoch 1 Batch 2085 Loss 0.8544\n",
      "Epoch 1 Batch 2086 Loss 0.8489\n",
      "Epoch 1 Batch 2087 Loss 1.1022\n",
      "Epoch 1 Batch 2088 Loss 1.3306\n",
      "Epoch 1 Batch 2089 Loss 1.0378\n",
      "Epoch 1 Batch 2090 Loss 0.8983\n",
      "Epoch 1 Batch 2091 Loss 0.9634\n",
      "Epoch 1 Batch 2092 Loss 0.8170\n",
      "Epoch 1 Batch 2093 Loss 0.8592\n",
      "Epoch 1 Batch 2094 Loss 0.9814\n",
      "Epoch 1 Batch 2095 Loss 0.9839\n",
      "Epoch 1 Batch 2096 Loss 1.0524\n",
      "Epoch 1 Batch 2097 Loss 1.0653\n",
      "Epoch 1 Batch 2098 Loss 1.0979\n",
      "Epoch 1 Batch 2099 Loss 0.8587\n",
      "Epoch 1 Batch 2100 Loss 0.9860\n",
      "Epoch 1 Batch 2101 Loss 1.0056\n",
      "Epoch 1 Batch 2102 Loss 1.0047\n",
      "Epoch 1 Batch 2103 Loss 1.1457\n",
      "Epoch 1 Batch 2104 Loss 1.0298\n",
      "Epoch 1 Batch 2105 Loss 0.9286\n",
      "Epoch 1 Batch 2106 Loss 1.0037\n",
      "Epoch 1 Batch 2107 Loss 0.8802\n",
      "Epoch 1 Batch 2108 Loss 0.9906\n",
      "Epoch 1 Batch 2109 Loss 0.9832\n",
      "Epoch 1 Batch 2110 Loss 1.0169\n",
      "Epoch 1 Batch 2111 Loss 1.0904\n",
      "Epoch 1 Batch 2112 Loss 0.7423\n",
      "Epoch 1 Batch 2113 Loss 0.9058\n",
      "Epoch 1 Batch 2114 Loss 0.8816\n",
      "Epoch 1 Batch 2115 Loss 0.8098\n",
      "Epoch 1 Batch 2116 Loss 0.8591\n",
      "Epoch 1 Batch 2117 Loss 0.8670\n",
      "Epoch 1 Batch 2118 Loss 0.9218\n",
      "Epoch 1 Batch 2119 Loss 0.9724\n",
      "Epoch 1 Batch 2120 Loss 0.9045\n",
      "Epoch 1 Batch 2121 Loss 0.9417\n",
      "Epoch 1 Batch 2122 Loss 0.6664\n",
      "Epoch 1 Batch 2123 Loss 0.9200\n",
      "Epoch 1 Batch 2124 Loss 1.1389\n",
      "Epoch 1 Batch 2125 Loss 1.0057\n",
      "Epoch 1 Batch 2126 Loss 0.9684\n",
      "Epoch 1 Batch 2127 Loss 1.1966\n",
      "Epoch 1 Batch 2128 Loss 0.9113\n",
      "Epoch 1 Batch 2129 Loss 1.0428\n",
      "Epoch 1 Batch 2130 Loss 1.0365\n",
      "Epoch 1 Batch 2131 Loss 0.9398\n",
      "Epoch 1 Batch 2132 Loss 0.8552\n",
      "Epoch 1 Batch 2133 Loss 1.2367\n",
      "Epoch 1 Batch 2134 Loss 1.0883\n",
      "Epoch 1 Batch 2135 Loss 0.9187\n",
      "Epoch 1 Batch 2136 Loss 0.8974\n",
      "Epoch 1 Batch 2137 Loss 0.7842\n",
      "Epoch 1 Batch 2138 Loss 1.3848\n",
      "Epoch 1 Batch 2139 Loss 0.9601\n",
      "Epoch 1 Batch 2140 Loss 0.6814\n",
      "Epoch 1 Batch 2141 Loss 0.8882\n",
      "Epoch 1 Batch 2142 Loss 0.9374\n",
      "Epoch 1 Batch 2143 Loss 0.9537\n",
      "Epoch 1 Batch 2144 Loss 0.8679\n",
      "Epoch 1 Batch 2145 Loss 0.9988\n",
      "Epoch 1 Batch 2146 Loss 0.9821\n",
      "Epoch 1 Batch 2147 Loss 1.0469\n",
      "Epoch 1 Batch 2148 Loss 0.9726\n",
      "Epoch 1 Batch 2149 Loss 0.9803\n",
      "Epoch 1 Batch 2150 Loss 0.9896\n",
      "Epoch 1 Batch 2151 Loss 1.0858\n",
      "Epoch 1 Batch 2152 Loss 1.0697\n",
      "Epoch 1 Batch 2153 Loss 1.3263\n",
      "Epoch 1 Batch 2154 Loss 0.8935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 2155 Loss 0.7105\n",
      "Epoch 1 Batch 2156 Loss 0.9611\n",
      "Epoch 1 Batch 2157 Loss 0.7394\n",
      "Epoch 1 Batch 2158 Loss 0.8613\n",
      "Epoch 1 Batch 2159 Loss 1.1283\n",
      "Epoch 1 Batch 2160 Loss 0.8348\n",
      "Epoch 1 Batch 2161 Loss 1.0238\n",
      "Epoch 1 Batch 2162 Loss 0.9496\n",
      "Epoch 1 Batch 2163 Loss 0.9995\n",
      "Epoch 1 Batch 2164 Loss 0.8916\n",
      "Epoch 1 Batch 2165 Loss 0.9198\n",
      "Epoch 1 Batch 2166 Loss 0.7738\n",
      "Epoch 1 Batch 2167 Loss 0.8763\n",
      "Epoch 1 Batch 2168 Loss 0.8287\n",
      "Epoch 1 Batch 2169 Loss 0.7469\n",
      "Epoch 1 Batch 2170 Loss 0.9489\n",
      "Epoch 1 Batch 2171 Loss 0.8101\n",
      "Epoch 1 Batch 2172 Loss 0.9008\n",
      "Epoch 1 Batch 2173 Loss 0.9207\n",
      "Epoch 1 Batch 2174 Loss 0.9031\n",
      "Epoch 1 Batch 2175 Loss 0.9473\n",
      "Epoch 1 Batch 2176 Loss 1.2229\n",
      "Epoch 1 Batch 2177 Loss 1.1129\n",
      "Epoch 1 Batch 2178 Loss 0.9330\n",
      "Epoch 1 Batch 2179 Loss 0.9319\n",
      "Epoch 1 Batch 2180 Loss 1.1033\n",
      "Epoch 1 Batch 2181 Loss 0.8760\n",
      "Epoch 1 Batch 2182 Loss 0.8292\n",
      "Epoch 1 Batch 2183 Loss 1.1210\n",
      "Epoch 1 Batch 2184 Loss 0.7225\n",
      "Epoch 1 Batch 2185 Loss 0.9643\n",
      "Epoch 1 Batch 2186 Loss 0.8924\n",
      "Epoch 1 Batch 2187 Loss 0.8935\n",
      "Epoch 1 Batch 2188 Loss 1.0372\n",
      "Epoch 1 Batch 2189 Loss 0.8674\n",
      "Epoch 1 Batch 2190 Loss 0.8302\n",
      "Epoch 1 Batch 2191 Loss 0.7387\n",
      "Epoch 1 Batch 2192 Loss 0.7656\n",
      "Epoch 1 Batch 2193 Loss 0.9671\n",
      "Epoch 1 Batch 2194 Loss 0.9935\n",
      "Epoch 1 Batch 2195 Loss 0.7171\n",
      "Epoch 1 Batch 2196 Loss 0.8000\n",
      "Epoch 1 Batch 2197 Loss 0.6897\n",
      "Epoch 1 Batch 2198 Loss 0.8729\n",
      "Epoch 1 Batch 2199 Loss 0.8366\n",
      "Epoch 1 Batch 2200 Loss 1.0141\n",
      "Epoch 1 Batch 2201 Loss 0.8703\n",
      "Epoch 1 Batch 2202 Loss 0.8844\n",
      "Epoch 1 Batch 2203 Loss 0.7711\n",
      "Epoch 1 Batch 2204 Loss 1.0609\n",
      "Epoch 1 Batch 2205 Loss 1.0368\n",
      "Epoch 1 Batch 2206 Loss 0.8086\n",
      "Epoch 1 Batch 2207 Loss 1.0942\n",
      "Epoch 1 Batch 2208 Loss 0.9363\n",
      "Epoch 1 Batch 2209 Loss 0.9004\n",
      "Epoch 1 Batch 2210 Loss 1.0726\n",
      "Epoch 1 Batch 2211 Loss 0.7898\n",
      "Epoch 1 Batch 2212 Loss 1.1034\n",
      "Epoch 1 Batch 2213 Loss 0.7272\n",
      "Epoch 1 Batch 2214 Loss 1.1095\n",
      "Epoch 1 Batch 2215 Loss 0.9412\n",
      "Epoch 1 Batch 2216 Loss 0.9732\n",
      "Epoch 1 Batch 2217 Loss 0.9882\n",
      "Epoch 1 Batch 2218 Loss 1.0256\n",
      "Epoch 1 Batch 2219 Loss 0.9296\n",
      "Epoch 1 Batch 2220 Loss 1.0440\n",
      "Epoch 1 Batch 2221 Loss 0.9855\n",
      "Epoch 1 Batch 2222 Loss 0.9845\n",
      "Epoch 1 Batch 2223 Loss 1.0008\n",
      "Epoch 1 Batch 2224 Loss 1.0555\n",
      "Epoch 1 Batch 2225 Loss 1.0209\n",
      "Epoch 1 Batch 2226 Loss 1.0890\n",
      "Epoch 1 Batch 2227 Loss 0.9999\n",
      "Epoch 1 Batch 2228 Loss 0.9146\n",
      "Epoch 1 Batch 2229 Loss 1.0182\n",
      "Epoch 1 Batch 2230 Loss 0.8772\n",
      "Epoch 1 Batch 2231 Loss 1.0770\n",
      "Epoch 1 Batch 2232 Loss 0.8720\n",
      "Epoch 1 Batch 2233 Loss 0.7594\n",
      "Epoch 1 Batch 2234 Loss 1.0098\n",
      "Epoch 1 Batch 2235 Loss 0.8157\n",
      "Epoch 1 Batch 2236 Loss 0.7846\n",
      "Epoch 1 Batch 2237 Loss 0.8065\n",
      "Epoch 1 Batch 2238 Loss 1.1305\n",
      "Epoch 1 Batch 2239 Loss 1.0393\n",
      "Epoch 1 Batch 2240 Loss 0.8988\n",
      "Epoch 1 Batch 2241 Loss 1.2042\n",
      "Epoch 1 Batch 2242 Loss 0.9868\n",
      "Epoch 1 Batch 2243 Loss 1.2580\n",
      "Epoch 1 Batch 2244 Loss 1.0356\n",
      "Epoch 1 Batch 2245 Loss 0.8001\n",
      "Epoch 1 Batch 2246 Loss 0.8860\n",
      "Epoch 1 Batch 2247 Loss 1.0101\n",
      "Epoch 1 Batch 2248 Loss 1.1872\n",
      "Epoch 1 Batch 2249 Loss 0.8346\n",
      "Epoch 1 Batch 2250 Loss 0.8778\n",
      "Epoch 1 Batch 2251 Loss 1.1195\n",
      "Epoch 1 Batch 2252 Loss 0.8319\n",
      "Epoch 1 Batch 2253 Loss 1.0278\n",
      "Epoch 1 Batch 2254 Loss 0.9754\n",
      "Epoch 1 Batch 2255 Loss 1.1182\n",
      "Epoch 1 Batch 2256 Loss 1.0195\n",
      "Epoch 1 Batch 2257 Loss 0.7681\n",
      "Epoch 1 Batch 2258 Loss 0.9239\n",
      "Epoch 1 Batch 2259 Loss 0.7197\n",
      "Epoch 1 Batch 2260 Loss 0.8150\n",
      "Epoch 1 Batch 2261 Loss 0.7090\n",
      "Epoch 1 Batch 2262 Loss 0.9858\n",
      "Epoch 1 Batch 2263 Loss 1.0054\n",
      "Epoch 1 Batch 2264 Loss 0.8704\n",
      "Epoch 1 Batch 2265 Loss 0.9599\n",
      "Epoch 1 Batch 2266 Loss 1.1364\n",
      "Epoch 1 Batch 2267 Loss 1.1091\n",
      "Epoch 1 Batch 2268 Loss 0.9242\n",
      "Epoch 1 Batch 2269 Loss 1.1766\n",
      "Epoch 1 Batch 2270 Loss 1.0519\n",
      "Epoch 1 Batch 2271 Loss 0.9529\n",
      "Epoch 1 Batch 2272 Loss 0.7557\n",
      "Epoch 1 Batch 2273 Loss 0.8047\n",
      "Epoch 1 Batch 2274 Loss 1.0829\n",
      "Epoch 1 Batch 2275 Loss 0.9711\n",
      "Epoch 1 Batch 2276 Loss 0.7528\n",
      "Epoch 1 Batch 2277 Loss 0.9488\n",
      "Epoch 1 Batch 2278 Loss 1.1975\n",
      "Epoch 1 Batch 2279 Loss 1.0493\n",
      "Epoch 1 Batch 2280 Loss 1.1050\n",
      "Epoch 1 Batch 2281 Loss 0.8283\n",
      "Epoch 1 Batch 2282 Loss 0.9309\n",
      "Epoch 1 Batch 2283 Loss 0.9490\n",
      "Epoch 1 Batch 2284 Loss 1.0454\n",
      "Epoch 1 Batch 2285 Loss 1.0126\n",
      "Epoch 1 Batch 2286 Loss 0.8913\n",
      "Epoch 1 Batch 2287 Loss 0.8407\n",
      "Epoch 1 Batch 2288 Loss 0.9235\n",
      "Epoch 1 Batch 2289 Loss 0.9025\n",
      "Epoch 1 Batch 2290 Loss 1.0465\n",
      "Epoch 1 Batch 2291 Loss 1.1468\n",
      "Epoch 1 Batch 2292 Loss 1.0304\n",
      "Epoch 1 Batch 2293 Loss 0.9014\n",
      "Epoch 1 Batch 2294 Loss 1.1357\n",
      "Epoch 1 Batch 2295 Loss 1.0346\n",
      "Epoch 1 Batch 2296 Loss 0.8053\n",
      "Epoch 1 Batch 2297 Loss 1.0243\n",
      "Epoch 1 Batch 2298 Loss 0.9009\n",
      "Epoch 1 Batch 2299 Loss 0.9893\n",
      "Epoch 1 Batch 2300 Loss 0.8347\n",
      "Epoch 1 Batch 2301 Loss 0.8073\n",
      "Epoch 1 Batch 2302 Loss 0.9067\n",
      "Epoch 1 Batch 2303 Loss 0.8138\n",
      "Epoch 1 Batch 2304 Loss 1.0277\n",
      "Epoch 1 Batch 2305 Loss 0.9464\n",
      "Epoch 1 Batch 2306 Loss 0.9894\n",
      "Epoch 1 Batch 2307 Loss 0.8883\n",
      "Epoch 1 Batch 2308 Loss 1.0130\n",
      "Epoch 1 Batch 2309 Loss 0.9609\n",
      "Epoch 1 Batch 2310 Loss 0.9949\n",
      "Epoch 1 Batch 2311 Loss 1.1759\n",
      "Epoch 1 Batch 2312 Loss 0.7956\n",
      "Epoch 1 Batch 2313 Loss 1.1231\n",
      "Epoch 1 Batch 2314 Loss 0.9607\n",
      "Epoch 1 Batch 2315 Loss 1.2071\n",
      "Epoch 1 Batch 2316 Loss 0.8844\n",
      "Epoch 1 Batch 2317 Loss 1.0043\n",
      "Epoch 1 Batch 2318 Loss 1.0662\n",
      "Epoch 1 Batch 2319 Loss 0.7631\n",
      "Epoch 1 Batch 2320 Loss 0.8938\n",
      "Epoch 1 Batch 2321 Loss 0.8130\n",
      "Epoch 1 Batch 2322 Loss 0.9008\n",
      "Epoch 1 Batch 2323 Loss 0.8901\n",
      "Epoch 1 Batch 2324 Loss 0.8012\n",
      "Epoch 1 Batch 2325 Loss 0.8519\n",
      "Epoch 1 Batch 2326 Loss 0.9643\n",
      "Epoch 1 Batch 2327 Loss 0.9100\n",
      "Epoch 1 Batch 2328 Loss 0.9439\n",
      "Epoch 1 Batch 2329 Loss 0.8212\n",
      "Epoch 1 Batch 2330 Loss 1.0836\n",
      "Epoch 1 Batch 2331 Loss 0.8955\n",
      "Epoch 1 Batch 2332 Loss 0.9931\n",
      "Epoch 1 Batch 2333 Loss 0.7966\n",
      "Epoch 1 Batch 2334 Loss 1.0884\n",
      "Epoch 1 Batch 2335 Loss 1.0645\n",
      "Epoch 1 Batch 2336 Loss 0.9662\n",
      "Epoch 1 Batch 2337 Loss 0.9721\n",
      "Epoch 1 Batch 2338 Loss 0.9514\n",
      "Epoch 1 Batch 2339 Loss 1.0779\n",
      "Epoch 1 Batch 2340 Loss 0.8111\n",
      "Epoch 1 Batch 2341 Loss 0.9402\n",
      "Epoch 1 Batch 2342 Loss 0.9794\n",
      "Epoch 1 Batch 2343 Loss 0.9905\n",
      "Epoch 1 Batch 2344 Loss 0.9858\n",
      "Epoch 1 Batch 2345 Loss 0.9706\n",
      "Epoch 1 Batch 2346 Loss 0.9381\n",
      "Epoch 1 Batch 2347 Loss 0.8652\n",
      "Epoch 1 Batch 2348 Loss 1.1258\n",
      "Epoch 1 Batch 2349 Loss 1.0508\n",
      "Epoch 1 Batch 2350 Loss 0.8968\n",
      "Epoch 1 Batch 2351 Loss 0.9431\n",
      "Epoch 1 Batch 2352 Loss 0.9684\n",
      "Epoch 1 Batch 2353 Loss 0.9009\n",
      "Epoch 1 Batch 2354 Loss 0.8977\n",
      "Epoch 1 Batch 2355 Loss 1.0411\n",
      "Epoch 1 Batch 2356 Loss 0.9074\n",
      "Epoch 1 Batch 2357 Loss 1.0029\n",
      "Epoch 1 Batch 2358 Loss 0.8115\n",
      "Epoch 1 Batch 2359 Loss 1.0953\n",
      "Epoch 1 Batch 2360 Loss 1.1335\n",
      "Epoch 1 Batch 2361 Loss 0.7904\n",
      "Epoch 1 Batch 2362 Loss 0.7152\n",
      "Epoch 1 Batch 2363 Loss 1.1276\n",
      "Epoch 1 Batch 2364 Loss 0.9153\n",
      "Epoch 1 Batch 2365 Loss 0.8550\n",
      "Epoch 1 Batch 2366 Loss 1.0608\n",
      "Epoch 1 Batch 2367 Loss 0.8584\n",
      "Epoch 1 Batch 2368 Loss 0.7681\n",
      "Epoch 1 Batch 2369 Loss 0.7393\n",
      "Epoch 1 Batch 2370 Loss 1.1807\n",
      "Epoch 1 Batch 2371 Loss 0.8373\n",
      "Epoch 1 Batch 2372 Loss 0.9307\n",
      "Epoch 1 Batch 2373 Loss 1.0352\n",
      "Epoch 1 Batch 2374 Loss 1.0978\n",
      "Epoch 1 Batch 2375 Loss 0.9383\n",
      "Epoch 1 Batch 2376 Loss 1.1517\n",
      "Epoch 1 Batch 2377 Loss 0.9527\n",
      "Epoch 1 Batch 2378 Loss 0.9739\n",
      "Epoch 1 Batch 2379 Loss 0.9238\n",
      "Epoch 1 Batch 2380 Loss 0.8366\n",
      "Epoch 1 Batch 2381 Loss 0.8914\n",
      "Epoch 1 Batch 2382 Loss 1.1195\n",
      "Epoch 1 Batch 2383 Loss 1.1771\n",
      "Epoch 1 Batch 2384 Loss 0.9845\n",
      "Epoch 1 Batch 2385 Loss 0.9531\n",
      "Epoch 1 Batch 2386 Loss 0.8457\n",
      "Epoch 1 Batch 2387 Loss 1.0468\n",
      "Epoch 1 Batch 2388 Loss 0.9442\n",
      "Epoch 1 Batch 2389 Loss 1.0319\n",
      "Epoch 1 Batch 2390 Loss 0.8945\n",
      "Epoch 1 Batch 2391 Loss 1.0268\n",
      "Epoch 1 Batch 2392 Loss 0.9949\n",
      "Epoch 1 Batch 2393 Loss 1.1359\n",
      "Epoch 1 Batch 2394 Loss 0.9777\n",
      "Epoch 1 Batch 2395 Loss 0.9129\n",
      "Epoch 1 Batch 2396 Loss 1.1659\n",
      "Epoch 1 Batch 2397 Loss 1.1414\n",
      "Epoch 1 Batch 2398 Loss 0.8892\n",
      "Epoch 1 Batch 2399 Loss 0.7475\n",
      "Epoch 1 Batch 2400 Loss 1.0217\n",
      "Epoch 1 Batch 2401 Loss 0.7273\n",
      "Epoch 1 Batch 2402 Loss 1.1295\n",
      "Epoch 1 Batch 2403 Loss 1.0691\n",
      "Epoch 1 Batch 2404 Loss 0.9137\n",
      "Epoch 1 Batch 2405 Loss 0.9109\n",
      "Epoch 1 Batch 2406 Loss 0.8993\n",
      "Epoch 1 Batch 2407 Loss 1.0429\n",
      "Epoch 1 Batch 2408 Loss 1.0096\n",
      "Epoch 1 Batch 2409 Loss 0.9812\n",
      "Epoch 1 Batch 2410 Loss 1.1384\n",
      "Epoch 1 Batch 2411 Loss 0.9443\n",
      "Epoch 1 Batch 2412 Loss 1.0838\n",
      "Epoch 1 Batch 2413 Loss 0.8320\n",
      "Epoch 1 Batch 2414 Loss 1.0219\n",
      "Epoch 1 Batch 2415 Loss 0.8579\n",
      "Epoch 1 Batch 2416 Loss 1.0111\n",
      "Epoch 1 Batch 2417 Loss 0.8608\n",
      "Epoch 1 Batch 2418 Loss 0.7791\n",
      "Epoch 1 Batch 2419 Loss 1.0725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 2420 Loss 0.9319\n",
      "Epoch 1 Batch 2421 Loss 0.7659\n",
      "Epoch 1 Batch 2422 Loss 1.0614\n",
      "Epoch 1 Batch 2423 Loss 0.9844\n",
      "Epoch 1 Batch 2424 Loss 0.8317\n",
      "Epoch 1 Batch 2425 Loss 1.0684\n",
      "Epoch 1 Batch 2426 Loss 1.0954\n",
      "Epoch 1 Batch 2427 Loss 1.0728\n",
      "Epoch 1 Batch 2428 Loss 0.8780\n",
      "Epoch 1 Batch 2429 Loss 0.9096\n",
      "Epoch 1 Batch 2430 Loss 0.9847\n",
      "Epoch 1 Batch 2431 Loss 0.9570\n",
      "Epoch 1 Batch 2432 Loss 1.0934\n",
      "Epoch 1 Batch 2433 Loss 0.7019\n",
      "Epoch 1 Batch 2434 Loss 0.7767\n",
      "Epoch 1 Batch 2435 Loss 0.7556\n",
      "Epoch 1 Batch 2436 Loss 0.8304\n",
      "Epoch 1 Batch 2437 Loss 1.0913\n",
      "Epoch 1 Batch 2438 Loss 1.0961\n",
      "Epoch 1 Batch 2439 Loss 0.7741\n",
      "Epoch 1 Batch 2440 Loss 1.1380\n",
      "Epoch 1 Batch 2441 Loss 1.1333\n",
      "Epoch 1 Batch 2442 Loss 0.9665\n",
      "Epoch 1 Batch 2443 Loss 0.7963\n",
      "Epoch 1 Batch 2444 Loss 0.9862\n",
      "Epoch 1 Batch 2445 Loss 0.8301\n",
      "Epoch 1 Batch 2446 Loss 0.9506\n",
      "Epoch 1 Batch 2447 Loss 0.8574\n",
      "Epoch 1 Batch 2448 Loss 0.8507\n",
      "Epoch 1 Batch 2449 Loss 1.0471\n",
      "Epoch 1 Batch 2450 Loss 1.1104\n",
      "Epoch 1 Batch 2451 Loss 0.7832\n",
      "Epoch 1 Batch 2452 Loss 1.2294\n",
      "Epoch 1 Batch 2453 Loss 0.9913\n",
      "Epoch 1 Batch 2454 Loss 1.1563\n",
      "Epoch 1 Batch 2455 Loss 0.9739\n",
      "Epoch 1 Batch 2456 Loss 0.9009\n",
      "Epoch 1 Batch 2457 Loss 1.0648\n",
      "Epoch 1 Batch 2458 Loss 0.9420\n",
      "Epoch 1 Batch 2459 Loss 0.8567\n",
      "Epoch 1 Batch 2460 Loss 0.9356\n",
      "Epoch 1 Batch 2461 Loss 1.1195\n",
      "Epoch 1 Batch 2462 Loss 0.9526\n",
      "Epoch 1 Batch 2463 Loss 0.9856\n",
      "Epoch 1 Batch 2464 Loss 1.0786\n",
      "Epoch 1 Batch 2465 Loss 1.1904\n",
      "Epoch 1 Batch 2466 Loss 0.9626\n",
      "Epoch 1 Batch 2467 Loss 0.8305\n",
      "Epoch 1 Batch 2468 Loss 0.8892\n",
      "Epoch 1 Batch 2469 Loss 0.9184\n",
      "Epoch 1 Batch 2470 Loss 0.8975\n",
      "Epoch 1 Batch 2471 Loss 1.1311\n",
      "Epoch 1 Batch 2472 Loss 1.0773\n",
      "Epoch 1 Batch 2473 Loss 0.9113\n",
      "Epoch 1 Batch 2474 Loss 0.8942\n",
      "Epoch 1 Batch 2475 Loss 1.3098\n",
      "Epoch 1 Batch 2476 Loss 0.8023\n",
      "Epoch 1 Batch 2477 Loss 0.7200\n",
      "Epoch 1 Batch 2478 Loss 1.0404\n",
      "Epoch 1 Batch 2479 Loss 1.0475\n",
      "Epoch 1 Batch 2480 Loss 1.0824\n",
      "Epoch 1 Batch 2481 Loss 0.8101\n",
      "Epoch 1 Batch 2482 Loss 1.1666\n",
      "Epoch 1 Batch 2483 Loss 0.8903\n",
      "Epoch 1 Batch 2484 Loss 0.8622\n",
      "Epoch 1 Batch 2485 Loss 0.7458\n",
      "Epoch 1 Batch 2486 Loss 0.9320\n",
      "Epoch 1 Batch 2487 Loss 0.9336\n",
      "Epoch 1 Batch 2488 Loss 0.9977\n",
      "Epoch 1 Batch 2489 Loss 0.9272\n",
      "Epoch 1 Batch 2490 Loss 0.8514\n",
      "Epoch 1 Batch 2491 Loss 1.0754\n",
      "Epoch 1 Batch 2492 Loss 0.8986\n",
      "Epoch 1 Batch 2493 Loss 0.9413\n",
      "Epoch 1 Batch 2494 Loss 0.8620\n",
      "Epoch 1 Batch 2495 Loss 0.9697\n",
      "Epoch 1 Batch 2496 Loss 0.8563\n",
      "Epoch 1 Batch 2497 Loss 0.9082\n",
      "Epoch 1 Batch 2498 Loss 0.7253\n",
      "Epoch 1 Batch 2499 Loss 1.0059\n",
      "Epoch 1 Batch 2500 Loss 1.0603\n",
      "Epoch 1 Batch 2501 Loss 1.1346\n",
      "Epoch 1 Batch 2502 Loss 0.7862\n",
      "Epoch 1 Batch 2503 Loss 1.0640\n",
      "Epoch 1 Batch 2504 Loss 1.0332\n",
      "Epoch 1 Batch 2505 Loss 0.9994\n",
      "Epoch 1 Batch 2506 Loss 0.8581\n",
      "Epoch 1 Batch 2507 Loss 0.9163\n",
      "Epoch 1 Batch 2508 Loss 0.9998\n",
      "Epoch 1 Batch 2509 Loss 1.0275\n",
      "Epoch 1 Batch 2510 Loss 0.9030\n",
      "Epoch 1 Batch 2511 Loss 1.0886\n",
      "Epoch 1 Batch 2512 Loss 0.8660\n",
      "Epoch 1 Batch 2513 Loss 0.9376\n",
      "Epoch 1 Batch 2514 Loss 0.7647\n",
      "Epoch 1 Batch 2515 Loss 1.3058\n",
      "Epoch 1 Batch 2516 Loss 0.9463\n",
      "Epoch 1 Batch 2517 Loss 1.0997\n",
      "Epoch 1 Batch 2518 Loss 0.7643\n",
      "Epoch 1 Batch 2519 Loss 0.7974\n",
      "Epoch 1 Batch 2520 Loss 0.9924\n",
      "Epoch 1 Batch 2521 Loss 1.0911\n",
      "Epoch 1 Batch 2522 Loss 1.0075\n",
      "Epoch 1 Batch 2523 Loss 0.8939\n",
      "Epoch 1 Batch 2524 Loss 0.9617\n",
      "Epoch 1 Batch 2525 Loss 0.9778\n",
      "Epoch 1 Batch 2526 Loss 1.2680\n",
      "Epoch 1 Batch 2527 Loss 0.8957\n",
      "Epoch 1 Batch 2528 Loss 1.0711\n",
      "Epoch 1 Batch 2529 Loss 0.8664\n",
      "Epoch 1 Batch 2530 Loss 0.9972\n",
      "Epoch 1 Batch 2531 Loss 1.0061\n",
      "Epoch 1 Batch 2532 Loss 0.9594\n",
      "Epoch 1 Batch 2533 Loss 1.1491\n",
      "Epoch 1 Batch 2534 Loss 0.8685\n",
      "Epoch 1 Batch 2535 Loss 1.0142\n",
      "Epoch 1 Batch 2536 Loss 1.2570\n",
      "Epoch 1 Batch 2537 Loss 0.8805\n",
      "Epoch 1 Batch 2538 Loss 0.8638\n",
      "Epoch 1 Batch 2539 Loss 0.9482\n",
      "Epoch 1 Batch 2540 Loss 1.1243\n",
      "Epoch 1 Batch 2541 Loss 0.9325\n",
      "Epoch 1 Batch 2542 Loss 0.9028\n",
      "Epoch 1 Batch 2543 Loss 0.8901\n",
      "Epoch 1 Batch 2544 Loss 0.8101\n",
      "Epoch 1 Batch 2545 Loss 1.2541\n",
      "Epoch 1 Batch 2546 Loss 0.8567\n",
      "Epoch 1 Batch 2547 Loss 1.1041\n",
      "Epoch 1 Batch 2548 Loss 1.0226\n",
      "Epoch 1 Batch 2549 Loss 1.1043\n",
      "Epoch 1 Batch 2550 Loss 0.8946\n",
      "Epoch 1 Batch 2551 Loss 0.8590\n",
      "Epoch 1 Batch 2552 Loss 0.8618\n",
      "Epoch 1 Batch 2553 Loss 0.8256\n",
      "Epoch 1 Batch 2554 Loss 0.9805\n",
      "Epoch 1 Batch 2555 Loss 0.9547\n",
      "Epoch 1 Batch 2556 Loss 1.1050\n",
      "Epoch 1 Batch 2557 Loss 1.0992\n",
      "Epoch 1 Batch 2558 Loss 0.7911\n",
      "Epoch 1 Batch 2559 Loss 0.7692\n",
      "Epoch 1 Batch 2560 Loss 0.9852\n",
      "Epoch 1 Batch 2561 Loss 0.8693\n",
      "Epoch 1 Batch 2562 Loss 1.1656\n",
      "Epoch 1 Batch 2563 Loss 0.9931\n",
      "Epoch 1 Batch 2564 Loss 0.8039\n",
      "Epoch 1 Batch 2565 Loss 0.7900\n",
      "Epoch 1 Batch 2566 Loss 0.9262\n",
      "Epoch 1 Batch 2567 Loss 0.8599\n",
      "Epoch 1 Batch 2568 Loss 0.8566\n",
      "Epoch 1 Batch 2569 Loss 1.2658\n",
      "Epoch 1 Batch 2570 Loss 0.9500\n",
      "Epoch 1 Batch 2571 Loss 0.9036\n",
      "Epoch 1 Batch 2572 Loss 1.1096\n",
      "Epoch 1 Batch 2573 Loss 0.7876\n",
      "Epoch 1 Batch 2574 Loss 1.1569\n",
      "Epoch 1 Batch 2575 Loss 0.9473\n",
      "Epoch 1 Batch 2576 Loss 0.8671\n",
      "Epoch 1 Batch 2577 Loss 0.8343\n",
      "Epoch 1 Batch 2578 Loss 0.8604\n",
      "Epoch 1 Batch 2579 Loss 0.9802\n",
      "Epoch 1 Batch 2580 Loss 1.0869\n",
      "Epoch 1 Batch 2581 Loss 1.1697\n",
      "Epoch 1 Batch 2582 Loss 0.9483\n",
      "Epoch 1 Batch 2583 Loss 0.8145\n",
      "Epoch 1 Batch 2584 Loss 0.8913\n",
      "Epoch 1 Batch 2585 Loss 0.9279\n",
      "Epoch 1 Batch 2586 Loss 0.9906\n",
      "Epoch 1 Batch 2587 Loss 0.9358\n",
      "Epoch 1 Batch 2588 Loss 1.2394\n",
      "Epoch 1 Loss 0.9259\n",
      "Time taken for 1 epoch 999.9490554332733 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8772\n",
      "Epoch 2 Batch 1 Loss 0.8222\n",
      "Epoch 2 Batch 2 Loss 0.7115\n",
      "Epoch 2 Batch 3 Loss 0.7813\n",
      "Epoch 2 Batch 4 Loss 0.8753\n",
      "Epoch 2 Batch 5 Loss 0.8823\n",
      "Epoch 2 Batch 6 Loss 0.8059\n",
      "Epoch 2 Batch 7 Loss 0.7796\n",
      "Epoch 2 Batch 8 Loss 0.5611\n",
      "Epoch 2 Batch 9 Loss 0.8998\n",
      "Epoch 2 Batch 10 Loss 0.7424\n",
      "Epoch 2 Batch 11 Loss 0.7205\n",
      "Epoch 2 Batch 12 Loss 0.7800\n",
      "Epoch 2 Batch 13 Loss 0.9880\n",
      "Epoch 2 Batch 14 Loss 0.7581\n",
      "Epoch 2 Batch 15 Loss 0.7251\n",
      "Epoch 2 Batch 16 Loss 0.8297\n",
      "Epoch 2 Batch 17 Loss 0.7946\n",
      "Epoch 2 Batch 18 Loss 0.8057\n",
      "Epoch 2 Batch 19 Loss 0.7244\n",
      "Epoch 2 Batch 20 Loss 0.6509\n",
      "Epoch 2 Batch 21 Loss 0.8829\n",
      "Epoch 2 Batch 22 Loss 0.7143\n",
      "Epoch 2 Batch 23 Loss 0.8336\n",
      "Epoch 2 Batch 24 Loss 0.9326\n",
      "Epoch 2 Batch 25 Loss 0.7949\n",
      "Epoch 2 Batch 26 Loss 0.7802\n",
      "Epoch 2 Batch 27 Loss 0.9233\n",
      "Epoch 2 Batch 28 Loss 0.8299\n",
      "Epoch 2 Batch 29 Loss 0.7358\n",
      "Epoch 2 Batch 30 Loss 0.6768\n",
      "Epoch 2 Batch 31 Loss 0.7550\n",
      "Epoch 2 Batch 32 Loss 0.9066\n",
      "Epoch 2 Batch 33 Loss 0.8785\n",
      "Epoch 2 Batch 34 Loss 0.8051\n",
      "Epoch 2 Batch 35 Loss 1.1743\n",
      "Epoch 2 Batch 36 Loss 0.7868\n",
      "Epoch 2 Batch 37 Loss 0.6066\n",
      "Epoch 2 Batch 38 Loss 0.6768\n",
      "Epoch 2 Batch 39 Loss 0.8984\n",
      "Epoch 2 Batch 40 Loss 1.0287\n",
      "Epoch 2 Batch 41 Loss 0.7464\n",
      "Epoch 2 Batch 42 Loss 0.9215\n",
      "Epoch 2 Batch 43 Loss 0.8008\n",
      "Epoch 2 Batch 44 Loss 0.8243\n",
      "Epoch 2 Batch 45 Loss 0.7222\n",
      "Epoch 2 Batch 46 Loss 0.9565\n",
      "Epoch 2 Batch 47 Loss 0.7050\n",
      "Epoch 2 Batch 48 Loss 0.9289\n",
      "Epoch 2 Batch 49 Loss 0.8027\n",
      "Epoch 2 Batch 50 Loss 0.8661\n",
      "Epoch 2 Batch 51 Loss 0.8978\n",
      "Epoch 2 Batch 52 Loss 1.0437\n",
      "Epoch 2 Batch 53 Loss 0.8892\n",
      "Epoch 2 Batch 54 Loss 0.7847\n",
      "Epoch 2 Batch 55 Loss 0.9526\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-18b8047d8d32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#         print('第{}个batch:   \\n 输入训练数据： {} \\n 输入标签： {}'.format(batch, inp.shape, targ.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Installer-software\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "#         print('第{}个batch:   \\n 输入训练数据： {} \\n 输入标签： {}'.format(batch, inp.shape, targ.shape))\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ = 41\n",
    "max_length_inp = 200\n",
    "units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,inputs):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_output, enc_hidden = model.encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([vocab['<START>']], 0)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        # max_length_targ：要预测的这句话的最大的长度，如果是40，就会执行40个循环\n",
    "        # 要么遇到结尾符，要么运行至整个循环结束\n",
    "        \n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        \n",
    "        # 预测的时候也是一样，拿到model以后，一步一步的进行decode\n",
    "        # 这里输入三个参数之后，得到结果中依旧会有一个dec_hidden,这是这个时间步的隐藏层的输出\n",
    "        # 而第一次传进去的dec_hidden是encoder层输出的隐藏层信息（输入为<START>时）\n",
    "        # 再往后就是decoder层中的当前进行预测的时间步的上一个时间步输出的隐藏状态\n",
    "        # 实现了dec_hidden这个变量的复用，就是实现了每循环一次就对dec_hidden进行更新，下一次循环\n",
    "        # 的时候，把更新后的dec_hidden传进去\n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        # 拿到预测结果之后来取概率值最大的ID\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += reverse_vocab[predicted_id] + ' '\n",
    "        # 如果概率值最大的ID对应的是<STOP>，表示到达句尾，就直接返回这句话\n",
    "        if reverse_vocab[predicted_id] == '<STOP>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sentence = preprocess_sentence(sentence,max_length_inp,vocab)\n",
    "    \n",
    "    result, sentence, attention_plot = evaluate(model,sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单句预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence='北京 汽车 BJ 20 自动挡 最低 配 <UNK> 速 续航 技师说'\n",
    "\n",
    "# translate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(inps):\n",
    "    \"\"\"\n",
    "    这里的输入就是一个batch_size大小的输入，比如说有32句话，句子长度为200，那么这里就是32*200为\n",
    "    大小的输入\n",
    "    \"\"\"\n",
    "    # 判断输入长度\n",
    "    batch_size=len(inps)\n",
    "    # 开辟结果存储list\n",
    "    preidicts=[''] * batch_size\n",
    "    \n",
    "    inps = tf.convert_to_tensor(inps)\n",
    "    # 0. 初始化隐藏层输入\n",
    "    hidden = [tf.zeros((batch_size, units))]\n",
    "    # 1. 构建encoder\n",
    "    enc_output, enc_hidden = model.encoder(inps, hidden)\n",
    "    # 2. 复制\n",
    "    dec_hidden = enc_hidden\n",
    "    # 3. <START> * BATCH_SIZE   为传进来的batch_size大小的句子集填充<START>\n",
    "    # 训练的时候也一样，会初始化很多的<START>\n",
    "    dec_input = tf.expand_dims([vocab['<START>']] * batch_size, 1)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(max_length_targ):\n",
    "        # 计算上下文\n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        # 单步预测\n",
    "        # 拿到一个batch所有的<START>之后，全部输入到decoder里边\n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "        \n",
    "        # id转换 贪婪搜索  拿到预测的结果，取预测结果的概率最大值\n",
    "        # 这里的axis=1表示横向取最大值，相当于每一个句子里边取概率最大的那一个词\n",
    "        # 这里由于是一个batch一个batch的预测，一次预测的是32个句子，则第一步输入32个<START>\n",
    "        # 这里就会得到32个概率最高的词对应的index\n",
    "        predicted_ids = tf.argmax(predictions,axis=1).numpy()\n",
    "        \n",
    "        # 将这里得到的32index分别赋值到不同的句子里边去，用一个字典来保存32个句子\n",
    "        for index,predicted_id in enumerate(predicted_ids):\n",
    "            preidicts[index]+= reverse_vocab[predicted_id] + ' '\n",
    "            # 这里就是把预测出来的index对应的词放到字典中来\n",
    "        \n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "\n",
    "    results=[]  # 最后返回结果\n",
    "    for preidict in preidicts:\n",
    "        # 去掉句子前后空格\n",
    "        preidict=preidict.strip()\n",
    "        # 句子小于max len就结束了 截断\n",
    "        if '<STOP>' in preidict:\n",
    "            # 截断stop\n",
    "            preidict=preidict[:preidict.index('<STOP>')]\n",
    "        # 保存结果\n",
    "        results.append(preidict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是预测结果的代码，就是将要预测的data_X(测试集，有20000个),和batch大小传进去\n",
    "# 进行一个batch一个batch的预测\n",
    "# 将预测出来的结果拼接在一起，预测结束后会拿到20000个句子\n",
    "def model_predict(data_X,batch_size):\n",
    "    # 存储结果\n",
    "    results=[]\n",
    "    # 样本数量\n",
    "    sample_size=len(data_X)\n",
    "    # batch 操作轮数 math.ceil向上取整 小数 +1\n",
    "    # 因为最后一个batch可能不足一个batch size 大小 ,但是依然需要计算  \n",
    "    steps_epoch = math.ceil(sample_size/batch_size)\n",
    "    # [0,steps_epoch)\n",
    "    for i in tqdm(range(steps_epoch)):\n",
    "        batch_data = data_X[i*batch_size:(i+1)*batch_size]\n",
    "        results+=batch_predict(batch_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 625/625 [05:01<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results=model_predict(test_X,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建提交结果文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1</td>\n",
       "      <td>大众(进口)</td>\n",
       "      <td>高尔夫(进口)</td>\n",
       "      <td>我的帕萨特烧机油怎么办怎么办？</td>\n",
       "      <td>技师说：你好，请问你的车跑了多少公里了，如果在保修期内，可以到当地的4店里面进行检查维修。如...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>一汽-大众奥迪</td>\n",
       "      <td>奥迪A6</td>\n",
       "      <td>修一下多少钱是换还是修</td>\n",
       "      <td>技师说：你好师傅！抛光处理一下就好了！50元左右就好了，希望能够帮到你！祝你生活愉快！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q3</td>\n",
       "      <td>上汽大众</td>\n",
       "      <td>帕萨特</td>\n",
       "      <td>帕萨特领域    喇叭坏了  店里说方向盘里线坏了 换一根两三百不等 感觉太贵</td>\n",
       "      <td>技师说：你好，气囊油丝坏了吗，这个价格不贵。可以更换。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q4</td>\n",
       "      <td>南京菲亚特</td>\n",
       "      <td>派力奥</td>\n",
       "      <td>发动机漏气会有什么征兆？</td>\n",
       "      <td>技师说：你好！一：发动机没力，并伴有“啪啪”的漏气声音。二：发动机没力，并伴有排气管冒黑烟。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q5</td>\n",
       "      <td>东风本田</td>\n",
       "      <td>思铂睿</td>\n",
       "      <td>请问 那天右后胎扎了订，补了胎后跑高速80多开始有点抖，110时速以上抖动明显，以为是未做动...</td>\n",
       "      <td>技师说：你好师傅！可能前轮平衡快脱落或者不平衡造成的！建议前轮做一下动平衡就好了！希望能够帮...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID    Brand    Model                                           Question  \\\n",
       "0  Q1   大众(进口)  高尔夫(进口)                                    我的帕萨特烧机油怎么办怎么办？   \n",
       "1  Q2  一汽-大众奥迪     奥迪A6                                        修一下多少钱是换还是修   \n",
       "2  Q3     上汽大众      帕萨特           帕萨特领域    喇叭坏了  店里说方向盘里线坏了 换一根两三百不等 感觉太贵    \n",
       "3  Q4    南京菲亚特      派力奥                                       发动机漏气会有什么征兆？   \n",
       "4  Q5     东风本田      思铂睿  请问 那天右后胎扎了订，补了胎后跑高速80多开始有点抖，110时速以上抖动明显，以为是未做动...   \n",
       "\n",
       "                                            Dialogue  \n",
       "0  技师说：你好，请问你的车跑了多少公里了，如果在保修期内，可以到当地的4店里面进行检查维修。如...  \n",
       "1        技师说：你好师傅！抛光处理一下就好了！50元左右就好了，希望能够帮到你！祝你生活愉快！  \n",
       "2                        技师说：你好，气囊油丝坏了吗，这个价格不贵。可以更换。  \n",
       "3  技师说：你好！一：发动机没力，并伴有“啪啪”的漏气声音。二：发动机没力，并伴有排气管冒黑烟。...  \n",
       "4  技师说：你好师傅！可能前轮平衡快脱落或者不平衡造成的！建议前轮做一下动平衡就好了！希望能够帮...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入测试集数据\n",
    "test_df=pd.read_csv(test_data_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断一下是否有空值   没啥实际的作用看着\n",
    "for idx,result in enumerate(results):\n",
    "    if result=='':print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 赋值结果  将前边得到结果赋值给预测的这一列\n",
    "test_df['Prediction'] = results\n",
    "#　提取ID和预测结果两列  再从上边导入的测试集中拿到QID这一列\n",
    "test_df = test_df[['QID','Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1</td>\n",
       "      <td>烧 机油 ， 需要 检查 ， 维修 店 进行 维修 ， 检查 机油 消耗 过大 活塞环 气门...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>师傅 ， 抛光 处理 一下 ！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q3</td>\n",
       "      <td>气囊 游丝 ， 价格 不 贵 ， 更换 气囊 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q4</td>\n",
       "      <td>分析 发动机 无 反应 ， 遇到 噗 噗声 噗 噗声 流水 生 属于 正常 现象 ， 白烟 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q5</td>\n",
       "      <td>客户 解释 ， 轮胎 动平衡 问题 ， 轮胎 动平衡 问题 ， 轮胎 动平衡 问题 ， 轮胎...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19995</td>\n",
       "      <td>Q19996</td>\n",
       "      <td>， 进气 压力 传感器 进气 VVT VVT 链轮 都 会 出现 ， 进气 VVT VVT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19996</td>\n",
       "      <td>Q19997</td>\n",
       "      <td>原厂 配件 汽车厂家 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19997</td>\n",
       "      <td>Q19998</td>\n",
       "      <td>车辆 不要 水洗 ， 拆掉 电瓶 负极 拆掉 避免 电瓶 亏损 ， 不用 经常 跑跑 高速 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19998</td>\n",
       "      <td>Q19999</td>\n",
       "      <td>砂纸 进行 焊接 一层 垫 之后 ， 前轮 压着 一点 深 一点 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>Q20000</td>\n",
       "      <td>， 这种 情况 没事 尽量 不要 操作 ， 容易 打坏 齿轮</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          QID                                         Prediction\n",
       "0          Q1  烧 机油 ， 需要 检查 ， 维修 店 进行 维修 ， 检查 机油 消耗 过大 活塞环 气门...\n",
       "1          Q2                                   师傅 ， 抛光 处理 一下 ！ \n",
       "2          Q3                          气囊 游丝 ， 价格 不 贵 ， 更换 气囊 。 \n",
       "3          Q4  分析 发动机 无 反应 ， 遇到 噗 噗声 噗 噗声 流水 生 属于 正常 现象 ， 白烟 ...\n",
       "4          Q5  客户 解释 ， 轮胎 动平衡 问题 ， 轮胎 动平衡 问题 ， 轮胎 动平衡 问题 ， 轮胎...\n",
       "...       ...                                                ...\n",
       "19995  Q19996  ， 进气 压力 传感器 进气 VVT VVT 链轮 都 会 出现 ， 进气 VVT VVT ...\n",
       "19996  Q19997                                      原厂 配件 汽车厂家 。 \n",
       "19997  Q19998  车辆 不要 水洗 ， 拆掉 电瓶 负极 拆掉 避免 电瓶 亏损 ， 不用 经常 跑跑 高速 ...\n",
       "19998  Q19999                砂纸 进行 焊接 一层 垫 之后 ， 前轮 压着 一点 深 一点 。 \n",
       "19999  Q20000                    ， 这种 情况 没事 尽量 不要 操作 ， 容易 打坏 齿轮 \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后结果的补救措施，就是把结果中，！。去掉，把所有的空格删掉\n",
    "def submit_proc(sentence):\n",
    "    sentence=sentence.lstrip(' ，！。')\n",
    "    sentence=sentence.replace(' ','')\n",
    "    if sentence=='':\n",
    "        sentence='随时联系'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Prediction']=test_df['Prediction'].apply(submit_proc)  # 进行一下预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1</td>\n",
       "      <td>烧机油，需要检查，维修店进行维修，检查机油消耗过大活塞环气门间隙过大气门气门间隙过大气门气门...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>师傅，抛光处理一下！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q3</td>\n",
       "      <td>气囊游丝，价格不贵，更换气囊。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q4</td>\n",
       "      <td>分析发动机无反应，遇到噗噗声噗噗声流水生属于正常现象，白烟，多数发动机漏气，需要检查发动机缸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q5</td>\n",
       "      <td>客户解释，轮胎动平衡问题，轮胎动平衡问题，轮胎动平衡问题，轮胎动平衡问题，轮胎动平衡问题，轮...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID                                         Prediction\n",
       "0  Q1  烧机油，需要检查，维修店进行维修，检查机油消耗过大活塞环气门间隙过大气门气门间隙过大气门气门...\n",
       "1  Q2                                         师傅，抛光处理一下！\n",
       "2  Q3                                    气囊游丝，价格不贵，更换气囊。\n",
       "3  Q4  分析发动机无反应，遇到噗噗声噗噗声流水生属于正常现象，白烟，多数发动机漏气，需要检查发动机缸...\n",
       "4  Q5  客户解释，轮胎动平衡问题，轮胎动平衡问题，轮胎动平衡问题，轮胎动平衡问题，轮胎动平衡问题，轮..."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_result_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取结果存储路径  随机生成函数名的方法，防止多次生成结果，会搞混，这里在文件名中加上时间戳，batch大小，\n",
    "# 训练了多少轮，最大长度，embedding_size这些信息，这样的话好处就是，经过不断地修改，发现结果一直不如\n",
    "# 之前的某一轮的结果好，这样的话就可以直接找到，并且看到当时的参数设置信息\n",
    "result_save_path = get_result_filename(params[\"batch_size\"],params[\"epochs\"] , params[\"max_enc_len\"], params[\"embed_size\"], commit='_4_1_submit_proc_add_masks_loss_seq2seq_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
