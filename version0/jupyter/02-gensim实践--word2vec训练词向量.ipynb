{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取项目根目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/Learning/Project/QA/version1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = pathlib.Path(os.path.abspath('__file__')).parent.parent\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  gensim实践：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Gensim这个包就可以进行我们word2vec的完整的训练过程，训练后得到词向量\n",
    "\n",
    "1. 读取预处理好的数据\n",
    "2. 训练\n",
    "3. 完事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merger_data_path = os.path.join(root, 'data', 'merged_train_test_seg_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merger_data_path data size 102871\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>方向机 重 助力 泵 方向机 都 换 新 都 换 助力 泵 方向机 换 方向机 带 助力 重...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>奔驰 ML500 排气 凸轮轴 调节 错误 有没有 电脑 检测 故障 代码 有发 一下 发动...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2010 款 宝马X1 2011 年 出厂 20 排量 通用 6L45 变速箱 原地 换挡 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30V6 发动机 号 位置 照片 最好 右侧 排气管 上方 缸体 上 靠近 变速箱 是不是 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2012 款 奔驰 c180 维修保养 动力 值得 拥有 家庭 用车 入手 维修保养 费用 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  方向机 重 助力 泵 方向机 都 换 新 都 换 助力 泵 方向机 换 方向机 带 助力 重...\n",
       "1  奔驰 ML500 排气 凸轮轴 调节 错误 有没有 电脑 检测 故障 代码 有发 一下 发动...\n",
       "2  2010 款 宝马X1 2011 年 出厂 20 排量 通用 6L45 变速箱 原地 换挡 ...\n",
       "3  30V6 发动机 号 位置 照片 最好 右侧 排气管 上方 缸体 上 靠近 变速箱 是不是 ...\n",
       "4  2012 款 奔驰 c180 维修保养 动力 值得 拥有 家庭 用车 入手 维修保养 费用 ..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里其实并不需要载入这部分数据，只是载入进来看一下\n",
    "# 载入的时候设置不读header\n",
    "merger_df = pd.read_csv(merger_data_path,header=None)\n",
    "print('merger_data_path data size {}'.format(len(merger_df)))\n",
    "merger_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型创建\n",
    "\n",
    "Gensim中 Word2Vec 模型的期望输入是进过分词的句子列表，即是某个二维数组。这里我们暂时使用 Python 内置的数组，不过其在输入数据集较大的情况下会占用大量的 RAM。Gensim 本身只是要求能够迭代的有序句子列表，因此在工程实践中我们可以使用自定义的生成器，只在内存中保存单条语句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 参数\n",
    "+ min_count\n",
    "\n",
    "在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。\n",
    "\n",
    "+ size\n",
    "\n",
    "<span class=\"burk\">size参数主要是用来设置神经网络的层数，即词向量的大小</span>。Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。\n",
    "\n",
    "+ workers\n",
    "\n",
    "workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用：\n",
    "最好定义成机器的核数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models.word2vec import LineSentence  # 可以按行读取训练好的语料，传入语料的路径自己就会去读\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Learning/Project/QA/data/merged_train_test_seg_data.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-16 18:21:46,724 : INFO : collecting all words and their counts\n",
      "2020-05-16 18:21:46,725 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-05-16 18:21:46,956 : INFO : PROGRESS: at sentence #10000, processed 937272 words, keeping 36653 word types\n",
      "2020-05-16 18:21:47,217 : INFO : PROGRESS: at sentence #20000, processed 1889029 words, keeping 53934 word types\n",
      "2020-05-16 18:21:47,478 : INFO : PROGRESS: at sentence #30000, processed 2829436 words, keeping 66707 word types\n",
      "2020-05-16 18:21:47,728 : INFO : PROGRESS: at sentence #40000, processed 3741910 words, keeping 77608 word types\n",
      "2020-05-16 18:21:47,992 : INFO : PROGRESS: at sentence #50000, processed 4714600 words, keeping 87459 word types\n",
      "2020-05-16 18:21:48,278 : INFO : PROGRESS: at sentence #60000, processed 5748570 words, keeping 97386 word types\n",
      "2020-05-16 18:21:48,559 : INFO : PROGRESS: at sentence #70000, processed 6805872 words, keeping 106961 word types\n",
      "2020-05-16 18:21:48,788 : INFO : PROGRESS: at sentence #80000, processed 7748075 words, keeping 115064 word types\n",
      "2020-05-16 18:21:49,013 : INFO : PROGRESS: at sentence #90000, processed 8606034 words, keeping 122978 word types\n",
      "2020-05-16 18:21:49,244 : INFO : PROGRESS: at sentence #100000, processed 9455619 words, keeping 130010 word types\n",
      "2020-05-16 18:21:49,321 : INFO : collected 132022 word types from a corpus of 9704885 raw words and 102871 sentences\n",
      "2020-05-16 18:21:49,322 : INFO : Loading a fresh vocabulary\n",
      "2020-05-16 18:21:49,453 : INFO : effective_min_count=5 retains 32801 unique words (24% of original 132022, drops 99221)\n",
      "2020-05-16 18:21:49,454 : INFO : effective_min_count=5 leaves 9555787 word corpus (98% of original 9704885, drops 149098)\n",
      "2020-05-16 18:21:49,541 : INFO : deleting the raw counts dictionary of 132022 items\n",
      "2020-05-16 18:21:49,544 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-05-16 18:21:49,545 : INFO : downsampling leaves estimated 8595117 word corpus (89.9% of prior 9555787)\n",
      "2020-05-16 18:21:49,630 : INFO : estimated required memory for 32801 words and 200 dimensions: 68882100 bytes\n",
      "2020-05-16 18:21:49,630 : INFO : resetting layer weights\n",
      "2020-05-16 18:21:54,949 : INFO : training model with 8 workers on 32801 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-05-16 18:21:55,962 : INFO : EPOCH 1 - PROGRESS: at 15.29% examples, 1296745 words/s, in_qsize 15, out_qsize 1\n",
      "2020-05-16 18:21:56,965 : INFO : EPOCH 1 - PROGRESS: at 32.93% examples, 1399899 words/s, in_qsize 14, out_qsize 0\n",
      "2020-05-16 18:21:57,966 : INFO : EPOCH 1 - PROGRESS: at 48.96% examples, 1395297 words/s, in_qsize 8, out_qsize 0\n",
      "2020-05-16 18:21:58,967 : INFO : EPOCH 1 - PROGRESS: at 64.90% examples, 1420893 words/s, in_qsize 14, out_qsize 0\n",
      "2020-05-16 18:21:59,991 : INFO : EPOCH 1 - PROGRESS: at 80.62% examples, 1406853 words/s, in_qsize 13, out_qsize 2\n",
      "2020-05-16 18:22:00,972 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-16 18:22:00,973 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-16 18:22:00,973 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-16 18:22:00,982 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-16 18:22:00,986 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-16 18:22:00,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-16 18:22:00,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-16 18:22:00,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-16 18:22:00,990 : INFO : EPOCH - 1 : training on 9704885 raw words (8594597 effective words) took 6.0s, 1423473 effective words/s\n",
      "2020-05-16 18:22:01,996 : INFO : EPOCH 2 - PROGRESS: at 15.19% examples, 1295773 words/s, in_qsize 13, out_qsize 1\n",
      "2020-05-16 18:22:02,999 : INFO : EPOCH 2 - PROGRESS: at 32.57% examples, 1391345 words/s, in_qsize 14, out_qsize 1\n",
      "2020-05-16 18:22:04,024 : INFO : EPOCH 2 - PROGRESS: at 49.15% examples, 1392858 words/s, in_qsize 8, out_qsize 2\n",
      "2020-05-16 18:22:05,029 : INFO : EPOCH 2 - PROGRESS: at 64.81% examples, 1411302 words/s, in_qsize 15, out_qsize 1\n",
      "2020-05-16 18:22:06,030 : INFO : EPOCH 2 - PROGRESS: at 80.36% examples, 1403984 words/s, in_qsize 8, out_qsize 1\n",
      "2020-05-16 18:22:07,031 : INFO : EPOCH 2 - PROGRESS: at 99.09% examples, 1410919 words/s, in_qsize 9, out_qsize 0\n",
      "2020-05-16 18:22:07,043 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-16 18:22:07,050 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-16 18:22:07,061 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-16 18:22:07,062 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-16 18:22:07,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-16 18:22:07,070 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-16 18:22:07,071 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-16 18:22:07,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-16 18:22:07,074 : INFO : EPOCH - 2 : training on 9704885 raw words (8594828 effective words) took 6.1s, 1413233 effective words/s\n",
      "2020-05-16 18:22:08,078 : INFO : EPOCH 3 - PROGRESS: at 15.61% examples, 1334856 words/s, in_qsize 9, out_qsize 0\n",
      "2020-05-16 18:22:09,081 : INFO : EPOCH 3 - PROGRESS: at 33.03% examples, 1411214 words/s, in_qsize 14, out_qsize 1\n",
      "2020-05-16 18:22:10,097 : INFO : EPOCH 3 - PROGRESS: at 49.60% examples, 1413587 words/s, in_qsize 16, out_qsize 0\n",
      "2020-05-16 18:22:11,111 : INFO : EPOCH 3 - PROGRESS: at 65.71% examples, 1434036 words/s, in_qsize 13, out_qsize 1\n",
      "2020-05-16 18:22:12,114 : INFO : EPOCH 3 - PROGRESS: at 80.25% examples, 1402877 words/s, in_qsize 3, out_qsize 3\n",
      "2020-05-16 18:22:13,119 : INFO : EPOCH 3 - PROGRESS: at 99.18% examples, 1412088 words/s, in_qsize 5, out_qsize 6\n",
      "2020-05-16 18:22:13,122 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-16 18:22:13,122 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-16 18:22:13,123 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-16 18:22:13,126 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-16 18:22:13,127 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-16 18:22:13,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-16 18:22:13,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-16 18:22:13,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-16 18:22:13,139 : INFO : EPOCH - 3 : training on 9704885 raw words (8595437 effective words) took 6.1s, 1418146 effective words/s\n",
      "2020-05-16 18:22:14,143 : INFO : EPOCH 4 - PROGRESS: at 14.97% examples, 1282605 words/s, in_qsize 8, out_qsize 1\n",
      "2020-05-16 18:22:15,155 : INFO : EPOCH 4 - PROGRESS: at 32.69% examples, 1391504 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-16 18:22:16,160 : INFO : EPOCH 4 - PROGRESS: at 48.04% examples, 1364329 words/s, in_qsize 13, out_qsize 2\n",
      "2020-05-16 18:22:17,179 : INFO : EPOCH 4 - PROGRESS: at 63.96% examples, 1391326 words/s, in_qsize 3, out_qsize 1\n",
      "2020-05-16 18:22:18,180 : INFO : EPOCH 4 - PROGRESS: at 79.11% examples, 1383153 words/s, in_qsize 0, out_qsize 0\n",
      "2020-05-16 18:22:19,187 : INFO : EPOCH 4 - PROGRESS: at 96.87% examples, 1380308 words/s, in_qsize 8, out_qsize 1\n",
      "2020-05-16 18:22:19,311 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-16 18:22:19,323 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-16 18:22:19,333 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-16 18:22:19,335 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-16 18:22:19,337 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-16 18:22:19,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-16 18:22:19,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-16 18:22:19,345 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-16 18:22:19,346 : INFO : EPOCH - 4 : training on 9704885 raw words (8594576 effective words) took 6.2s, 1385191 effective words/s\n",
      "2020-05-16 18:22:20,354 : INFO : EPOCH 5 - PROGRESS: at 14.70% examples, 1251419 words/s, in_qsize 13, out_qsize 2\n",
      "2020-05-16 18:22:21,367 : INFO : EPOCH 5 - PROGRESS: at 30.79% examples, 1309702 words/s, in_qsize 6, out_qsize 0\n",
      "2020-05-16 18:22:22,372 : INFO : EPOCH 5 - PROGRESS: at 46.50% examples, 1316385 words/s, in_qsize 16, out_qsize 0\n",
      "2020-05-16 18:22:23,384 : INFO : EPOCH 5 - PROGRESS: at 62.22% examples, 1353427 words/s, in_qsize 15, out_qsize 0\n",
      "2020-05-16 18:22:24,388 : INFO : EPOCH 5 - PROGRESS: at 77.32% examples, 1353312 words/s, in_qsize 8, out_qsize 3\n",
      "2020-05-16 18:22:25,388 : INFO : EPOCH 5 - PROGRESS: at 95.92% examples, 1370149 words/s, in_qsize 14, out_qsize 0\n",
      "2020-05-16 18:22:25,564 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-16 18:22:25,571 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-16 18:22:25,571 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-16 18:22:25,572 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-16 18:22:25,577 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-16 18:22:25,584 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-16 18:22:25,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-16 18:22:25,588 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-16 18:22:25,589 : INFO : EPOCH - 5 : training on 9704885 raw words (8594959 effective words) took 6.2s, 1377747 effective words/s\n",
      "2020-05-16 18:22:25,589 : INFO : training on a 48524425 raw words (42974397 effective words) took 30.6s, 1402606 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n这里传入训练数据的路径，自己会去读，每一行就是一个字符串，通过LineSentencee就会得到sentence,也就是\\n每一行的数据。然后定义用几个workers来执行，min_count就是之前讲的词频，这里词频小于5的就把它滤掉，这里\\n的size就是词向量训练的维度\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(LineSentence(merger_data_path), workers=8,min_count=5,size=200)\n",
    "\"\"\"\n",
    "这里传入训练数据的路径，自己会去读，每一行就是一个字符串，通过LineSentencee就会得到sentence,也就是\n",
    "每一行的数据。然后定义用几个workers来执行，min_count就是之前讲的词频，这里词频小于5的就把它滤掉，这里\n",
    "的size就是词向量训练的维度\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查找最近的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "这里是一个建立词向量的过程，得到的是词向量的结果，是对应着每一个词的词向量，所以可以进行一些词级别的操作，如计算词与词之间的相似度等\n",
    "```\n",
    "\n",
    "```\n",
    "这里下边拿到训练好的word2vec之后赋给一个model，model里边的wv里边有一个相似度计算的包，\n",
    "放进去一个奇瑞，可以直接计算一下相似度。这里的参数topn表示相似度最高的多少个词\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-16 18:22:25,609 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('东南', 0.8473502397537231),\n",
       " ('瑞虎5', 0.8420183658599854),\n",
       " ('名爵', 0.8392105102539062),\n",
       " ('江淮', 0.8293333649635315),\n",
       " ('福美来', 0.8245681524276733),\n",
       " ('二代', 0.8205920457839966),\n",
       " ('铃木', 0.8200820684432983),\n",
       " ('s3', 0.819768488407135),\n",
       " ('海马', 0.8177907466888428),\n",
       " ('鹰', 0.8176668882369995)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'], topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.join(root, 'data/wv', 'word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-16 18:31:44,249 : INFO : saving Word2Vec object under D:\\Learning\\Project\\QA\\data/wv\\word2vec.model, separately None\n",
      "2020-05-16 18:31:44,249 : INFO : not storing attribute vectors_norm\n",
      "2020-05-16 18:31:44,250 : INFO : not storing attribute cum_table\n",
      "2020-05-16 18:31:44,706 : INFO : saved D:\\Learning\\Project\\QA\\data/wv\\word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-16 18:31:47,702 : INFO : loading Word2Vec object from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model\n",
      "2020-05-16 18:31:48,237 : INFO : loading wv recursively from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model.wv.* with mmap=None\n",
      "2020-05-16 18:31:48,238 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-05-16 18:31:48,239 : INFO : loading vocabulary recursively from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model.vocabulary.* with mmap=None\n",
      "2020-05-16 18:31:48,240 : INFO : loading trainables recursively from D:\\Learning\\Project\\QA\\data/wv\\word2vec.model.trainables.* with mmap=None\n",
      "2020-05-16 18:31:48,241 : INFO : setting ignored attribute cum_table to None\n",
      "2020-05-16 18:31:48,241 : INFO : loaded D:\\Learning\\Project\\QA\\data/wv\\word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-16 18:31:48,846 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('东南', 0.8473502397537231),\n",
       " ('瑞虎5', 0.8420183658599854),\n",
       " ('名爵', 0.8392105102539062),\n",
       " ('江淮', 0.8293333649635315),\n",
       " ('福美来', 0.8245681524276733),\n",
       " ('二代', 0.8205920457839966),\n",
       " ('铃木', 0.8200820684432983),\n",
       " ('s3', 0.819768488407135),\n",
       " ('海马', 0.8177907466888428),\n",
       " ('鹰', 0.8176668882369995)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
