{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "# 引入 word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 作业2要求：\n",
    "\n",
    "1. 通过gensim训练词向量 即Gensim工具的使用\n",
    " + 1.1 利用分词后的项目数据生成训练词向量用的训练数据\n",
    " + 1.2 保存词向量训练数据\n",
    " + 1.3 应用gensim中Word2Vec或Fasttext训练词向量\n",
    " + 1.4 保存训练好的词向量\n",
    "\n",
    "2. 构建embedding_matrix\n",
    "\n",
    "> 读取上步计算词向量和构建的`vocab`词表，以`vocab`中的`index`为`key`值构建`embedding_matrix`\n",
    "\n",
    "`eg: embedding_matrix[i] = [embedding_vector]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "merger_data_path = 'data/merged_train_test_seg_data.csv'\n",
    "# 模型保存路径\n",
    "save_model_path='data/wv/word2vec.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 使用word2vec训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |  \n",
      " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |  \n",
      " |  Once you're finished training a model (=no more updates, only querying)\n",
      " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |  \n",
      " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |  \n",
      " |  Some important attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use `self.wv.__contains__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |      Deprecated. Use `self.wv.accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and scoring, to save memory.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Use only if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace_word_vectors_with_normalized : bool, optional\n",
      " |          If True, forget the original (not normalized) word vectors and only keep\n",
      " |          the L2-normalized word vectors, to save even more memory.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Deprecated. Use `self.wv.init_sims` instead.\n",
      " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看包的具体功能的时候，？不好使的话可以使用help()\n",
    "help(word2vec.Word2Vec)\n",
    "里边有一个sg参数，通过设置该参数来指定是使用哪一个算法\n",
    "sg : {0, 1}, optional\n",
    "           Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "    \n",
    "上一节讲到的一个softmax的优化的方法，这里使用下边这个参数就可以指定使用哪一个优化方法\n",
    "hs : {0, 1}, optional\n",
    "           If 1, hierarchical softmax will be used for model training. 分层softmax\n",
    "           If 0, and `negative` is non-zero, negative sampling will be used.  负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:16:32,152 : INFO : collecting all words and their counts\n",
      "2020-03-04 17:16:32,163 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-04 17:16:32,467 : INFO : PROGRESS: at sentence #10000, processed 941657 words, keeping 36796 word types\n",
      "2020-03-04 17:16:32,787 : INFO : PROGRESS: at sentence #20000, processed 1897796 words, keeping 54149 word types\n",
      "2020-03-04 17:16:33,107 : INFO : PROGRESS: at sentence #30000, processed 2842477 words, keeping 66984 word types\n",
      "2020-03-04 17:16:33,437 : INFO : PROGRESS: at sentence #40000, processed 3759167 words, keeping 77921 word types\n",
      "2020-03-04 17:16:33,787 : INFO : PROGRESS: at sentence #50000, processed 4736386 words, keeping 87832 word types\n",
      "2020-03-04 17:16:34,134 : INFO : PROGRESS: at sentence #60000, processed 5775137 words, keeping 97810 word types\n",
      "2020-03-04 17:16:34,470 : INFO : PROGRESS: at sentence #70000, processed 6837177 words, keeping 107437 word types\n",
      "2020-03-04 17:16:34,796 : INFO : PROGRESS: at sentence #80000, processed 7783493 words, keeping 115575 word types\n",
      "2020-03-04 17:16:35,113 : INFO : PROGRESS: at sentence #90000, processed 8645033 words, keeping 123503 word types\n",
      "2020-03-04 17:16:35,404 : INFO : PROGRESS: at sentence #100000, processed 9498123 words, keeping 130550 word types\n",
      "2020-03-04 17:16:35,492 : INFO : collected 132569 word types from a corpus of 9748591 raw words and 102871 sentences\n",
      "2020-03-04 17:16:35,493 : INFO : Loading a fresh vocabulary\n",
      "2020-03-04 17:16:35,586 : INFO : effective_min_count=5 retains 32905 unique words (24% of original 132569, drops 99664)\n",
      "2020-03-04 17:16:35,587 : INFO : effective_min_count=5 leaves 9598795 word corpus (98% of original 9748591, drops 149796)\n",
      "2020-03-04 17:16:35,674 : INFO : deleting the raw counts dictionary of 132569 items\n",
      "2020-03-04 17:16:35,678 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-03-04 17:16:35,679 : INFO : downsampling leaves estimated 8611596 word corpus (89.7% of prior 9598795)\n",
      "2020-03-04 17:16:35,797 : INFO : estimated required memory for 32905 words and 200 dimensions: 69100500 bytes\n",
      "2020-03-04 17:16:35,798 : INFO : resetting layer weights\n",
      "2020-03-04 17:16:36,218 : INFO : training model with 8 workers on 32905 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-04 17:16:37,231 : INFO : EPOCH 1 - PROGRESS: at 5.98% examples, 514577 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:38,238 : INFO : EPOCH 1 - PROGRESS: at 12.93% examples, 548850 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:39,265 : INFO : EPOCH 1 - PROGRESS: at 19.33% examples, 547337 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:40,271 : INFO : EPOCH 1 - PROGRESS: at 25.92% examples, 554215 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:41,313 : INFO : EPOCH 1 - PROGRESS: at 32.83% examples, 553980 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:42,323 : INFO : EPOCH 1 - PROGRESS: at 38.80% examples, 542911 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:43,351 : INFO : EPOCH 1 - PROGRESS: at 44.82% examples, 537106 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:44,357 : INFO : EPOCH 1 - PROGRESS: at 51.22% examples, 543836 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:45,379 : INFO : EPOCH 1 - PROGRESS: at 57.46% examples, 547228 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:46,382 : INFO : EPOCH 1 - PROGRESS: at 63.42% examples, 549375 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:47,388 : INFO : EPOCH 1 - PROGRESS: at 69.31% examples, 550785 words/s, in_qsize 13, out_qsize 0\n",
      "2020-03-04 17:16:48,403 : INFO : EPOCH 1 - PROGRESS: at 76.07% examples, 552343 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:49,417 : INFO : EPOCH 1 - PROGRESS: at 83.35% examples, 553746 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:16:50,424 : INFO : EPOCH 1 - PROGRESS: at 90.44% examples, 556480 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:51,433 : INFO : EPOCH 1 - PROGRESS: at 98.18% examples, 556638 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:51,569 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:16:51,588 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:16:51,631 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:16:51,637 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:16:51,652 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:16:51,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:16:51,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:16:51,674 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:16:51,675 : INFO : EPOCH - 1 : training on 9748591 raw words (8610584 effective words) took 15.4s, 557422 effective words/s\n",
      "2020-03-04 17:16:52,699 : INFO : EPOCH 2 - PROGRESS: at 5.98% examples, 507103 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:53,715 : INFO : EPOCH 2 - PROGRESS: at 12.93% examples, 542070 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:54,735 : INFO : EPOCH 2 - PROGRESS: at 19.33% examples, 544571 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:16:55,738 : INFO : EPOCH 2 - PROGRESS: at 26.02% examples, 554411 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:56,786 : INFO : EPOCH 2 - PROGRESS: at 33.19% examples, 557125 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:57,793 : INFO : EPOCH 2 - PROGRESS: at 40.13% examples, 560082 words/s, in_qsize 13, out_qsize 0\n",
      "2020-03-04 17:16:58,796 : INFO : EPOCH 2 - PROGRESS: at 45.83% examples, 551300 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:16:59,803 : INFO : EPOCH 2 - PROGRESS: at 50.72% examples, 538819 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:00,809 : INFO : EPOCH 2 - PROGRESS: at 55.56% examples, 528421 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:01,819 : INFO : EPOCH 2 - PROGRESS: at 60.19% examples, 519955 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:17:02,823 : INFO : EPOCH 2 - PROGRESS: at 65.27% examples, 516371 words/s, in_qsize 11, out_qsize 0\n",
      "2020-03-04 17:17:03,831 : INFO : EPOCH 2 - PROGRESS: at 69.81% examples, 509536 words/s, in_qsize 12, out_qsize 0\n",
      "2020-03-04 17:17:04,855 : INFO : EPOCH 2 - PROGRESS: at 75.31% examples, 505826 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:17:05,873 : INFO : EPOCH 2 - PROGRESS: at 80.94% examples, 502200 words/s, in_qsize 13, out_qsize 1\n",
      "2020-03-04 17:17:06,890 : INFO : EPOCH 2 - PROGRESS: at 86.52% examples, 496427 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:07,895 : INFO : EPOCH 2 - PROGRESS: at 91.41% examples, 491613 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:08,897 : INFO : EPOCH 2 - PROGRESS: at 98.18% examples, 491645 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:09,072 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:17:09,104 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:17:09,134 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:17:09,163 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:17:09,165 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:17:09,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:17:09,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:17:09,194 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:17:09,196 : INFO : EPOCH - 2 : training on 9748591 raw words (8611061 effective words) took 17.5s, 491632 effective words/s\n",
      "2020-03-04 17:17:10,237 : INFO : EPOCH 3 - PROGRESS: at 4.97% examples, 413933 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:11,254 : INFO : EPOCH 3 - PROGRESS: at 10.60% examples, 439370 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:12,267 : INFO : EPOCH 3 - PROGRESS: at 15.85% examples, 442862 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:13,276 : INFO : EPOCH 3 - PROGRESS: at 21.26% examples, 451274 words/s, in_qsize 14, out_qsize 0\n",
      "2020-03-04 17:17:14,294 : INFO : EPOCH 3 - PROGRESS: at 26.85% examples, 455637 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:17:15,294 : INFO : EPOCH 3 - PROGRESS: at 32.40% examples, 456888 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:16,300 : INFO : EPOCH 3 - PROGRESS: at 37.79% examples, 453977 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:17,309 : INFO : EPOCH 3 - PROGRESS: at 42.68% examples, 449442 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:18,353 : INFO : EPOCH 3 - PROGRESS: at 48.20% examples, 452521 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:19,378 : INFO : EPOCH 3 - PROGRESS: at 53.23% examples, 452572 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:20,384 : INFO : EPOCH 3 - PROGRESS: at 58.34% examples, 455815 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:21,402 : INFO : EPOCH 3 - PROGRESS: at 63.01% examples, 454484 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:17:22,410 : INFO : EPOCH 3 - PROGRESS: at 67.87% examples, 455591 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:23,416 : INFO : EPOCH 3 - PROGRESS: at 73.01% examples, 455330 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:24,451 : INFO : EPOCH 3 - PROGRESS: at 78.60% examples, 454858 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:25,495 : INFO : EPOCH 3 - PROGRESS: at 83.94% examples, 451105 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:26,496 : INFO : EPOCH 3 - PROGRESS: at 89.63% examples, 452822 words/s, in_qsize 14, out_qsize 0\n",
      "2020-03-04 17:17:27,505 : INFO : EPOCH 3 - PROGRESS: at 95.79% examples, 452426 words/s, in_qsize 12, out_qsize 0\n",
      "2020-03-04 17:17:28,105 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:17:28,125 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:17:28,127 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:17:28,150 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:17:28,173 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:17:28,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:17:28,193 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:17:28,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:17:28,203 : INFO : EPOCH - 3 : training on 9748591 raw words (8612781 effective words) took 19.0s, 453260 effective words/s\n",
      "2020-03-04 17:17:29,214 : INFO : EPOCH 4 - PROGRESS: at 4.97% examples, 425950 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:30,223 : INFO : EPOCH 4 - PROGRESS: at 10.48% examples, 443170 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:31,227 : INFO : EPOCH 4 - PROGRESS: at 15.19% examples, 432025 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:17:32,233 : INFO : EPOCH 4 - PROGRESS: at 20.46% examples, 439159 words/s, in_qsize 13, out_qsize 0\n",
      "2020-03-04 17:17:33,277 : INFO : EPOCH 4 - PROGRESS: at 25.64% examples, 436883 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:34,285 : INFO : EPOCH 4 - PROGRESS: at 31.22% examples, 442040 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:35,294 : INFO : EPOCH 4 - PROGRESS: at 36.84% examples, 443510 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:36,343 : INFO : EPOCH 4 - PROGRESS: at 42.37% examples, 444554 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:37,364 : INFO : EPOCH 4 - PROGRESS: at 47.80% examples, 448391 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:38,381 : INFO : EPOCH 4 - PROGRESS: at 53.23% examples, 452647 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:39,416 : INFO : EPOCH 4 - PROGRESS: at 58.34% examples, 454679 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:40,448 : INFO : EPOCH 4 - PROGRESS: at 63.51% examples, 456487 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:41,458 : INFO : EPOCH 4 - PROGRESS: at 68.47% examples, 458054 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:42,500 : INFO : EPOCH 4 - PROGRESS: at 74.25% examples, 460158 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:43,533 : INFO : EPOCH 4 - PROGRESS: at 80.13% examples, 461084 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:44,564 : INFO : EPOCH 4 - PROGRESS: at 86.41% examples, 461088 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:45,586 : INFO : EPOCH 4 - PROGRESS: at 91.53% examples, 459151 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:46,603 : INFO : EPOCH 4 - PROGRESS: at 98.18% examples, 460124 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:46,782 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:17:46,807 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:17:46,826 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:17:46,836 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:17:46,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:17:46,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:17:46,892 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:17:46,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:17:46,898 : INFO : EPOCH - 4 : training on 9748591 raw words (8610121 effective words) took 18.7s, 460688 effective words/s\n",
      "2020-03-04 17:17:47,911 : INFO : EPOCH 5 - PROGRESS: at 4.59% examples, 390528 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:48,947 : INFO : EPOCH 5 - PROGRESS: at 10.27% examples, 428280 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:49,948 : INFO : EPOCH 5 - PROGRESS: at 15.29% examples, 431362 words/s, in_qsize 14, out_qsize 0\n",
      "2020-03-04 17:17:50,975 : INFO : EPOCH 5 - PROGRESS: at 21.06% examples, 447117 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:52,008 : INFO : EPOCH 5 - PROGRESS: at 25.84% examples, 437277 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:17:53,012 : INFO : EPOCH 5 - PROGRESS: at 31.24% examples, 439891 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:54,061 : INFO : EPOCH 5 - PROGRESS: at 36.64% examples, 436726 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:55,073 : INFO : EPOCH 5 - PROGRESS: at 41.97% examples, 438453 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:17:56,098 : INFO : EPOCH 5 - PROGRESS: at 47.13% examples, 439959 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:57,135 : INFO : EPOCH 5 - PROGRESS: at 52.48% examples, 443180 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:17:58,162 : INFO : EPOCH 5 - PROGRESS: at 57.46% examples, 444853 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:17:59,217 : INFO : EPOCH 5 - PROGRESS: at 62.31% examples, 445221 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:18:00,232 : INFO : EPOCH 5 - PROGRESS: at 67.17% examples, 446192 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:18:01,255 : INFO : EPOCH 5 - PROGRESS: at 71.78% examples, 443637 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:18:02,265 : INFO : EPOCH 5 - PROGRESS: at 77.20% examples, 444085 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:18:03,266 : INFO : EPOCH 5 - PROGRESS: at 82.92% examples, 444257 words/s, in_qsize 12, out_qsize 0\n",
      "2020-03-04 17:18:04,272 : INFO : EPOCH 5 - PROGRESS: at 88.40% examples, 443906 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:18:05,275 : INFO : EPOCH 5 - PROGRESS: at 94.30% examples, 444963 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:18:06,120 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:18:06,136 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:18:06,163 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:18:06,167 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:18:06,192 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:18:06,201 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:18:06,212 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:18:06,219 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:18:06,220 : INFO : EPOCH - 5 : training on 9748591 raw words (8612386 effective words) took 19.3s, 445838 effective words/s\n",
      "2020-03-04 17:18:06,221 : INFO : training on a 48742955 raw words (43056933 effective words) took 90.0s, 478395 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# 这里直接使用word2vec.Word2Vec这个包来训练word2vec这个模型,训练词向量\n",
    "# 实例化word2vec模型为model_wv\n",
    "model_wv = word2vec.Word2Vec(LineSentence(merger_data_path), sg=1,workers=8,min_count=5,size=200)\n",
    "# sg=1: 使用Skip-Gram来构建word2vec\n",
    "# workers=8: 使用8个进程来跑\n",
    "# min_count=5：词频小于5的直接滤掉\n",
    "# size=200：训练一个200维的词向量\n",
    "# 这里word2vec如何定义是使用Skip-Gram还是CBOW，这里可以直接通过help(word2vec.Word2Vec)来查看\n",
    "# 这里名字里边的wv就是word2vec的缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:19:31,773 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('瑞虎5', 0.734700083732605),\n",
       " ('瑞虎', 0.7169025540351868),\n",
       " ('风云', 0.6548324227333069),\n",
       " ('东方之子', 0.6316857933998108),\n",
       " ('旗云1', 0.6134951710700989),\n",
       " ('昌河', 0.611372172832489),\n",
       " ('福田', 0.6112220287322998),\n",
       " ('瑞虎3', 0.6103478670120239),\n",
       " ('吉利', 0.6091591119766235),\n",
       " ('旗云2', 0.6053952574729919)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wv.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用FastText训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:19:52,710 : INFO : resetting layer weights\n",
      "2020-03-04 17:20:04,056 : INFO : collecting all words and their counts\n",
      "2020-03-04 17:20:04,063 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-04 17:20:04,381 : INFO : PROGRESS: at sentence #10000, processed 941657 words, keeping 36796 word types\n",
      "2020-03-04 17:20:04,701 : INFO : PROGRESS: at sentence #20000, processed 1897796 words, keeping 54149 word types\n",
      "2020-03-04 17:20:04,953 : INFO : PROGRESS: at sentence #30000, processed 2842477 words, keeping 66984 word types\n",
      "2020-03-04 17:20:05,255 : INFO : PROGRESS: at sentence #40000, processed 3759167 words, keeping 77921 word types\n",
      "2020-03-04 17:20:05,587 : INFO : PROGRESS: at sentence #50000, processed 4736386 words, keeping 87832 word types\n",
      "2020-03-04 17:20:05,945 : INFO : PROGRESS: at sentence #60000, processed 5775137 words, keeping 97810 word types\n",
      "2020-03-04 17:20:06,298 : INFO : PROGRESS: at sentence #70000, processed 6837177 words, keeping 107437 word types\n",
      "2020-03-04 17:20:06,615 : INFO : PROGRESS: at sentence #80000, processed 7783493 words, keeping 115575 word types\n",
      "2020-03-04 17:20:06,886 : INFO : PROGRESS: at sentence #90000, processed 8645033 words, keeping 123503 word types\n",
      "2020-03-04 17:20:07,174 : INFO : PROGRESS: at sentence #100000, processed 9498123 words, keeping 130550 word types\n",
      "2020-03-04 17:20:07,261 : INFO : collected 132569 word types from a corpus of 9748591 raw words and 102871 sentences\n",
      "2020-03-04 17:20:07,262 : INFO : Loading a fresh vocabulary\n",
      "2020-03-04 17:20:07,425 : INFO : effective_min_count=5 retains 32905 unique words (24% of original 132569, drops 99664)\n",
      "2020-03-04 17:20:07,426 : INFO : effective_min_count=5 leaves 9598795 word corpus (98% of original 9748591, drops 149796)\n",
      "2020-03-04 17:20:07,532 : INFO : deleting the raw counts dictionary of 132569 items\n",
      "2020-03-04 17:20:07,536 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-03-04 17:20:07,536 : INFO : downsampling leaves estimated 8611596 word corpus (89.7% of prior 9598795)\n",
      "2020-03-04 17:20:07,753 : INFO : estimated required memory for 32905 words, 114857 buckets and 200 dimensions: 163675484 bytes\n",
      "2020-03-04 17:20:07,758 : INFO : resetting layer weights\n",
      "2020-03-04 17:20:14,062 : INFO : training model with 8 workers on 32905 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-04 17:20:15,077 : INFO : EPOCH 1 - PROGRESS: at 8.43% examples, 711661 words/s, in_qsize 10, out_qsize 0\n",
      "2020-03-04 17:20:16,078 : INFO : EPOCH 1 - PROGRESS: at 17.83% examples, 761823 words/s, in_qsize 9, out_qsize 1\n",
      "2020-03-04 17:20:17,082 : INFO : EPOCH 1 - PROGRESS: at 27.28% examples, 780841 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:18,089 : INFO : EPOCH 1 - PROGRESS: at 37.51% examples, 794588 words/s, in_qsize 13, out_qsize 2\n",
      "2020-03-04 17:20:19,101 : INFO : EPOCH 1 - PROGRESS: at 47.44% examples, 808737 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:20,110 : INFO : EPOCH 1 - PROGRESS: at 56.70% examples, 817100 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:21,121 : INFO : EPOCH 1 - PROGRESS: at 65.72% examples, 821890 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:22,139 : INFO : EPOCH 1 - PROGRESS: at 75.18% examples, 824512 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:20:23,156 : INFO : EPOCH 1 - PROGRESS: at 85.96% examples, 825968 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:20:24,165 : INFO : EPOCH 1 - PROGRESS: at 96.41% examples, 824341 words/s, in_qsize 12, out_qsize 2\n",
      "2020-03-04 17:20:24,415 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:20:24,430 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:20:24,435 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:20:24,446 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:20:24,448 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:20:24,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:20:24,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:20:24,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:20:24,474 : INFO : EPOCH - 1 : training on 9748591 raw words (8611669 effective words) took 10.4s, 827535 effective words/s\n",
      "2020-03-04 17:20:25,500 : INFO : EPOCH 2 - PROGRESS: at 9.62% examples, 806245 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:20:26,508 : INFO : EPOCH 2 - PROGRESS: at 19.53% examples, 828393 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:20:27,509 : INFO : EPOCH 2 - PROGRESS: at 29.42% examples, 835331 words/s, in_qsize 16, out_qsize 1\n",
      "2020-03-04 17:20:28,509 : INFO : EPOCH 2 - PROGRESS: at 39.74% examples, 841013 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:29,510 : INFO : EPOCH 2 - PROGRESS: at 49.47% examples, 847614 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:30,533 : INFO : EPOCH 2 - PROGRESS: at 58.42% examples, 843491 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:31,548 : INFO : EPOCH 2 - PROGRESS: at 67.68% examples, 848878 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:32,550 : INFO : EPOCH 2 - PROGRESS: at 77.40% examples, 847596 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:33,566 : INFO : EPOCH 2 - PROGRESS: at 88.40% examples, 848612 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:20:34,566 : INFO : EPOCH 2 - PROGRESS: at 99.05% examples, 846221 words/s, in_qsize 9, out_qsize 0\n",
      "2020-03-04 17:20:34,587 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:20:34,588 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:20:34,601 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:20:34,606 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:20:34,616 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:20:34,619 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:20:34,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:20:34,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:20:34,632 : INFO : EPOCH - 2 : training on 9748591 raw words (8612360 effective words) took 10.2s, 848256 effective words/s\n",
      "2020-03-04 17:20:35,655 : INFO : EPOCH 3 - PROGRESS: at 9.54% examples, 800419 words/s, in_qsize 16, out_qsize 2\n",
      "2020-03-04 17:20:36,657 : INFO : EPOCH 3 - PROGRESS: at 19.42% examples, 827473 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:20:37,657 : INFO : EPOCH 3 - PROGRESS: at 29.51% examples, 840299 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:38,664 : INFO : EPOCH 3 - PROGRESS: at 39.83% examples, 843811 words/s, in_qsize 14, out_qsize 0\n",
      "2020-03-04 17:20:39,672 : INFO : EPOCH 3 - PROGRESS: at 49.57% examples, 848279 words/s, in_qsize 12, out_qsize 0\n",
      "2020-03-04 17:20:40,683 : INFO : EPOCH 3 - PROGRESS: at 58.88% examples, 851615 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:20:41,690 : INFO : EPOCH 3 - PROGRESS: at 67.97% examples, 854434 words/s, in_qsize 12, out_qsize 0\n",
      "2020-03-04 17:20:42,701 : INFO : EPOCH 3 - PROGRESS: at 77.65% examples, 850388 words/s, in_qsize 14, out_qsize 2\n",
      "2020-03-04 17:20:43,703 : INFO : EPOCH 3 - PROGRESS: at 88.46% examples, 851443 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:44,716 : INFO : EPOCH 3 - PROGRESS: at 98.84% examples, 845075 words/s, in_qsize 10, out_qsize 1\n",
      "2020-03-04 17:20:44,749 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:20:44,762 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:20:44,769 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:20:44,772 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:20:44,783 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:20:44,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:20:44,793 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:20:44,803 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:20:44,804 : INFO : EPOCH - 3 : training on 9748591 raw words (8612003 effective words) took 10.2s, 847034 effective words/s\n",
      "2020-03-04 17:20:45,817 : INFO : EPOCH 4 - PROGRESS: at 8.12% examples, 686271 words/s, in_qsize 0, out_qsize 0\n",
      "2020-03-04 17:20:46,818 : INFO : EPOCH 4 - PROGRESS: at 17.36% examples, 740582 words/s, in_qsize 0, out_qsize 0\n",
      "2020-03-04 17:20:47,828 : INFO : EPOCH 4 - PROGRESS: at 25.74% examples, 736571 words/s, in_qsize 3, out_qsize 0\n",
      "2020-03-04 17:20:48,839 : INFO : EPOCH 4 - PROGRESS: at 35.12% examples, 744946 words/s, in_qsize 0, out_qsize 0\n",
      "2020-03-04 17:20:49,840 : INFO : EPOCH 4 - PROGRESS: at 44.29% examples, 750204 words/s, in_qsize 10, out_qsize 1\n",
      "2020-03-04 17:20:50,849 : INFO : EPOCH 4 - PROGRESS: at 53.23% examples, 762425 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:51,849 : INFO : EPOCH 4 - PROGRESS: at 61.87% examples, 772474 words/s, in_qsize 12, out_qsize 1\n",
      "2020-03-04 17:20:52,856 : INFO : EPOCH 4 - PROGRESS: at 71.13% examples, 783483 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:20:53,872 : INFO : EPOCH 4 - PROGRESS: at 81.54% examples, 790315 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:20:54,878 : INFO : EPOCH 4 - PROGRESS: at 91.78% examples, 794274 words/s, in_qsize 11, out_qsize 0\n",
      "2020-03-04 17:20:55,533 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:20:55,552 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:20:55,555 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:20:55,571 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:20:55,580 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:20:55,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:20:55,589 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:20:55,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:20:55,598 : INFO : EPOCH - 4 : training on 9748591 raw words (8610650 effective words) took 10.8s, 798107 effective words/s\n",
      "2020-03-04 17:20:56,632 : INFO : EPOCH 5 - PROGRESS: at 9.43% examples, 781948 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:57,640 : INFO : EPOCH 5 - PROGRESS: at 19.62% examples, 828851 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:20:58,661 : INFO : EPOCH 5 - PROGRESS: at 29.61% examples, 832705 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:20:59,675 : INFO : EPOCH 5 - PROGRESS: at 40.13% examples, 840688 words/s, in_qsize 15, out_qsize 1\n",
      "2020-03-04 17:21:00,685 : INFO : EPOCH 5 - PROGRESS: at 49.47% examples, 838889 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:21:01,703 : INFO : EPOCH 5 - PROGRESS: at 58.88% examples, 844078 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:21:02,712 : INFO : EPOCH 5 - PROGRESS: at 67.86% examples, 846408 words/s, in_qsize 15, out_qsize 0\n",
      "2020-03-04 17:21:03,721 : INFO : EPOCH 5 - PROGRESS: at 77.98% examples, 847911 words/s, in_qsize 14, out_qsize 1\n",
      "2020-03-04 17:21:04,733 : INFO : EPOCH 5 - PROGRESS: at 88.55% examples, 846430 words/s, in_qsize 16, out_qsize 0\n",
      "2020-03-04 17:21:05,714 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-04 17:21:05,717 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-04 17:21:05,718 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-04 17:21:05,725 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-04 17:21:05,738 : INFO : EPOCH 5 - PROGRESS: at 99.70% examples, 847256 words/s, in_qsize 3, out_qsize 1\n",
      "2020-03-04 17:21:05,739 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-04 17:21:05,750 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-04 17:21:05,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-04 17:21:05,756 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-04 17:21:05,757 : INFO : EPOCH - 5 : training on 9748591 raw words (8612366 effective words) took 10.2s, 848090 effective words/s\n",
      "2020-03-04 17:21:05,758 : INFO : training on a 48742955 raw words (43059048 effective words) took 51.7s, 832941 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# 这里直接使用FastText这个包来训练FastText这个模型，训练词向量\n",
    "# 实例化FastText模型为model_ft\n",
    "model_ft = FastText(sentences=LineSentence(merger_data_path), workers=8, min_count=5, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:22:04,837 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-03-04 17:22:04,870 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('奇瑞E5', 0.8922537565231323),\n",
       " ('奇瑞A1', 0.8768239617347717),\n",
       " ('奇瑞A5', 0.8766922950744629),\n",
       " ('东南', 0.8725718855857849),\n",
       " ('奇瑞QQ', 0.8679873943328857),\n",
       " ('奇瑞QQ6', 0.8551181554794312),\n",
       " ('瑞虎5', 0.8534832000732422),\n",
       " ('瑞虎', 0.8531208038330078),\n",
       " ('奇瑞A3', 0.8509798049926758),\n",
       " ('奇瑞E3', 0.8476381301879883)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:22:21,372 : INFO : saving Word2Vec object under data/wv/word2vec.model, separately None\n",
      "2020-03-04 17:22:21,373 : INFO : not storing attribute vectors_norm\n",
      "2020-03-04 17:22:21,374 : INFO : not storing attribute cum_table\n",
      "2020-03-04 17:22:21,930 : INFO : saved data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model_wv.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 模型的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:22:29,916 : INFO : loading Word2Vec object from data/wv/word2vec.model\n",
      "2020-03-04 17:22:30,351 : INFO : loading wv recursively from data/wv/word2vec.model.wv.* with mmap=None\n",
      "2020-03-04 17:22:30,351 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-03-04 17:22:30,352 : INFO : loading vocabulary recursively from data/wv/word2vec.model.vocabulary.* with mmap=None\n",
      "2020-03-04 17:22:30,353 : INFO : loading trainables recursively from data/wv/word2vec.model.trainables.* with mmap=None\n",
      "2020-03-04 17:22:30,353 : INFO : setting ignored attribute cum_table to None\n",
      "2020-03-04 17:22:30,354 : INFO : loaded data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-04 17:22:35,805 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('瑞虎5', 0.734700083732605),\n",
       " ('瑞虎', 0.7169025540351868),\n",
       " ('风云', 0.6548324227333069),\n",
       " ('东方之子', 0.6316857933998108),\n",
       " ('旗云1', 0.6134951710700989),\n",
       " ('昌河', 0.611372172832489),\n",
       " ('福田', 0.6112220287322998),\n",
       " ('瑞虎3', 0.6103478670120239),\n",
       " ('吉利', 0.6091591119766235),\n",
       " ('旗云2', 0.6053952574729919)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 构建embedding_matrix\n",
    "这里提前构建好词的embedding矩阵，这样的话后边进行模型训练的时候，就不用再进行词的Embedding了，直接将这里的Embedding矩阵导入即可。\n",
    "这里就相当于在研究BERT的Attention时，拿着要输入的词到词向量的表里查找对应的词向量一样，种类就是构建词向量表的一个过程。（一个字典的感觉，根据key查找value的过程）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 构建vocab\n",
    "查看构建出来的vocab效果好不好：\n",
    "在Gensim里边有这么一个方法：score，用来判断这个词向量好还是不好，也就是看一个它的输入和输出，它的输入是词，输出也是词。后边自己试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个vocab是从model里边来，model里边是定义了一个词频参数，低于5的就滤掉，原先的语料就不用再去统计\n",
    "# 这个框架就直接是低于5的就滤掉了，就直接实现了过滤掉了低频词\n",
    "vocab = {word:index for index, word in enumerate(model_wv.wv.index2word)}\n",
    "# 所以这里的表要定义成字典的形式，便于根据key得到value（词向量）\n",
    "reverse_vocab = {index: word for index, word in enumerate(model_wv.wv.index2word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 获取embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法一\n",
    "这种方法就完整的复现了第二次课里所描述的方法，就是直接拿到第i个词的词向量赋值给初始化的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义保存路径\n",
    "save_embedding_matrix_path='data/embedding_matrix.txt'\n",
    "\n",
    "def get_embedding_matrix(wv_model):\n",
    "    # 获取vocab大小\n",
    "    vocab_size = len(wv_model.wv.vocab)\n",
    "    # 获取embedding维度\n",
    "    embedding_dim = wv_model.wv.vector_size\n",
    "    print('vocab_size, embedding_dim:', vocab_size, embedding_dim)\n",
    "    # 初始化矩阵  shape和词向量矩阵一样\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    # 这里的vocab_size就是词的个数\n",
    "    # 按顺序填充\n",
    "    for i in range(vocab_size):\n",
    "        embedding_matrix[i, :] = wv_model.wv[wv_model.wv.index2word[i]]\n",
    "        # wv_model.wv.index2word[i]从第一个词开始依次输出词表里边的词，拿到它对应的向量，然后赋值给这个初始化全为0的numpy矩阵矩阵\n",
    "        # 转换一下格式\n",
    "        embedding_matrix = embedding_matrix.astype('float32')\n",
    "    # 断言检查维度是否符合要求，是否是自己想要的大小\n",
    "    assert embedding_matrix.shape == (vocab_size, embedding_dim)\n",
    "    # 保存矩阵\n",
    "    np.savetxt('save_embedding_matrix_path', embedding_matrix, fmt='%0.8f')\n",
    "    print('embedding matrix extracted')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size, embedding_dim: 32905 200\n",
      "embedding matrix extracted\n",
      "(32905, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix=get_embedding_matrix(model_wv)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32905, 200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法二\n",
    "这里直接通过这里的方法，直接拿矩阵也可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_wv=model_wv.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32905, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_wv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比\n",
    "对比两种方法得到的矩阵，所有的参数都是一样的，所以整体来说这个方法要好一些，直接拿取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix==embedding_matrix_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding_matrix==embedding_matrix_wv).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 有没有一个标准的处理流程,怕前期数据处理影响后期项目效果? \n",
    "对于数据处理这个部分，一开始的方法可能会是一个比较low的方法，后边会不断的去完善数据处理这个部分，结合任务，不断的优化这个模块，这是一个不断修改，不断矫正的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
