{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本次研讨课内容:\n",
    "   问答摘要内容讲解:\n",
    "    1. vocab对象构建\n",
    "    2. batcher 方法构建\n",
    "    3. PGN Model.\n",
    "    4. coverage\n",
    "    5. coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蓝色的字体表示的是参考摘要，三个模型的生成摘要的结果差别挺大。红色字体表明了不准确的摘要细节生成(UNK未登录词，无法解决OOV问题)，绿色的字体表明了模型生成了重复文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rouge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/aistudio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.PGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sequence-to-sequence mode baseline,\n",
    "2. pointer generater mode \n",
    "3. coverage机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "![](img/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGN\n",
    "![](img/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. vocab对象构建 \n",
    "将vocab处理为一个对象，即建立一个vocab类，把之前处理vocab字典的操作全部封进去，为了后边写代码的时候方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    UNKNOWN_TOKEN = '<UNK>'\n",
    "    START_DECODING = '<START>'\n",
    "    STOP_DECODING = '<STOP>'\n",
    "\n",
    "    def __init__(self, vocab_file, vocab_max_size=None):\n",
    "        \"\"\"\n",
    "        Vocab 对象,vocab基本操作封装\n",
    "        :param vocab_file: Vocab 存储路径\n",
    "        :param vocab_max_size: 最大字典数量\n",
    "        \"\"\"\n",
    "        self.word2id, self.id2word = self.load_vocab(vocab_file, vocab_max_size)\n",
    "        self.count = len(self.word2id)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(file_path, vocab_max_size=None):\n",
    "        \"\"\"\n",
    "        读取字典\n",
    "        :param file_path: 文件路径\n",
    "        :return: 返回读取后的字典\n",
    "        \"\"\"\n",
    "        vocab = {}\n",
    "        reverse_vocab = {}\n",
    "        for line in open(file_path, \"r\", encoding='utf-8').readlines():\n",
    "            word, index = line.strip().split(\"\\t\")\n",
    "            index = int(index)\n",
    "            # 如果vocab 超过了指定大小\n",
    "            # 跳出循环 截断\n",
    "            if vocab_max_size and index > vocab_max_size:\n",
    "                print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (\n",
    "                    vocab_max_size, index))\n",
    "                break\n",
    "            vocab[word] = index\n",
    "            reverse_vocab[index] = word\n",
    "        return vocab, reverse_vocab\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            return self.word2id[self.UNKNOWN_TOKEN]\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def id_to_word(self, word_id):\n",
    "        if word_id not in self.id2word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self.id2word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1. oov词去哪里取?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. batcher 改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于之前对词典的处理将OOV词都清除掉了，再拿着UNK去找对应的词的时候就找不到了，所以这里对batcher进行一个改进，实现在输入的时候，将OOV词的列表也带进去，这一部分计算attention时要用到。所以这里构造了一个batch的帮助类，这个类里边主要包括了，一个batch里边主要包括哪些输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions -> idx2word -> `QQ224,这是车辆<UNK><UNK>`\n",
    "\n",
    "### enc_extended_inp -> idx2word ->  `道奇,锋哲,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### enc_inp -> idx2word->  `<UNK>,<UNK>,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### article_oovs ->` [道奇 , 锋哲]`\n",
    "\n",
    "### dec_input -> idx2word-> `<start>QQ224,这是车辆<UNK><UNK><end><pad><pad>`\n",
    "\n",
    "### target -> idx2word-> ` QQ224,这是车辆道奇,锋哲<end><pad>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整个过程使用生成器给封装起来了，再训练的时候，直接迭代生成器就可以了\n",
    "# 相当于是将处理数据的逻辑和构造batch的逻辑，全部放到生成器里边去迭代了\n",
    "# 这样就实现了，在你训练的时候，你的数据预处理的操作你可以把它放到你模型训练里边来做，让你的CPU来做你的\n",
    "# 数据预处理，Gpu来跑训练，而且可以经常的改动你的预处理的处理模块，再运行的时候J就直接run就可以了，\n",
    "# 不用首先build_dataset\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(params, vocab, max_enc_len, max_dec_len, mode, batch_size),\n",
    "        output_types={  # 每个batch里边会包含以下这些数据\n",
    "            \"enc_len\": tf.int32,  # 输入句子长度\n",
    "            \"enc_input\": tf.int32,  #输入的句子\n",
    "            \"enc_extended_inp\": tf.int32,  # 原始的输入语句，即没有使用UNK等替换过的原始句子，如5.0.2\n",
    "            # 但是真实训练的句子是enc_input，即切完词，替换之后的句子,如5.0.3，输入依旧都是索引\n",
    "            \n",
    "            \"article_oovs\": tf.string,  # OOV的词，将这些OOV的词也存起来，这些词的索引是如何得到的呢？\n",
    "            # vocab.size() + article_oovs.index(w),在词表中的序号下继续往下排，这个词表不是共享的，\n",
    "            # 每一句话都有一个oov的index，查找UNK的时候，就看它对应的索引位置，然后到\n",
    "            # 遇到UNK的时候就会自动调整UNK的注意力权重，更加倾向于vocab之外的词，具体的实现是通过神经网络自\n",
    "            # 动调节 \n",
    "            \"dec_input\": tf.int32,  # decoder的输入\n",
    "            \"target\": tf.int32,  # 最终的结果\n",
    "            \"dec_len\": tf.int32,\n",
    "            \"article\": tf.string,\n",
    "            \"abstract\": tf.string,\n",
    "            \"abstract_sents\": tf.string,\n",
    "            \"sample_decoder_pad_mask\": tf.int32,\n",
    "            \"sample_encoder_pad_mask\": tf.int32,\n",
    "        },\n",
    "        output_shapes={\n",
    "            \"enc_len\": [],\n",
    "            \"enc_input\": [None],\n",
    "            \"enc_extended_inp\": [None],\n",
    "            \n",
    "            \"article_oovs\": [None],\n",
    "            \"dec_input\": [None],\n",
    "            \"target\": [None],\n",
    "            \"dec_len\": [],\n",
    "            \"article\": [],\n",
    "            \"abstract\": [],\n",
    "            \"abstract_sents\": [],\n",
    "            \"sample_decoder_pad_mask\": [None],\n",
    "            \"sample_encoder_pad_mask\": [None]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(batch_size,\n",
    "               padded_shapes=({\"enc_len\": [],\n",
    "                               \"enc_input\": [None],\n",
    "                               \"enc_input_extend_vocab\": [None],\n",
    "                               \"article_oovs\": [None],\n",
    "                               \"dec_input\": [max_dec_len],\n",
    "                               \"target\": [max_dec_len],\n",
    "                               \"dec_len\": [],\n",
    "                               \"article\": [],\n",
    "                               \"abstract\": [],\n",
    "                               }),\n",
    "               padding_values={\"enc_len\": -1,\n",
    "                               \"enc_input\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"enc_input_extend_vocab\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"article_oovs\": b'',\n",
    "                               \"dec_input\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"target\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"dec_len\": -1,\n",
    "                               \"article\": b\"\",\n",
    "                               \"abstract\": b\"\",\n",
    "                               },\n",
    "               drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于前边的两步都处理vocab构建好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ppgn  \n",
    "\n",
    "### seq2seq+point\n",
    "\n",
    "混合了 Baseline seq2seq和PointerNetwork的网络，它具有Baseline seq2seq的生成能力和PointerNetwork的Copy能力。如何权衡一个词应该是生成的还是复制的？原文中引入了一个权重$p_{gen}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从Baseline seq2seq的模型结构中得到了$S_t$和$h^*_t$，和解码器输入 $x_t$ 一起来计算 $p_{gen}$ ： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pgn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pgen ∈ [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ context vector $h^*_t$\n",
    "+ decoder input $x_t$ \n",
    "+ the decoder state $S_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.w_s_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_i_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_c_reduce = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, context_vector, dec_hidden, dec_inp):\n",
    "        return tf.nn.sigmoid(self.w_s_reduce(dec_hidden) + self.w_c_reduce(context_vector) + self.w_i_reduce(dec_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gens = []\n",
    "for t in range(dec_target.shape[1]):\n",
    "    .....\n",
    "    p_gen = self.pointer(context_vector, dec_hidden, dec_x)\n",
    "\n",
    "p_gens.append(p_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，会扩充单词表形成一个更大的单词表--扩充单词表(将原文当中的单词也加入到其中)，该时间步的预测词概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $a_i^t$ 表示的是原文档中的词。我们可以看到解码器一个词的输出概率有其是否拷贝是否生成的概率和决定。当一个词不出现在常规的单词表上时$P_{vocab}(w)$ 为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions -> idx2word -> `QQ224,这是车辆<UNK><UNK>`\n",
    "\n",
    "## enc_extended_inp -> idx2word ->  `道奇,锋哲,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "## enc_inp -> idx2word->  `<UNK>,<UNK>,昨晚看到一台车车牌是绿色的？这是什么牌？`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用这样的方法，来得到最终概率的生成\n",
    "final_dists=calc_final_dist(enc_extended_inp,# 原始输入\n",
    "                             predictions,# 原始预测概率，经过decoder call之后得到的概率\n",
    "                             attentions, # att权重  调用attention得到的权重\n",
    "                             p_gens, # pgn概率\n",
    "                             batch_oov_len,# 2  比如有两个UNK长度就是2\n",
    "                             self.params[\"vocab_size\"],# 原始的wocab size\n",
    "                             self.params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体的vocab -> extended_vocab的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab -> `{\n",
    "    <UNK>:0,\n",
    "    昨晚:1,\n",
    "    看到:2,\n",
    "    一台车:3,\n",
    "    车牌:4,\n",
    "    是绿色的:5,\n",
    "    ？:6,\n",
    "    这是什么:7,\n",
    "    牌:8\n",
    "}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extended_vsize=10\n",
    "\n",
    "extend_vocab ->\n",
    "`{\n",
    "    <UNK>:0,\n",
    "    昨晚:1,\n",
    "    看到:2,\n",
    "    一台车:3,\n",
    "    车牌:4,\n",
    "    是绿色的:5,\n",
    "    ？:6,\n",
    "    这是什么:7,\n",
    "    牌:8,\n",
    "    道奇:9,\n",
    "    锋哲:10.\n",
    "}`  \n",
    "\n",
    "这里的9,10就是通过上边的batch_oov_len来确定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions -> idx2word -> `QQ224,这是车辆<UNK><UNK>`\n",
    "\n",
    "### enc_extended_inp -> idx2word ->  `道奇,锋哲,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### enc_inp -> idx2word->  `<UNK>,<UNK>,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### article_oovs ->` [道奇 , 锋哲]`\n",
    "\n",
    "### dec_input -> idx2word-> `<start>QQ224,这是车辆<UNK><UNK><end><pad><pad>`\n",
    "\n",
    "### target -> idx2word-> ` QQ224,这是车辆道奇,锋哲<end><pad>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc_inp <unk> <unk> 1 2 3 4 5 6 7 8\n",
    "\n",
    "enc_extended_inp 9 10 1 2 3 4 5 6 7 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dists=calc_final_dist(enc_extended_inp,# 原始输入：enc_extended_inp 9 10 1 2 3 4 5 6 7 8\n",
    "                             predictions,# 原始预测概率  图中绿色的部分\n",
    "                             attentions, # att权重  图中蓝色的部分\n",
    "                             p_gens, # pgn概率\n",
    "                             batch_oov_len,# 2\n",
    "                             self.params[\"vocab_size\"],# 原始的wocab size\n",
    "                             self.params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算最终概率的代码实现\n",
    "这里函数中传入的参数是和final_dists里边的参数一一对应的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_final_dist(_enc_batch_extend_vocab, vocab_dists, attn_dists, p_gens, batch_oov_len, vocab_size, batch_size):\n",
    "    \"\"\"\n",
    "    Calculate the final distribution, for the pointer-generator model\n",
    "    Args:\n",
    "    vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays.\n",
    "                The words are in the order they appear in the vocabulary file.\n",
    "    attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n",
    "    Returns:\n",
    "    final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n",
    "    \"\"\"\n",
    "    # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n",
    "    # 这里会构建两个dictionary，一个是vocabd_disruption ictionary，一个是attention_disruption dictionary\n",
    "    # 一个是p_gen, 一个是 1- p_gen，和公式里边是一样的\n",
    "    vocab_dists = [p_gen * dist for (p_gen, dist) in zip(p_gens, vocab_dists)]\n",
    "    \n",
    "    # attn_dists传进来以后先计算了一下1 - p_gen的概率，这里之后会构建一个图中左边attention的结果\n",
    "    attn_dists = [(1 - p_gen) * dist for (p_gen, dist) in zip(p_gens, attn_dists)]\n",
    "\n",
    "    # Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words\n",
    "    extended_vsize = vocab_size + batch_oov_len  # the maximum (over the batch) size of the extended vocabulary\n",
    "    extra_zeros = tf.zeros((batch_size, batch_oov_len))\n",
    "    # list length max_dec_steps of shape (batch_size, extended_vsize)\n",
    "    # 通过遍历vocab的分布来建立vocab_dists_extended的分布，扩展到加上OOV的length\n",
    "    # 就相当于在后边补上一部分，到这里构建的是图中的绿色部分的分布，接下来看一下蓝色的部分是如何构建的\n",
    "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
    "\n",
    "    # Project the values in the attention distributions onto the appropriate entries in the final distributions\n",
    "    # This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary,\n",
    "    # then we add 0.1 onto the 500th entry of the final distribution\n",
    "    # This is done for each decoder timestep.\n",
    "    # This is fiddly; we use tf.scatter_nd to do the projection\n",
    "    batch_nums = tf.range(0, limit=batch_size)  # shape (batch_size)\n",
    "    batch_nums = tf.expand_dims(batch_nums, 1)  # shape (batch_size, 1)\n",
    "    attn_len = tf.shape(_enc_batch_extend_vocab)[1]  # number of states we attend over\n",
    "    batch_nums = tf.tile(batch_nums, [1, attn_len])  # shape (batch_size, attn_len)\n",
    "    # shape (batch_size, enc_t, 2)\n",
    "    indices = tf.stack((batch_nums, _enc_batch_extend_vocab), axis=2)  \n",
    "    shape = [batch_size, extended_vsize]\n",
    "    \n",
    "    # list length max_dec_steps (batch_size, extended_vsize) extended_vsize = 30000 + 2\n",
    "    # 蓝色部分分布的构建。这里相当于是在attention里边做一个映射，得到(batch_size, extended_vsize)\n",
    "    # 这样的一个大小。这里的是通过indices，也就是上边的原始输入的那句话得到的，到这里copy这么大小的数据\n",
    "    # 放进来\n",
    "    attn_dists_projected = [tf.scatter_nd(indices, copy_dist, shape) for copy_dist in attn_dists]\n",
    "\n",
    "    # Add the vocab distributions and the copy distributions together to get the final distributions\n",
    "    # final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving\n",
    "    # the final distribution for that decoder timestep\n",
    "    # Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.\n",
    "    final_dists = [vocab_dist + copy_dist for (vocab_dist, copy_dist) in\n",
    "                   zip(vocab_dists_extended, attn_dists_projected)]\n",
    "\n",
    "    return final_dists</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 .模型运行的时候 上一步预测出来的词 超出vocab范围,下一步输入会不会出问题?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using teacher forcing\n",
    "dec_input = tf.expand_dims(dec_target[:, t], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换掉 oov token unknown token\n",
    "latest_tokens = [t if t in vocab.id2word else unk_index for t in latest_tokens]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Seq2Seq和Point分别起到什么作用?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里要将coverage放到模型里边需要改变一下下边的两个部分的代码\n",
    "1. attention\n",
    "2. loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output):\n",
    "        # query为上次的GRU隐藏层\n",
    "        # values为编码器的编码结果enc_output\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "       \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Attention\n",
    "\n",
    "下边的两个图是原来 的Attention分数计算公式和coverage Attention的分数计算公式的对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/e_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改造$e^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        # query 隐藏层\n",
    "        # values为 编码器的编码结果enc_output\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "        # self.W_s(values)  [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "        # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]    \n",
    "        score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # [batch_sz, max_len, enc_units]\n",
    "        context_vector = attention_weights * enc_output\n",
    "        # [batch_sz, enc_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask + coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        # query为上次的GRU隐藏层\n",
    "        # values为编码器的编码结果enc_output\n",
    "        # 在seq2seq模型中，St是后面的query向量，而编码过程的隐藏状态hi是values。\n",
    "\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            # self.W_s(values) [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "            # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]\n",
    "            score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "            # attention_weights shape (batch_size, max_len, 1)\n",
    "\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "            coverage = attention_weights + prev_coverage\n",
    "        else:\n",
    "            # score shape == (batch_size, max_length, 1)\n",
    "            # we get 1 at the last axis because we are applying score to self.V\n",
    "            # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "            # 计算注意力权重值\n",
    "            score = self.V(tf.nn.tanh(\n",
    "                self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "            # attention_weights = masked_attention(attention_weights)\n",
    "            if use_coverage:\n",
    "                coverage = attention_weights\n",
    "\n",
    "        # attention_weights sha== (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # # 使用注意力权重*编码器输出作为返回值，将来会作为解码器的输入\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector,attention_weights, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 coverage_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数  一个loss的计算\n",
    "# 一开始的loss计算就是拿真实值和预测值做了一个交叉熵\n",
    "# 这里新定义的loss就是加了一个mask，mask对应位置的loss就不去计算\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    mask = tf.math.logical_not(pad_mask)\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log loss + mask batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "# 新定义的loss就是加了一个mask，mask对应位置的loss就不去计算\n",
    "def loss_function(real, pred, padding_mask):\n",
    "    loss = 0\n",
    "    for t in range(real.shape[1]):\n",
    "        if padding_mask:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            mask = tf.cast(padding_mask[:, t], dtype=loss_.dtype)\n",
    "            loss_ *= mask\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "        else:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是将先前时间步的注意力权重加到一起得到所谓的覆盖向量 $c_t$ (coverage vector)，用先前的注意力权重决策来影响当前注意力权重的决策，这样就避免在同一位置重复，从而避免重复生成文本。计算上，先计算coverage vector $c_t$\n",
    "![](img/c_t.png)\n",
    "+ $c^t$就是一个长度为输入长度的向量\n",
    "+ 第一项是之前时刻输入第一个词attention权重的叠加和\n",
    "+ 加这个参数的目的是为了给attention之前生成词的信息，如果之前生成过这些词那么后续要抑制。抑制通过loss函数加惩罚项实现."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个地方使用$c_t$:\n",
    "\n",
    "+ 注意力权重的计算过程中 $e^t_i$\n",
    "+ cov_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        pass\n",
    "        \n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            pass\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            # 如果使用coverage，那么这个coverage就是attention的权重，加上上一步coverage的权重\n",
    "            # 上一步的coverage的权重是什么呢？如果是第一步的话，那就直接是attention的权重\n",
    "            # 这样会出现的效果就是：反复出现的那个词的权重，会变得更大，由于是累加，后边再进行计算的\n",
    "            # 时候，对于权重更大的词，就进行更强的惩罚\n",
    "            coverage = attention_weights + prev_coverage\n",
    "        else:    \n",
    "            if use_coverage:\n",
    "                coverage = attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cobloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<START> 举起 车辆 左 前轮 缸体 上 <STOP> <PAD> <PAD> `\n",
    "一个提升的部分：除了上边的改进，这里还可以加入一个padding_mask，mask就是把对应的位置不去计算\n",
    "这里可以准确的告诉你<PAD>对应的都是哪些位，然后再和cover_losses做一个乘积，乘以0，就忽略不计了，乘以1就保留，相当于将后边两位的结果就不去计算了，即填充位的loss就不去计算了，由于这里的loss是用来计算优化 更新权重的，所以如果不计算进来的话，影响会更小一些，可以更加专注于真正的自然语言的部分，这也是一个提升的点\n",
    "`padding_mask`->`[1,1,1,1,1,1,1,0,0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_coverage_loss(attn_dists, coverages, padding_mask):\n",
    "    # 这里计算loss的时候使用padding_mask \n",
    "    \"\"\"\n",
    "    Calculates the coverage loss from the attention distributions.\n",
    "      Args:\n",
    "        attn_dists coverages: [max_len_y, batch_sz, max_len_x, 1]\n",
    "        padding_mask: shape (batch_size, max_len_y).\n",
    "      Returns:\n",
    "        coverage_loss: scalar\n",
    "    \"\"\"\n",
    "    cover_losses = []\n",
    "    # attn_dists 和coverages 拿进来之后，将为1的维数拿掉，全部处理成[max_len_y, batch_sz, max_len_x]\n",
    "    # 这样大小的数据\n",
    "    # transfer attn_dists coverages to [max_len_y, batch_sz, max_len_x]\n",
    "    attn_dists = tf.squeeze(attn_dists, axis=3)\n",
    "    coverages = tf.squeeze(coverages, axis=3)\n",
    "\n",
    "    \n",
    "    for t in range(attn_dists.shape[0]):\n",
    "        # 取这两个对应位置最小的值，就是上边的公式\n",
    "        # 拿到attn_dists和coverages两个对应位的最小的值，拿到cover_loss_ 放到list里边去\n",
    "        # 这里遍历一下就可以拿到一整句话的loss\n",
    "        cover_loss_ = tf.reduce_sum(tf.minimum(attn_dists[t, :, :], coverages[t, :, :]), axis=-1)  # max_len_x wise\n",
    "        cover_losses.append(cover_loss_)\n",
    "    \n",
    "    # change from[max_len_y, batch_sz] to [batch_sz, max_len_y]\n",
    "    cover_losses = tf.stack(cover_losses, 1)\n",
    "\n",
    "    # cover_loss_ [batch_sz, max_len_y]\n",
    "    mask = tf.cast(padding_mask, dtype=cover_loss_.dtype)\n",
    "    cover_losses *= mask\n",
    "    \n",
    "    # mean loss of each time step and then sum up\n",
    "    loss = tf.reduce_sum(tf.reduce_mean(cover_losses, axis=0))  \n",
    "    tf.print('coverage loss(batch sum):', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss改变\n",
    "最后的loss就是将两个部分做一个求和，使用一个超参来决定两个部分的比重\n",
    "等号右边的第一个是原来的loss（交叉熵），加上这里计算的attention和coverage累加的一个loss\n",
    "实现位置train_helper.py 79行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_t_coverage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = loss_function(dec_target[:, 1:], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = loss_function(dec_target, predictions, padding_mask) + \\\n",
    "                         cov_loss_wt * coverage_loss(attentions, coverages, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义个coverage loss来多次惩罚对相对位置的关注.原理很直观，如果之前该词出现过了，那么它的$c^t_i$就很大，那么为了减少$loss$，就需要$a^t_i$变小（因为loss是取两者较小值）,$a^t_i$小就代表着这个位置被注意的概率减少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.constant 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=37, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 1, 2],\n",
       "       [1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=tf.constant([[1,1,5],\n",
    "                [1,1,1]])\n",
    "\n",
    "x2=tf.constant([[1,3,2],\n",
    "                [3,1,3]])\n",
    "tf.minimum(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.reduce_sum 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=62, shape=(), dtype=int32, numpy=6>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant([[1,1,1],[1,1,1]])\n",
    "tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=64, shape=(3,), dtype=int32, numpy=array([2, 2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=66, shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x,1)"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Learning/Project/Q&A/lecture_6/lecture6_PGN.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
